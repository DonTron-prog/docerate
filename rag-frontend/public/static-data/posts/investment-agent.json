{
  "slug": "investment-agent",
  "title": "Investment Research Workflow",
  "date": "2025-06-24",
  "tags": [
    "AI",
    "Agents",
    "Investment",
    "Financial Analysis",
    "Pydantic-AI",
    "Multi-Agent",
    "Research",
    "Automation",
    "Finance"
  ],
  "category": "Technology",
  "description": "A multi-agent investment research system built with Pydantic-AI that autonomously gathers data, performs financial analysis, and generates actionable investment insights.",
  "image": null,
  "content": "- **[Try the Investment Research Agent on Hugging Face](https://huggingface.co/spaces/DonTron/investment-research)**\n- **[See implementation on Github](https://github.com/DonTron-prog/vector-chain.git)**\n\n# Pydantic-AI Investment Research System\n\nThis document details a multi-agent investment research system designed to autonomously gather and analyze financial data. The system performs comprehensive financial analysis and generates actionable investment insights. Its adaptive architecture, featuring dynamic plan adjustment and intelligent memory management, allows it to evolve its research strategy based on real-time findings.\n\n## Overview\n\nThis investment research system leverages a multi-agent architecture built on Pydantic-AI to provide comprehensive, adaptive financial analysis. The system combines autonomous AI agents with specialized tools to gather data from multiple sources, perform calculations, and generate actionable investment insights. The system uses adaptive memory management and dynamic plan adjustment, to evolve its research strategy based on findings.\n\n##  The Agents\n\nThe system employs specialized agents built using `pydantic-ai` that handle different aspects of the research process.\n![Investment agent topology](investment_topology.png)\n*Figure 1: Agent topology. Blue - planning agents, Green - Research agents, Purple - Specilized agents. Red - agent output, Yellow - Tools*\n\n## Architecture\n\nThe project follows an atomic, modular architecture with clear separation of concerns to help me focus on composability:\n\n```\nagents/\n├── dependencies.py        # Type-safe shared context and resources\n├── memory_processors.py   # Advanced memory management for conversations\n├── planning_agent.py      # Investment research planning and adaptation\n└── research_agent.py      # Research execution with tool orchestration\n\ntools/\n├── calculator.py          # Financial metrics and ratio calculations\n├── pdf_extractor.py       # Hybrid PDF extraction (PyMuPDF + VLM)\n├── vector_search.py       # ChromaDB document search with caching\n├── web_scraper.py         # BeautifulSoup content extraction\n└── web_search.py          # Tavily API integration (not SearxNG)\n\nmodels/\n└── schemas.py             # Pydantic data models for type safety\n\nstreamlit_app.py           # Web interface with multiple research modes\nmain.py                    # CLI entry point and core workflows\nconfig.py                  # OpenRouter/OpenAI configuration\nlogfire_config.py          # Observability and monitoring setup\n```\n\n## Core Agents\n\n### Planning Agents\n\n| Agent                       | Purpose                                                                  | Input                                                 | Output                                                       | Memory Strategy                                                                   |\n| --------------------------- | ------------------------------------------------------------------------ | ----------------------------------------------------- | ------------------------------------------------------------ | --------------------------------------------------------------------------------- |\n| **planning_agent**          | Creates initial 2-4 step research plans following investment methodology | User query + context                                  | `ResearchPlan` with logical steps, reasoning, and priorities | Uses `adaptive_memory_processor` for conversation management                      |\n| **adaptive_planning_agent** | Evaluates execution feedback and dynamically adjusts plans               | `PlanUpdateRequest` with feedback and remaining steps | `PlanUpdateResponse` indicating plan updates needed          | Minimal context strategy - keeps only system prompt + last 2 successful decisions |\n\n### Research Agents\n\n| Agent              | Purpose                                       | Input                                      | Output                                                                      | Memory Strategy                                               |\n| ------------------ | --------------------------------------------- | ------------------------------------------ | --------------------------------------------------------------------------- | ------------------------------------------------------------- |\n| **research_agent** | Executes research plans using available tools | Query + plan + `ResearchDependencies`      | `InvestmentFindings` with summary, insights, metrics, risks, recommendation | Uses `filter_research_responses` to preserve valuable content |\n| **feedback_agent** | Evaluates research quality after each step    | Step description + findings + expectations | `ExecutionFeedback` with quality scores, gaps, confidence                   | Stateless evaluation                                          |\n\n### Specialized Agents\n\n| Agent             | Purpose                                                      | Usage                                          | Key Features                                            |\n| ----------------- | ------------------------------------------------------------ | ---------------------------------------------- | ------------------------------------------------------- |\n| **summary_agent** | Condenses conversation history while preserving key findings | Activated when conversations exceed 6 messages | Summarizes older messages while keeping recent 3 intact |\n| **vlm_agent**     | Vision-Language Model for PDF text extraction                | Fallback when PyMuPDF quality < threshold      | Processes PDF pages as images for complex layouts       |\n\n## Available Tools\n\nThe `research_agent` autonomously decides when and how to use these tools:\n\n| Tool                            | Function                          | Features                                                        | Implementation                    |\n| ------------------------------- | --------------------------------- | --------------------------------------------------------------- | --------------------------------- |\n| **search_internal_docs**        | Searches ChromaDB vector database | Query enhancement, result caching (5min TTL), relevance scoring | ChromaDB with embeddings          |\n| **search_web**                  | Current market news and analysis  | Privacy-focused search, multiple categories                     | Tavily API (not SearxNG)          |\n| **scrape_webpage**              | Extract content from web pages    | Article/table/full content modes                                | aiohttp + BeautifulSoup4          |\n| **calculate_financial_metrics** | Compute financial ratios          | P/E, debt ratios, ROE, margins, etc.                            | LLM-based parsing and calculation |\n\n## Memory Management System\n\nThe [`adaptive_memory_processor`](agents/memory_processors.py:203) implements sophisticated conversation management:\n\n- **Short conversations (≤6 messages)**: Keep all with validation\n- **Medium conversations (7-12 messages)**: Filter responses, keep 8 recent\n- **Long conversations (>12 messages)**: Aggressive filtering, keep 6 essential\n\nKey features:\n- **Tool call sequence integrity**: Maintains proper tool call → response pairs\n- **Research keyword preservation**: Keeps messages with \"analysis\", \"findings\", \"recommendation\", etc.\n- **Context preservation**: Always maintains system prompts\n- **Session-level caching**: 5-minute TTL for vector search results\n\n## How It All Connects: The Workflow\n\nThe system operates to transforms a user's investment query into comprehensive analysis. The process begins when the user submits their question (either through either the CLI or the Streamlit web app). This query, along with any relevant context about investment goals or constraints, is processed by the planning agent, which analyzes the request and formulates a logical research strategy consisting of 2-4 steps. That is followed by orchestration of research consisting of a loop of data gathering and analysis, culminating in a report, final recommendation, and its confidence.\n\nUpon creation of the initial plan, the research agent takes over execution, working through each planned step sequentially. For each step, the agent autonomously determines which tools are most appropriate for gathering the required information. It might start by searching the internal vector database for relevant SEC filings and earnings reports, then expand to web searches for current market sentiment and news, and finally perform financial calculations on the gathered data. (It will soon be able to use API as well) Throughout this process, the agent maintains awareness of what information has already been collected and what gaps remain to be filled.\n\nThe adaptive workflow mode adds an additional layer of intelligence through continuous evaluation and adjustment of the plan. After each research step completes, the feedback agent assesses the quality and completeness of the findings, generating structured feedback that includes confidence scores, identified data gaps, and unexpected discoveries. This feedback is then passed to the adaptive planning agent, which decides whether the original plan needs modification. It updates the state through memory. If significant gaps are identified or valuable unexpected information is discovered, the planning agent can insert new steps, reorder priorities, or remove redundant tasks. This creates an iterative loop where the system learns and adapts as it progresses, much like a human analyst who adjusts their approach based on initial findings.\n![Adaptive Workflow](adaptive_workflow.png)\n*Blue - Start/End, Red - Core Process, Orange - Decisions, Green - Adaptation, Purple - Execution.*\n\nThroughout the entire workflow, the memory management system ensures efficient operation by intelligently filtering and summarizing conversation history to prevent token explosion while preserving essential context. The system maintains multiple types of memory: permanent storage in the vector database for documents, session-level caching for search results, and adaptive conversation memory that adjusts its retention strategy based on conversation length. All findings are continuously aggregated and structured according to Pydantic schemas, ensuring type safety and consistency. The workflow concludes when either all planned steps are completed or the confidence threshold is met, at which point the system generates a final InvestmentAnalysis containing a comprehensive summary, key insights, risk factors, opportunities, and a clear investment recommendation with supporting rationale.\n\n## Workflow Modes\n\nThe system supports multiple research modes through the Streamlit interface:\n\n1. **Simple Chat**: Basic Q&A without tools\n2. **RAG Only**: Vector database search only\n3. **Deep Research**: Web search + analysis\n4. **Full Planning**: Complete workflow with planning agent\n5. **Adaptive Memory**: Full workflow with dynamic plan adaptation\n\n## Running the System\n\n### Prerequisites\n\n```bash\n# Install dependencies\npoetry install\n\n# Set environment variables\nexport OPENROUTER_API_KEY=\"your-openrouter-api-key\"\nexport TAVILY_API_KEY=\"your-tavily-api-key\"  # For web search\n\n# Optional: Logfire for observability\nexport LOGFIRE_TOKEN=\"your-logfire-token\"\n```\n\n### Usage Options\n\n1. **Command Line Interface**:\n```bash\npython main.py\n```\n\n2. **Streamlit Web Interface** (recommended):\n```bash\nstreamlit run streamlit_app.py\n```\n\n3. **Programmatic Usage**:\n```python\nfrom main import adaptive_research_investment\n\nanalysis = await adaptive_research_investment(\n    query=\"Should I invest in Apple?\",\n    context=\"Conservative investor, 5-year horizon\",\n    max_adaptations=3\n)\n```\n\n### Configuration\n\n- **LLM Provider**: Configured for OpenRouter (supports OpenAI-compatible APIs)\n- **Vector Database**: ChromaDB with local persistence at `./investment_chroma_db/`\n- **Knowledge Base**: Pre-loaded documents in `./knowledge_base/`\n- **Default Model**: GPT-4 variants via OpenRouter\n\nThe system's modular design allows easy extension with new agents, tools, or data sources while maintaining clear boundaries between components.\n\n## Conclusion: Augmenting Human Expertise\n\nThis Pydantic-AI based system offers a powerful and flexible framework for building sophisticated, AI-driven investment research applications. Its emphasis on modularity, composability, natural tool use, and adaptive planning.\n\nBefore embarking on any endeavour research must be conducted. By automating the time-consuming work of data gathering and analysis, the agent significantly reduces the search space, illuminating a path for humans. It serves as a powerful tool to enhance context, recognize patterns, and ultimately empower us to make better, faster decisions. The complexity of financial markets are growing, agentic systems like this one are vital for maintaining a competitive edge.",
  "html_content": "<ul>\n<li><strong><a href=\"https://huggingface.co/spaces/DonTron/investment-research\">Try the Investment Research Agent on Hugging Face</a></strong></li>\n<li><strong><a href=\"https://github.com/DonTron-prog/vector-chain.git\">See implementation on Github</a></strong></li>\n</ul>\n<h1 id=\"pydantic-ai-investment-research-system\">Pydantic-AI Investment Research System</h1>\n<p>This document details a multi-agent investment research system designed to autonomously gather and analyze financial data. The system performs comprehensive financial analysis and generates actionable investment insights. Its adaptive architecture, featuring dynamic plan adjustment and intelligent memory management, allows it to evolve its research strategy based on real-time findings.</p>\n<h2 id=\"overview\">Overview</h2>\n<p>This investment research system leverages a multi-agent architecture built on Pydantic-AI to provide comprehensive, adaptive financial analysis. The system combines autonomous AI agents with specialized tools to gather data from multiple sources, perform calculations, and generate actionable investment insights. The system uses adaptive memory management and dynamic plan adjustment, to evolve its research strategy based on findings.</p>\n<h2 id=\"the-agents\">The Agents</h2>\n<p>The system employs specialized agents built using <code>pydantic-ai</code> that handle different aspects of the research process.<br />\n<img alt=\"Investment agent topology\" src=\"investment_topology.png\" /><br />\n<em>Figure 1: Agent topology. Blue - planning agents, Green - Research agents, Purple - Specilized agents. Red - agent output, Yellow - Tools</em></p>\n<h2 id=\"architecture\">Architecture</h2>\n<p>The project follows an atomic, modular architecture with clear separation of concerns to help me focus on composability:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"n\">agents</span><span class=\"o\">/</span>\n<span class=\"err\">├──</span><span class=\"w\"> </span><span class=\"n\">dependencies</span><span class=\"o\">.</span><span class=\"n\">py</span><span class=\"w\">        </span><span class=\"c1\"># Type-safe shared context and resources</span>\n<span class=\"err\">├──</span><span class=\"w\"> </span><span class=\"n\">memory_processors</span><span class=\"o\">.</span><span class=\"n\">py</span><span class=\"w\">   </span><span class=\"c1\"># Advanced memory management for conversations</span>\n<span class=\"err\">├──</span><span class=\"w\"> </span><span class=\"n\">planning_agent</span><span class=\"o\">.</span><span class=\"n\">py</span><span class=\"w\">      </span><span class=\"c1\"># Investment research planning and adaptation</span>\n<span class=\"err\">└──</span><span class=\"w\"> </span><span class=\"n\">research_agent</span><span class=\"o\">.</span><span class=\"n\">py</span><span class=\"w\">      </span><span class=\"c1\"># Research execution with tool orchestration</span>\n\n<span class=\"n\">tools</span><span class=\"o\">/</span>\n<span class=\"err\">├──</span><span class=\"w\"> </span><span class=\"n\">calculator</span><span class=\"o\">.</span><span class=\"n\">py</span><span class=\"w\">          </span><span class=\"c1\"># Financial metrics and ratio calculations</span>\n<span class=\"err\">├──</span><span class=\"w\"> </span><span class=\"n\">pdf_extractor</span><span class=\"o\">.</span><span class=\"n\">py</span><span class=\"w\">       </span><span class=\"c1\"># Hybrid PDF extraction (PyMuPDF + VLM)</span>\n<span class=\"err\">├──</span><span class=\"w\"> </span><span class=\"n\">vector_search</span><span class=\"o\">.</span><span class=\"n\">py</span><span class=\"w\">       </span><span class=\"c1\"># ChromaDB document search with caching</span>\n<span class=\"err\">├──</span><span class=\"w\"> </span><span class=\"n\">web_scraper</span><span class=\"o\">.</span><span class=\"n\">py</span><span class=\"w\">         </span><span class=\"c1\"># BeautifulSoup content extraction</span>\n<span class=\"err\">└──</span><span class=\"w\"> </span><span class=\"n\">web_search</span><span class=\"o\">.</span><span class=\"n\">py</span><span class=\"w\">          </span><span class=\"c1\"># Tavily API integration (not SearxNG)</span>\n\n<span class=\"n\">models</span><span class=\"o\">/</span>\n<span class=\"err\">└──</span><span class=\"w\"> </span><span class=\"n\">schemas</span><span class=\"o\">.</span><span class=\"n\">py</span><span class=\"w\">             </span><span class=\"c1\"># Pydantic data models for type safety</span>\n\n<span class=\"n\">streamlit_app</span><span class=\"o\">.</span><span class=\"n\">py</span><span class=\"w\">           </span><span class=\"c1\"># Web interface with multiple research modes</span>\n<span class=\"n\">main</span><span class=\"o\">.</span><span class=\"n\">py</span><span class=\"w\">                    </span><span class=\"c1\"># CLI entry point and core workflows</span>\n<span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">py</span><span class=\"w\">                  </span><span class=\"c1\"># OpenRouter/OpenAI configuration</span>\n<span class=\"n\">logfire_config</span><span class=\"o\">.</span><span class=\"n\">py</span><span class=\"w\">          </span><span class=\"c1\"># Observability and monitoring setup</span>\n</code></pre></div>\n\n<h2 id=\"core-agents\">Core Agents</h2>\n<h3 id=\"planning-agents\">Planning Agents</h3>\n<table>\n<thead>\n<tr>\n<th>Agent</th>\n<th>Purpose</th>\n<th>Input</th>\n<th>Output</th>\n<th>Memory Strategy</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>planning_agent</strong></td>\n<td>Creates initial 2-4 step research plans following investment methodology</td>\n<td>User query + context</td>\n<td><code>ResearchPlan</code> with logical steps, reasoning, and priorities</td>\n<td>Uses <code>adaptive_memory_processor</code> for conversation management</td>\n</tr>\n<tr>\n<td><strong>adaptive_planning_agent</strong></td>\n<td>Evaluates execution feedback and dynamically adjusts plans</td>\n<td><code>PlanUpdateRequest</code> with feedback and remaining steps</td>\n<td><code>PlanUpdateResponse</code> indicating plan updates needed</td>\n<td>Minimal context strategy - keeps only system prompt + last 2 successful decisions</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"research-agents\">Research Agents</h3>\n<table>\n<thead>\n<tr>\n<th>Agent</th>\n<th>Purpose</th>\n<th>Input</th>\n<th>Output</th>\n<th>Memory Strategy</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>research_agent</strong></td>\n<td>Executes research plans using available tools</td>\n<td>Query + plan + <code>ResearchDependencies</code></td>\n<td><code>InvestmentFindings</code> with summary, insights, metrics, risks, recommendation</td>\n<td>Uses <code>filter_research_responses</code> to preserve valuable content</td>\n</tr>\n<tr>\n<td><strong>feedback_agent</strong></td>\n<td>Evaluates research quality after each step</td>\n<td>Step description + findings + expectations</td>\n<td><code>ExecutionFeedback</code> with quality scores, gaps, confidence</td>\n<td>Stateless evaluation</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"specialized-agents\">Specialized Agents</h3>\n<table>\n<thead>\n<tr>\n<th>Agent</th>\n<th>Purpose</th>\n<th>Usage</th>\n<th>Key Features</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>summary_agent</strong></td>\n<td>Condenses conversation history while preserving key findings</td>\n<td>Activated when conversations exceed 6 messages</td>\n<td>Summarizes older messages while keeping recent 3 intact</td>\n</tr>\n<tr>\n<td><strong>vlm_agent</strong></td>\n<td>Vision-Language Model for PDF text extraction</td>\n<td>Fallback when PyMuPDF quality &lt; threshold</td>\n<td>Processes PDF pages as images for complex layouts</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"available-tools\">Available Tools</h2>\n<p>The <code>research_agent</code> autonomously decides when and how to use these tools:</p>\n<table>\n<thead>\n<tr>\n<th>Tool</th>\n<th>Function</th>\n<th>Features</th>\n<th>Implementation</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>search_internal_docs</strong></td>\n<td>Searches ChromaDB vector database</td>\n<td>Query enhancement, result caching (5min TTL), relevance scoring</td>\n<td>ChromaDB with embeddings</td>\n</tr>\n<tr>\n<td><strong>search_web</strong></td>\n<td>Current market news and analysis</td>\n<td>Privacy-focused search, multiple categories</td>\n<td>Tavily API (not SearxNG)</td>\n</tr>\n<tr>\n<td><strong>scrape_webpage</strong></td>\n<td>Extract content from web pages</td>\n<td>Article/table/full content modes</td>\n<td>aiohttp + BeautifulSoup4</td>\n</tr>\n<tr>\n<td><strong>calculate_financial_metrics</strong></td>\n<td>Compute financial ratios</td>\n<td>P/E, debt ratios, ROE, margins, etc.</td>\n<td>LLM-based parsing and calculation</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"memory-management-system\">Memory Management System</h2>\n<p>The <a href=\"agents/memory_processors.py:203\"><code>adaptive_memory_processor</code></a> implements sophisticated conversation management:</p>\n<ul>\n<li><strong>Short conversations (≤6 messages)</strong>: Keep all with validation</li>\n<li><strong>Medium conversations (7-12 messages)</strong>: Filter responses, keep 8 recent</li>\n<li><strong>Long conversations (&gt;12 messages)</strong>: Aggressive filtering, keep 6 essential</li>\n</ul>\n<p>Key features:<br />\n- <strong>Tool call sequence integrity</strong>: Maintains proper tool call → response pairs<br />\n- <strong>Research keyword preservation</strong>: Keeps messages with &ldquo;analysis&rdquo;, &ldquo;findings&rdquo;, &ldquo;recommendation&rdquo;, etc.<br />\n- <strong>Context preservation</strong>: Always maintains system prompts<br />\n- <strong>Session-level caching</strong>: 5-minute TTL for vector search results</p>\n<h2 id=\"how-it-all-connects-the-workflow\">How It All Connects: The Workflow</h2>\n<p>The system operates to transforms a user&rsquo;s investment query into comprehensive analysis. The process begins when the user submits their question (either through either the CLI or the Streamlit web app). This query, along with any relevant context about investment goals or constraints, is processed by the planning agent, which analyzes the request and formulates a logical research strategy consisting of 2-4 steps. That is followed by orchestration of research consisting of a loop of data gathering and analysis, culminating in a report, final recommendation, and its confidence.</p>\n<p>Upon creation of the initial plan, the research agent takes over execution, working through each planned step sequentially. For each step, the agent autonomously determines which tools are most appropriate for gathering the required information. It might start by searching the internal vector database for relevant SEC filings and earnings reports, then expand to web searches for current market sentiment and news, and finally perform financial calculations on the gathered data. (It will soon be able to use API as well) Throughout this process, the agent maintains awareness of what information has already been collected and what gaps remain to be filled.</p>\n<p>The adaptive workflow mode adds an additional layer of intelligence through continuous evaluation and adjustment of the plan. After each research step completes, the feedback agent assesses the quality and completeness of the findings, generating structured feedback that includes confidence scores, identified data gaps, and unexpected discoveries. This feedback is then passed to the adaptive planning agent, which decides whether the original plan needs modification. It updates the state through memory. If significant gaps are identified or valuable unexpected information is discovered, the planning agent can insert new steps, reorder priorities, or remove redundant tasks. This creates an iterative loop where the system learns and adapts as it progresses, much like a human analyst who adjusts their approach based on initial findings.<br />\n<img alt=\"Adaptive Workflow\" src=\"adaptive_workflow.png\" /><br />\n<em>Blue - Start/End, Red - Core Process, Orange - Decisions, Green - Adaptation, Purple - Execution.</em></p>\n<p>Throughout the entire workflow, the memory management system ensures efficient operation by intelligently filtering and summarizing conversation history to prevent token explosion while preserving essential context. The system maintains multiple types of memory: permanent storage in the vector database for documents, session-level caching for search results, and adaptive conversation memory that adjusts its retention strategy based on conversation length. All findings are continuously aggregated and structured according to Pydantic schemas, ensuring type safety and consistency. The workflow concludes when either all planned steps are completed or the confidence threshold is met, at which point the system generates a final InvestmentAnalysis containing a comprehensive summary, key insights, risk factors, opportunities, and a clear investment recommendation with supporting rationale.</p>\n<h2 id=\"workflow-modes\">Workflow Modes</h2>\n<p>The system supports multiple research modes through the Streamlit interface:</p>\n<ol>\n<li><strong>Simple Chat</strong>: Basic Q&amp;A without tools</li>\n<li><strong>RAG Only</strong>: Vector database search only</li>\n<li><strong>Deep Research</strong>: Web search + analysis</li>\n<li><strong>Full Planning</strong>: Complete workflow with planning agent</li>\n<li><strong>Adaptive Memory</strong>: Full workflow with dynamic plan adaptation</li>\n</ol>\n<h2 id=\"running-the-system\">Running the System</h2>\n<h3 id=\"prerequisites\">Prerequisites</h3>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># Install dependencies</span>\npoetry<span class=\"w\"> </span>install\n\n<span class=\"c1\"># Set environment variables</span>\n<span class=\"nb\">export</span><span class=\"w\"> </span><span class=\"nv\">OPENROUTER_API_KEY</span><span class=\"o\">=</span><span class=\"s2\">&quot;your-openrouter-api-key&quot;</span>\n<span class=\"nb\">export</span><span class=\"w\"> </span><span class=\"nv\">TAVILY_API_KEY</span><span class=\"o\">=</span><span class=\"s2\">&quot;your-tavily-api-key&quot;</span><span class=\"w\">  </span><span class=\"c1\"># For web search</span>\n\n<span class=\"c1\"># Optional: Logfire for observability</span>\n<span class=\"nb\">export</span><span class=\"w\"> </span><span class=\"nv\">LOGFIRE_TOKEN</span><span class=\"o\">=</span><span class=\"s2\">&quot;your-logfire-token&quot;</span>\n</code></pre></div>\n\n<h3 id=\"usage-options\">Usage Options</h3>\n<ol>\n<li><strong>Command Line Interface</strong>:</li>\n</ol>\n<div class=\"highlight\"><pre><span></span><code>python<span class=\"w\"> </span>main.py\n</code></pre></div>\n\n<ol start=\"2\">\n<li><strong>Streamlit Web Interface</strong> (recommended):</li>\n</ol>\n<div class=\"highlight\"><pre><span></span><code>streamlit<span class=\"w\"> </span>run<span class=\"w\"> </span>streamlit_app.py\n</code></pre></div>\n\n<ol start=\"3\">\n<li><strong>Programmatic Usage</strong>:</li>\n</ol>\n<div class=\"highlight\"><pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">main</span> <span class=\"kn\">import</span> <span class=\"n\">adaptive_research_investment</span>\n\n<span class=\"n\">analysis</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">adaptive_research_investment</span><span class=\"p\">(</span>\n    <span class=\"n\">query</span><span class=\"o\">=</span><span class=\"s2\">&quot;Should I invest in Apple?&quot;</span><span class=\"p\">,</span>\n    <span class=\"n\">context</span><span class=\"o\">=</span><span class=\"s2\">&quot;Conservative investor, 5-year horizon&quot;</span><span class=\"p\">,</span>\n    <span class=\"n\">max_adaptations</span><span class=\"o\">=</span><span class=\"mi\">3</span>\n<span class=\"p\">)</span>\n</code></pre></div>\n\n<h3 id=\"configuration\">Configuration</h3>\n<ul>\n<li><strong>LLM Provider</strong>: Configured for OpenRouter (supports OpenAI-compatible APIs)</li>\n<li><strong>Vector Database</strong>: ChromaDB with local persistence at <code>./investment_chroma_db/</code></li>\n<li><strong>Knowledge Base</strong>: Pre-loaded documents in <code>./knowledge_base/</code></li>\n<li><strong>Default Model</strong>: GPT-4 variants via OpenRouter</li>\n</ul>\n<p>The system&rsquo;s modular design allows easy extension with new agents, tools, or data sources while maintaining clear boundaries between components.</p>\n<h2 id=\"conclusion-augmenting-human-expertise\">Conclusion: Augmenting Human Expertise</h2>\n<p>This Pydantic-AI based system offers a powerful and flexible framework for building sophisticated, AI-driven investment research applications. Its emphasis on modularity, composability, natural tool use, and adaptive planning.</p>\n<p>Before embarking on any endeavour research must be conducted. By automating the time-consuming work of data gathering and analysis, the agent significantly reduces the search space, illuminating a path for humans. It serves as a powerful tool to enhance context, recognize patterns, and ultimately empower us to make better, faster decisions. The complexity of financial markets are growing, agentic systems like this one are vital for maintaining a competitive edge.</p>\n<div class=\"footnote\">\n<hr />\n<ol>\n<li id=\"fn:1\">\n<p>I love markdown! I can edit it in any application including mobile, it&rsquo;s simple but has all the formatting I need without bloat, and most importantly I can write and edit using keyboard shortcuts like code.&#160;<a class=\"footnote-backref\" href=\"#fnref:1\" title=\"Jump back to footnote 1 in the text\">&#8617;</a></p>\n</li>\n</ol>\n</div>",
  "reading_time": 7,
  "excerpt": "- Try the Investment Research Agent on Hugging Face - See implementation on Github Pydantic-AI Investment Research System This document details a multi-agent...",
  "metadata": {}
}