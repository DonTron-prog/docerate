{
  "slug": "agentic-reliability-engineering",
  "title": "Agentic Reliability Engineering",
  "date": "2025-05-29",
  "tags": [
    "SRE",
    "AI",
    "Agents",
    "Orchestration",
    "Incident Response",
    "Reliability",
    "Atomic Agents"
  ],
  "category": "DevOps",
  "description": "Exploring how AI agents can revolutionize Site Reliability Engineering by automating incident response, reducing search space, and augmenting human expertise through intelligent orchestration.",
  "image": null,
  "content": "# Building an Agentic-SRE: Future of Incident Response\n\nIn Site Reliability Engineering (SRE), every second counts. When an alert fires, its a race to identify the root cause and implement a fix. This process is often a manual, time-consuming journey through logs, metrics, and documentation. \n\nIn this article I outline an SRE agent that can plan, reflect, identify root causes, and recommend remediation when an alert arrives. While the ultimate goal is a fully reactive system, the initial objective is to significantly reduce the search space that a human engineer needs to navigate. \n\nI demonstrate a proof-of-concept orchestrator agent I've been building that will be expanded to form the main core of an agentic SRE, https://github.com/DonTron-prog/agent_sre.git. The orchestrator chooses a tool (RAG, search, deep-research, and calculator) that is based off the awesome Atomic-Agents framework.\n\n## How it Works: An Overview\n\nAt its core, the Agentic-SRE is an intelligent system designed to mimic the troubleshooting process of a seasoned engineer. Upon receiving an alert, it embarks on a structured investigation, leveraging a suite of powerful tools and techniques.(Pienaar et. al.)[[https://cleric.ai/blog/what-is-an-ai-sre](https://cleric.ai/blog/what-is-an-ai-sre)]\n\n![sre agent overview](sre_agent_overview.png)\n*Operational workflow of the Agentic SRE, showing how it processes alerts through a structured decision-making pipeline that includes trajectory planning, reflection-based reasoning, and orchestration of task execution and tool integration, before generating final responses.*\n\nThe process can be visualized in two key stages: the high-level orchestration of tools and the iterative reasoning loop.\n\nFirst, the **Orchestrator Agent** acts as the central coordinator. When an alert comes in, it analyzes the context and decides which tool is best suited for the initial investigation. This could be a quick search of internal knowledge bases or a more complex deep research query.\n\nOnce a plan is formed, the agent enters a **ReAct (Reasoning and Acting) loop**[Yao el al. 2022](https://arxiv.org/abs/2210.03629). It executes a task, reflects on the output, and refines its approach. This iterative process of execution, reflection, and error correction allows the agent to intelligently navigate the problem space until a satisfactory solution is found.\n\n## The Starting Point: Alerts\n\nAn alert set the process in motion. These can come from various monitoring services like Datadog, Prometheus, or via webhooks. To be useful, these alerts are normalized into a structured format, typically a JSON object, containing key information:\n\n```python\nexample_alerts = [\n    {\n        \"alert\": \"Critical failure: 'ExtPluginReplicationError: Code 7749 - Sync Timeout with AlphaNode' in 'experimental-geo-sync-plugin v0.1.2'\",\n        \"context\": \"System: Primary PostgreSQL Database. Plugin: experimental third-party plugin integrated yesterday. Internal Documentation: NO internal docs exist for this plugin.\"\n    },\n    {\n        \"alert\": \"Pod CrashLoopBackOff for 'checkout-service'. Error: 'java.lang.OutOfMemoryError: Java heap space'\",\n        \"context\": \"System: Kubernetes microservice (Java Spring Boot). Traffic: 3x normal load due to flash sale.\"\n    },\n    {\n        \"alert\": \"API endpoint /api/v2/orders returning 503 Service Unavailable for 5% of requests. Latency P99 is 2500ms.\",\n        \"context\": \"Current error rate threshold: < 1%. Latency SLO: P99 < 800ms.\"\n    }\n]\n```\n\nThe more system information that can be added to the alert is better (such as time, origin or type). The `alert` and `context` form the basis of our queries for gathering more context.\n\n## The Power of Memory: Learning from Past Incidents\n\nRecurring issues can often be resolved more quickly if the resolution was documented. One of the first things the Agentic-SRE should do is check its memory for similar past incidents. If a new alert matches a previously resolved one, the agent can immediately suggest the documented fix and associated Runbook. This addresses a common scenario where on-call engineers may not be aware of prior resolutions for repeat incidents.\n\nIn this implementation, we, naturally, have the internal memory of the LLMs, and have focused on a short-term, conversational memory system that:\n\n- Manages the chat history for the AI agents,\n- Maintains a coherent conversational flow,\n- Has a configurable message limit to manage memory usage.\n\nThis memory is key to the agent's ability to understand the problem and iterate on its plan by retaining the context of previous steps. It will be built upon to include long-term memory connected to RAG to store user or engineer info and preferences, significant events or findings, and record postmortems.[Bae et al]([https://arxiv.org/abs/2210.08750](https://arxiv.org/abs/2210.08750))\n\n## Planning: Charting a Course to Resolution\n\nFor new and complex alerts, a plan is essential. The agent must break down the high-level task of \"find the root cause\" into a series of smaller, manageable steps. This planning is hierarchical. A planner agent devises the overall strategy, while the orchestrator executes the individual tasks.\n\nPlanning is inherently a path-finding problem. The agent must choose the most promising path from various options and tools. As it gathers more information with each step, it can prune unproductive paths and learn necessary parameters, zeroing in on the solution.\n\nTo aid in this process, the planner must have the necessary context of the system. For example a **knowledge graph** provides a high-level, up-to-date view of the system's architecture. This graph, which can be deterministically built and using tools like `kubectl` and injected into the planners context. The graph represents the interconnected components of the platform, from regions and projects down to individual containers and processes and helps in isolating the root cause.[Hao et al]([https://arxiv.org/abs/2305.14992](https://arxiv.org/abs/2305.14992))\n\n### Example of Distributed cloud infrastrucrture\n![Distributed cloud infrastructure](sre_distributed_infra.png)\n*hierarchical structure of modern cloud infrastructure, showing how applications flow from regional cloud boundaries down through networking layers to containerized workloads running application code. The architecture demonstrates the nested relationship between cloud services, from high-level account organization to the granular execution environment where business logic operates.*\n### **Built on Atomic Agents: A Developer-First Framework**\n\nThe Agentic-SRE described bellow is built using [Atomic Agents](https://github.com/BrainBlend-AI/atomic-agents \"https://github.com/BrainBlend-AI/atomic-agents\"), which is an approach to AI agent development that prioritizes developer control and maintainability. It was easy to build upon the prebuilt tools and examples to suit this task.\n\nUnlike frameworks that bury you in abstractions, Atomic Agents follows a simple **Input-Process-Output (IPO)** model. Every agent and tool has:\n\n- **Input Schema** (via Pydantic) - exactly what data goes in\n- **Processing Function** - the actual logic\n- **Output Schema** - precisely what comes out\n\nThis means no guesswork about data shapes, no debugging nightmares through layers of abstraction, and no black-box orchestrators making decisions for you.\n\n```python\n# Clean, predictable agent definition\nagent = BaseAgent(\n    BaseAgentConfig(\n        client=instructor.from_openai(openai.OpenAI()),\n        model=\"gpt-4o-mini\",\n        input_schema=OrchestratorInputSchema,\n        output_schema=OrchestratorOutputSchema,\n        system_prompt_generator=system_prompt_generator\n    )\n)\n```\n\nWhat makes Atomic Agents particularly powerful for SRE use cases:\n- Modularity: Each tool (RAG, web search, calculator) is an independent \"atom\" that can be swapped, tested, or debugged in isolation.\n- Schema Chaining: Tools connect seamlessly when their input/output schemas align - no manual data transformation needed.\n- Debuggability: Set breakpoints anywhere. See exactly what's in your system prompt, input data, or output JSON. \n- Performance: scale it like any traditional backend using python with standard deployment patterns.\n\nThe framework's philosophy of \"doing one thing well\" aligns perfectly with SRE principles. After getting lost in LangGraph and and CrewAI I found it an ideal foundation for the Agentic-SRE.  \n## Orchestration: The Intelligent Coordinator\n\nThe orchestrator is the heart of our Agentic-SRE. It's an intelligent decision-making coordinator that analyzes incoming requests and routes them to the most appropriate tool. To try it out follow the setup in the README then run:\n\n```Bash\npython orchestration_agent/orchestrator.py\n```\n\n![orchestrator](sre_orchestrator_agent.png)\n\n*Orchestrator Agent architecture that intelligently routes alerts through specialized tools - using Calulator, Web-Search, RAG Search for knowledge base queries against documents and incident history, and Deep Research for complex web-based investigations - before generating comprehensive resolutions through an iterative decision-making process.*\n\nIt follows a simple yet powerful pattern: analyze, decide, and route.\n\n1. **Analyze the context**: It examines the input (alerts, error messages, user queries) along with any relevant contextual information.\n2. **Make intelligent decisions**: It uses LLM reasoning to determine which specialized tool will provide the most valuable information.\n3. **Routes and executes**: It directs the request to the chosen tool with the correctly formatted parameters.\n4. **Repeats as needed**: This loop continues as the agent gathers more information.\n\n\n```python\ndef execute_tool(searxng_tool, calculator_tool, rag_tool, deep_research_tool, orchestrator_output):\n    \"\"\"Route to the appropriate tool based on orchestrator decision.\"\"\"\n    \n    if orchestrator_output.tool in (\"search\", \"web-search\"):\n        if not isinstance(orchestrator_output.tool_parameters, SearxNGSearchToolInputSchema):\n            raise ValueError(f\"Invalid parameters for search tool\")\n        return searxng_tool.run(orchestrator_output.tool_parameters)\n    \n    elif orchestrator_output.tool == \"rag\":\n        if not isinstance(orchestrator_output.tool_parameters, RAGSearchToolInputSchema):\n            raise ValueError(f\"Invalid parameters for RAG tool\")\n        return rag_tool.run(orchestrator_output.tool_parameters)\n    \n    elif orchestrator_output.tool == \"deep-research\":\n        return deep_research_tool.run(orchestrator_output.tool_parameters)\n    \n    elif orchestrator_output.tool == \"calculator\":\n        return calculator_tool.run(orchestrator_output.tool_parameters)\n    \n    else:\n        raise ValueError(f\"Unknown tool: {orchestrator_output.tool}\")\n```\n\n\nIn my implementation, the Agentic-SRE uses the following tools:\n\n- **RAG (Retrieval-Augmented Generation)**: For querying internal knowledge bases, runbooks, and documentation.\n- **Web Search**: For finding external information like error codes, CVEs, or troubleshooting guides. For this, I used SearxNG, a privacy-respecting metasearch engine.\n- **Deep Research**: For comprehensive, multi-source analysis of complex problems.\n- **Calculator**: For metric calculations, threshold analysis, and numerical computations.\n\nTo ensure reliability, Atomic-Agents uses `Pydantic` for validating inputs and outputs and `Instructor` to work with structured outputs from LLMs, which simplifies managing validation, retries, and streaming responses.\n\n## Instructor Setup\n```python\n# Setup structured LLM client\nclient = instructor.from_openai(openai.OpenAI(api_key=config[\"openai_api_key\"]))\n\n# Agent automatically validates and retries based on Pydantic schemas\nagent = BaseAgent(\n    BaseAgentConfig(\n        client=client,\n\t\t...\n    )\n)\n```\n\n## Pydantic validations\n```python\nclass OrchestratorInputSchema(BaseIOSchema):\n    \"\"\"Input schema for the SRE Orchestrator Agent.\"\"\"\n    \n    system_alert: str = Field(..., description=\"The system alert received (e.g., 'High CPU utilization on server X').\")\n    system_context: str = Field(..., description=\"Contextual information about the system (e.g., 'Production web server, recent deployment v1.2').\")\n\nclass OrchestratorOutputSchema(BaseIOSchema):\n    \"\"\"Output schema containing the tool to use and its parameters.\"\"\"\n    \n    tool: str = Field(..., description=\"The tool to use: 'search', 'calculator', 'rag', or 'deep-research'\")\n    tool_parameters: Union[SearxNGSearchToolInputSchema, CalculatorToolInputSchema, RAGSearchToolInputSchema, DeepResearchToolInputSchema] = Field(\n        ..., description=\"The parameters for the selected tool\"\n    )\n```\n\n## Tools\n### Internal Knowledge: Retrieval-Augmented Generation (RAG)\n\nWhen an alert isn't immediately recognized from memory, the agent needs to gather more context. This is where Retrieval-Augmented Generation (RAG) comes in. RAG provides the LLM with relevant reference material from an external knowledge base. For this pipeline, I'm using `ChromaDB` as the vector store.\n\nFor example, a generic LLM might know what a ‘CrashLoopBackOff’ is. But with RAG, we can provide it with our internal documentation, informing it that for our specific `auth-service`, a common cause is a misconfigured database connection—something only found in our private docs.\n\nThe RAG process involves:\n\n1. **Ingesting Data**: Relevant data sources like infrastructure knowledge graphs, code repositories, documentation, and communication channels are ingested. Given that an estimated two-thirds of outages are due to configuration and human errors, recent information from Git and Slack is particularly valuable.\n2. **Chunking and Embedding**: The source data is broken into chunks and converted into numerical representations (embeddings) using an embedding model.\n3. **Storing**: These embeddings are stored in a vector store (ChromaDB) for efficient semantic search.\n4. **Retrieving**: When an alert comes in, its summary and details are used to query the vector store, which returns the most relevant documents.\n\nTo test the stand alone RAG interactively on your own documents Run `orchestration_agent/tools/rag_search/interactive.py`\n\n### Uncovering Nuance with Deep Research\n\nThe deep research tool acts as an intelligent research assistant, capable of synthesizing information from multiple sources across the web. This is invaluable for:\n\n- **Error Investigation**: Looking up obscure error codes or messages.\n- **Technology Stack Issues**: Researching bugs or performance issues in open-source components.\n- **Third-Party Service Outages**: Checking if a third-party service is experiencing a known outage.\n- **Security Vulnerabilities**: Investigating potential security threats.\n\nThis deep research tool employs a multi-agent approach, with specialized agents for decision-making, query generation, and question-answering. This allows it to autonomously gather, process, and synthesize information, providing a comprehensive overview with citations and suggestions for follow-up questions. checkout `orchestration_agent/tools/deep_research/interactive.py` to run it as a stand alone deep researcher.\n\n## Reflection: The Key to Improved Performance\n\nInterleaving reasoning and action (the ReAct framework) with moments of reflection has been shown to significantly improve performance. The more complex a task, the more potential failure points exist. After each step, our agent reflects on the generated response and the retrieved context. If the answer isn't satisfactory, the agent can iterate to refine its output.\n\n\n```python\ndef generate_final_answer(agent, input_schema, tool_response):\n    \"\"\"Generate a final answer based on the tool's output - this is the reflection step.\"\"\"\n    \n    # Temporarily switch to final answer schema\n    original_schema = agent.output_schema\n    agent.output_schema = FinalAnswerSchema\n    \n    # Add tool response to memory for context\n    agent.memory.add_message(\"system\", tool_response)\n    \n    # Run agent again to synthesize final answer\n    final_answer_obj = agent.run(input_schema)\n    \n    # Restore original schema\n    agent.output_schema = original_schema\n    \n    return final_answer_obj\n```\n\n## Bringing It All Together: LLM Reasoning and Prompt Assembly\n\nWith all the pieces in place, the agent assembles a prompt for the LLM. This prompt includes:\n\n- The alert itself (the problem statement).\n- The results from each step of the executed plan (logs, deployment info, metrics).\n- Any additional context retrieved from the knowledge base.\n\nThe prompt instructs the LLM to analyze all this information and derive a root cause and a solution. The goal is to get a summarized, actionable response that explains what happened, where it happened, and how to fix it.\n\n## Conclusion: Augmenting, Not Replacing, Human Expertise\n\nBy combining AI-driven analysis with domain-specific knowledge and tools, an Agentic-SRE can dramatically reduce the complexity and time burden of incident management.\n\nThe key to success is maintaining a balance between automation and human judgment. While this AI assistant can significantly reduce cognitive load and accelerate troubleshooting, it remains a tool designed to augment, not replace, human expertise. By focusing on search space reduction, context enhancement, and pattern recognition, the assistant empowers SRE teams to make better, faster decisions.\n\nAs modern digital infrastructure continues to grow in complexity, tools like the Agentic-SRE will become increasingly vital for maintaining the reliability and performance that users expect. With Atomic-Agents framework they can be quickly integrated and modified. Through thoughtful implementation and continuous refinement, we can achieve new levels of operational excellence.\n\nLet me know what you think?\n\n- Can this agent help narrow the search space in SRE?\n- Are you an SRE? how has new tooling changed your workflow?\n- Where else could this type of orchestrator be applied?\n- What other features would you like to see?\n\nI'd love to hear your thoughts, comment or DM.\n\n**Donald McGillivray**\n\n**mcgillivray.d@gmail.com**",
  "html_content": "<h1 id=\"building-an-agentic-sre-future-of-incident-response\">Building an Agentic-SRE: Future of Incident Response</h1>\n<p>In Site Reliability Engineering (SRE), every second counts. When an alert fires, its a race to identify the root cause and implement a fix. This process is often a manual, time-consuming journey through logs, metrics, and documentation. </p>\n<p>In this article I outline an SRE agent that can plan, reflect, identify root causes, and recommend remediation when an alert arrives. While the ultimate goal is a fully reactive system, the initial objective is to significantly reduce the search space that a human engineer needs to navigate. </p>\n<p>I demonstrate a proof-of-concept orchestrator agent I&rsquo;ve been building that will be expanded to form the main core of an agentic SRE, https://github.com/DonTron-prog/agent_sre.git. The orchestrator chooses a tool (RAG, search, deep-research, and calculator) that is based off the awesome Atomic-Agents framework.</p>\n<h2 id=\"how-it-works-an-overview\">How it Works: An Overview</h2>\n<p>At its core, the Agentic-SRE is an intelligent system designed to mimic the troubleshooting process of a seasoned engineer. Upon receiving an alert, it embarks on a structured investigation, leveraging a suite of powerful tools and techniques.(Pienaar et. al.)[<a href=\"https://cleric.ai/blog/what-is-an-ai-sre\">https://cleric.ai/blog/what-is-an-ai-sre</a>]</p>\n<p><img alt=\"sre agent overview\" src=\"sre_agent_overview.png\" /><br />\n<em>Operational workflow of the Agentic SRE, showing how it processes alerts through a structured decision-making pipeline that includes trajectory planning, reflection-based reasoning, and orchestration of task execution and tool integration, before generating final responses.</em></p>\n<p>The process can be visualized in two key stages: the high-level orchestration of tools and the iterative reasoning loop.</p>\n<p>First, the <strong>Orchestrator Agent</strong> acts as the central coordinator. When an alert comes in, it analyzes the context and decides which tool is best suited for the initial investigation. This could be a quick search of internal knowledge bases or a more complex deep research query.</p>\n<p>Once a plan is formed, the agent enters a <strong>ReAct (Reasoning and Acting) loop</strong><a href=\"https://arxiv.org/abs/2210.03629\">Yao el al. 2022</a>. It executes a task, reflects on the output, and refines its approach. This iterative process of execution, reflection, and error correction allows the agent to intelligently navigate the problem space until a satisfactory solution is found.</p>\n<h2 id=\"the-starting-point-alerts\">The Starting Point: Alerts</h2>\n<p>An alert set the process in motion. These can come from various monitoring services like Datadog, Prometheus, or via webhooks. To be useful, these alerts are normalized into a structured format, typically a JSON object, containing key information:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"n\">example_alerts</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;alert&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;Critical failure: &#39;ExtPluginReplicationError: Code 7749 - Sync Timeout with AlphaNode&#39; in &#39;experimental-geo-sync-plugin v0.1.2&#39;&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;context&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;System: Primary PostgreSQL Database. Plugin: experimental third-party plugin integrated yesterday. Internal Documentation: NO internal docs exist for this plugin.&quot;</span>\n    <span class=\"p\">},</span>\n    <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;alert&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;Pod CrashLoopBackOff for &#39;checkout-service&#39;. Error: &#39;java.lang.OutOfMemoryError: Java heap space&#39;&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;context&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;System: Kubernetes microservice (Java Spring Boot). Traffic: 3x normal load due to flash sale.&quot;</span>\n    <span class=\"p\">},</span>\n    <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;alert&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;API endpoint /api/v2/orders returning 503 Service Unavailable for 5</span><span class=\"si\">% o</span><span class=\"s2\">f requests. Latency P99 is 2500ms.&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;context&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;Current error rate threshold: &lt; 1%. Latency SLO: P99 &lt; 800ms.&quot;</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">]</span>\n</code></pre></div>\n\n<p>The more system information that can be added to the alert is better (such as time, origin or type). The <code>alert</code> and <code>context</code> form the basis of our queries for gathering more context.</p>\n<h2 id=\"the-power-of-memory-learning-from-past-incidents\">The Power of Memory: Learning from Past Incidents</h2>\n<p>Recurring issues can often be resolved more quickly if the resolution was documented. One of the first things the Agentic-SRE should do is check its memory for similar past incidents. If a new alert matches a previously resolved one, the agent can immediately suggest the documented fix and associated Runbook. This addresses a common scenario where on-call engineers may not be aware of prior resolutions for repeat incidents.</p>\n<p>In this implementation, we, naturally, have the internal memory of the LLMs, and have focused on a short-term, conversational memory system that:</p>\n<ul>\n<li>Manages the chat history for the AI agents,</li>\n<li>Maintains a coherent conversational flow,</li>\n<li>Has a configurable message limit to manage memory usage.</li>\n</ul>\n<p>This memory is key to the agent&rsquo;s ability to understand the problem and iterate on its plan by retaining the context of previous steps. It will be built upon to include long-term memory connected to RAG to store user or engineer info and preferences, significant events or findings, and record postmortems.<a href=\"[https://arxiv.org/abs/2210.08750](https://arxiv.org/abs/2210.08750)\">Bae et al</a></p>\n<h2 id=\"planning-charting-a-course-to-resolution\">Planning: Charting a Course to Resolution</h2>\n<p>For new and complex alerts, a plan is essential. The agent must break down the high-level task of &ldquo;find the root cause&rdquo; into a series of smaller, manageable steps. This planning is hierarchical. A planner agent devises the overall strategy, while the orchestrator executes the individual tasks.</p>\n<p>Planning is inherently a path-finding problem. The agent must choose the most promising path from various options and tools. As it gathers more information with each step, it can prune unproductive paths and learn necessary parameters, zeroing in on the solution.</p>\n<p>To aid in this process, the planner must have the necessary context of the system. For example a <strong>knowledge graph</strong> provides a high-level, up-to-date view of the system&rsquo;s architecture. This graph, which can be deterministically built and using tools like <code>kubectl</code> and injected into the planners context. The graph represents the interconnected components of the platform, from regions and projects down to individual containers and processes and helps in isolating the root cause.<a href=\"[https://arxiv.org/abs/2305.14992](https://arxiv.org/abs/2305.14992)\">Hao et al</a></p>\n<h3 id=\"example-of-distributed-cloud-infrastrucrture\">Example of Distributed cloud infrastrucrture</h3>\n<p><img alt=\"Distributed cloud infrastructure\" src=\"sre_distributed_infra.png\" /><br />\n<em>hierarchical structure of modern cloud infrastructure, showing how applications flow from regional cloud boundaries down through networking layers to containerized workloads running application code. The architecture demonstrates the nested relationship between cloud services, from high-level account organization to the granular execution environment where business logic operates.</em></p>\n<h3 id=\"built-on-atomic-agents-a-developer-first-framework\"><strong>Built on Atomic Agents: A Developer-First Framework</strong></h3>\n<p>The Agentic-SRE described bellow is built using <a href=\"https://github.com/BrainBlend-AI/atomic-agents\" title=\"https://github.com/BrainBlend-AI/atomic-agents\">Atomic Agents</a>, which is an approach to AI agent development that prioritizes developer control and maintainability. It was easy to build upon the prebuilt tools and examples to suit this task.</p>\n<p>Unlike frameworks that bury you in abstractions, Atomic Agents follows a simple <strong>Input-Process-Output (IPO)</strong> model. Every agent and tool has:</p>\n<ul>\n<li><strong>Input Schema</strong> (via Pydantic) - exactly what data goes in</li>\n<li><strong>Processing Function</strong> - the actual logic</li>\n<li><strong>Output Schema</strong> - precisely what comes out</li>\n</ul>\n<p>This means no guesswork about data shapes, no debugging nightmares through layers of abstraction, and no black-box orchestrators making decisions for you.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># Clean, predictable agent definition</span>\n<span class=\"n\">agent</span> <span class=\"o\">=</span> <span class=\"n\">BaseAgent</span><span class=\"p\">(</span>\n    <span class=\"n\">BaseAgentConfig</span><span class=\"p\">(</span>\n        <span class=\"n\">client</span><span class=\"o\">=</span><span class=\"n\">instructor</span><span class=\"o\">.</span><span class=\"n\">from_openai</span><span class=\"p\">(</span><span class=\"n\">openai</span><span class=\"o\">.</span><span class=\"n\">OpenAI</span><span class=\"p\">()),</span>\n        <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"s2\">&quot;gpt-4o-mini&quot;</span><span class=\"p\">,</span>\n        <span class=\"n\">input_schema</span><span class=\"o\">=</span><span class=\"n\">OrchestratorInputSchema</span><span class=\"p\">,</span>\n        <span class=\"n\">output_schema</span><span class=\"o\">=</span><span class=\"n\">OrchestratorOutputSchema</span><span class=\"p\">,</span>\n        <span class=\"n\">system_prompt_generator</span><span class=\"o\">=</span><span class=\"n\">system_prompt_generator</span>\n    <span class=\"p\">)</span>\n<span class=\"p\">)</span>\n</code></pre></div>\n\n<p>What makes Atomic Agents particularly powerful for SRE use cases:<br />\n- Modularity: Each tool (RAG, web search, calculator) is an independent &ldquo;atom&rdquo; that can be swapped, tested, or debugged in isolation.<br />\n- Schema Chaining: Tools connect seamlessly when their input/output schemas align - no manual data transformation needed.<br />\n- Debuggability: Set breakpoints anywhere. See exactly what&rsquo;s in your system prompt, input data, or output JSON. <br />\n- Performance: scale it like any traditional backend using python with standard deployment patterns.</p>\n<p>The framework&rsquo;s philosophy of &ldquo;doing one thing well&rdquo; aligns perfectly with SRE principles. After getting lost in LangGraph and and CrewAI I found it an ideal foundation for the Agentic-SRE.  </p>\n<h2 id=\"orchestration-the-intelligent-coordinator\">Orchestration: The Intelligent Coordinator</h2>\n<p>The orchestrator is the heart of our Agentic-SRE. It&rsquo;s an intelligent decision-making coordinator that analyzes incoming requests and routes them to the most appropriate tool. To try it out follow the setup in the README then run:</p>\n<div class=\"highlight\"><pre><span></span><code>python<span class=\"w\"> </span>orchestration_agent/orchestrator.py\n</code></pre></div>\n\n<p><img alt=\"orchestrator\" src=\"sre_orchestrator_agent.png\" /></p>\n<p><em>Orchestrator Agent architecture that intelligently routes alerts through specialized tools - using Calulator, Web-Search, RAG Search for knowledge base queries against documents and incident history, and Deep Research for complex web-based investigations - before generating comprehensive resolutions through an iterative decision-making process.</em></p>\n<p>It follows a simple yet powerful pattern: analyze, decide, and route.</p>\n<ol>\n<li><strong>Analyze the context</strong>: It examines the input (alerts, error messages, user queries) along with any relevant contextual information.</li>\n<li><strong>Make intelligent decisions</strong>: It uses LLM reasoning to determine which specialized tool will provide the most valuable information.</li>\n<li><strong>Routes and executes</strong>: It directs the request to the chosen tool with the correctly formatted parameters.</li>\n<li><strong>Repeats as needed</strong>: This loop continues as the agent gathers more information.</li>\n</ol>\n<div class=\"highlight\"><pre><span></span><code><span class=\"k\">def</span> <span class=\"nf\">execute_tool</span><span class=\"p\">(</span><span class=\"n\">searxng_tool</span><span class=\"p\">,</span> <span class=\"n\">calculator_tool</span><span class=\"p\">,</span> <span class=\"n\">rag_tool</span><span class=\"p\">,</span> <span class=\"n\">deep_research_tool</span><span class=\"p\">,</span> <span class=\"n\">orchestrator_output</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Route to the appropriate tool based on orchestrator decision.&quot;&quot;&quot;</span>\n\n    <span class=\"k\">if</span> <span class=\"n\">orchestrator_output</span><span class=\"o\">.</span><span class=\"n\">tool</span> <span class=\"ow\">in</span> <span class=\"p\">(</span><span class=\"s2\">&quot;search&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;web-search&quot;</span><span class=\"p\">):</span>\n        <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"nb\">isinstance</span><span class=\"p\">(</span><span class=\"n\">orchestrator_output</span><span class=\"o\">.</span><span class=\"n\">tool_parameters</span><span class=\"p\">,</span> <span class=\"n\">SearxNGSearchToolInputSchema</span><span class=\"p\">):</span>\n            <span class=\"k\">raise</span> <span class=\"ne\">ValueError</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Invalid parameters for search tool&quot;</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"n\">searxng_tool</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">orchestrator_output</span><span class=\"o\">.</span><span class=\"n\">tool_parameters</span><span class=\"p\">)</span>\n\n    <span class=\"k\">elif</span> <span class=\"n\">orchestrator_output</span><span class=\"o\">.</span><span class=\"n\">tool</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;rag&quot;</span><span class=\"p\">:</span>\n        <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"nb\">isinstance</span><span class=\"p\">(</span><span class=\"n\">orchestrator_output</span><span class=\"o\">.</span><span class=\"n\">tool_parameters</span><span class=\"p\">,</span> <span class=\"n\">RAGSearchToolInputSchema</span><span class=\"p\">):</span>\n            <span class=\"k\">raise</span> <span class=\"ne\">ValueError</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Invalid parameters for RAG tool&quot;</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"n\">rag_tool</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">orchestrator_output</span><span class=\"o\">.</span><span class=\"n\">tool_parameters</span><span class=\"p\">)</span>\n\n    <span class=\"k\">elif</span> <span class=\"n\">orchestrator_output</span><span class=\"o\">.</span><span class=\"n\">tool</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;deep-research&quot;</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"n\">deep_research_tool</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">orchestrator_output</span><span class=\"o\">.</span><span class=\"n\">tool_parameters</span><span class=\"p\">)</span>\n\n    <span class=\"k\">elif</span> <span class=\"n\">orchestrator_output</span><span class=\"o\">.</span><span class=\"n\">tool</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;calculator&quot;</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"n\">calculator_tool</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">orchestrator_output</span><span class=\"o\">.</span><span class=\"n\">tool_parameters</span><span class=\"p\">)</span>\n\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"k\">raise</span> <span class=\"ne\">ValueError</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Unknown tool: </span><span class=\"si\">{</span><span class=\"n\">orchestrator_output</span><span class=\"o\">.</span><span class=\"n\">tool</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n</code></pre></div>\n\n<p>In my implementation, the Agentic-SRE uses the following tools:</p>\n<ul>\n<li><strong>RAG (Retrieval-Augmented Generation)</strong>: For querying internal knowledge bases, runbooks, and documentation.</li>\n<li><strong>Web Search</strong>: For finding external information like error codes, CVEs, or troubleshooting guides. For this, I used SearxNG, a privacy-respecting metasearch engine.</li>\n<li><strong>Deep Research</strong>: For comprehensive, multi-source analysis of complex problems.</li>\n<li><strong>Calculator</strong>: For metric calculations, threshold analysis, and numerical computations.</li>\n</ul>\n<p>To ensure reliability, Atomic-Agents uses <code>Pydantic</code> for validating inputs and outputs and <code>Instructor</code> to work with structured outputs from LLMs, which simplifies managing validation, retries, and streaming responses.</p>\n<h2 id=\"instructor-setup\">Instructor Setup</h2>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># Setup structured LLM client</span>\n<span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">instructor</span><span class=\"o\">.</span><span class=\"n\">from_openai</span><span class=\"p\">(</span><span class=\"n\">openai</span><span class=\"o\">.</span><span class=\"n\">OpenAI</span><span class=\"p\">(</span><span class=\"n\">api_key</span><span class=\"o\">=</span><span class=\"n\">config</span><span class=\"p\">[</span><span class=\"s2\">&quot;openai_api_key&quot;</span><span class=\"p\">]))</span>\n\n<span class=\"c1\"># Agent automatically validates and retries based on Pydantic schemas</span>\n<span class=\"n\">agent</span> <span class=\"o\">=</span> <span class=\"n\">BaseAgent</span><span class=\"p\">(</span>\n    <span class=\"n\">BaseAgentConfig</span><span class=\"p\">(</span>\n        <span class=\"n\">client</span><span class=\"o\">=</span><span class=\"n\">client</span><span class=\"p\">,</span>\n        <span class=\"o\">...</span>\n    <span class=\"p\">)</span>\n<span class=\"p\">)</span>\n</code></pre></div>\n\n<h2 id=\"pydantic-validations\">Pydantic validations</h2>\n<div class=\"highlight\"><pre><span></span><code><span class=\"k\">class</span> <span class=\"nc\">OrchestratorInputSchema</span><span class=\"p\">(</span><span class=\"n\">BaseIOSchema</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Input schema for the SRE Orchestrator Agent.&quot;&quot;&quot;</span>\n\n    <span class=\"n\">system_alert</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s2\">&quot;The system alert received (e.g., &#39;High CPU utilization on server X&#39;).&quot;</span><span class=\"p\">)</span>\n    <span class=\"n\">system_context</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s2\">&quot;Contextual information about the system (e.g., &#39;Production web server, recent deployment v1.2&#39;).&quot;</span><span class=\"p\">)</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">OrchestratorOutputSchema</span><span class=\"p\">(</span><span class=\"n\">BaseIOSchema</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Output schema containing the tool to use and its parameters.&quot;&quot;&quot;</span>\n\n    <span class=\"n\">tool</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s2\">&quot;The tool to use: &#39;search&#39;, &#39;calculator&#39;, &#39;rag&#39;, or &#39;deep-research&#39;&quot;</span><span class=\"p\">)</span>\n    <span class=\"n\">tool_parameters</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">SearxNGSearchToolInputSchema</span><span class=\"p\">,</span> <span class=\"n\">CalculatorToolInputSchema</span><span class=\"p\">,</span> <span class=\"n\">RAGSearchToolInputSchema</span><span class=\"p\">,</span> <span class=\"n\">DeepResearchToolInputSchema</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">Field</span><span class=\"p\">(</span>\n        <span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s2\">&quot;The parameters for the selected tool&quot;</span>\n    <span class=\"p\">)</span>\n</code></pre></div>\n\n<h2 id=\"tools\">Tools</h2>\n<h3 id=\"internal-knowledge-retrieval-augmented-generation-rag\">Internal Knowledge: Retrieval-Augmented Generation (RAG)</h3>\n<p>When an alert isn&rsquo;t immediately recognized from memory, the agent needs to gather more context. This is where Retrieval-Augmented Generation (RAG) comes in. RAG provides the LLM with relevant reference material from an external knowledge base. For this pipeline, I&rsquo;m using <code>ChromaDB</code> as the vector store.</p>\n<p>For example, a generic LLM might know what a ‘CrashLoopBackOff’ is. But with RAG, we can provide it with our internal documentation, informing it that for our specific <code>auth-service</code>, a common cause is a misconfigured database connection—something only found in our private docs.</p>\n<p>The RAG process involves:</p>\n<ol>\n<li><strong>Ingesting Data</strong>: Relevant data sources like infrastructure knowledge graphs, code repositories, documentation, and communication channels are ingested. Given that an estimated two-thirds of outages are due to configuration and human errors, recent information from Git and Slack is particularly valuable.</li>\n<li><strong>Chunking and Embedding</strong>: The source data is broken into chunks and converted into numerical representations (embeddings) using an embedding model.</li>\n<li><strong>Storing</strong>: These embeddings are stored in a vector store (ChromaDB) for efficient semantic search.</li>\n<li><strong>Retrieving</strong>: When an alert comes in, its summary and details are used to query the vector store, which returns the most relevant documents.</li>\n</ol>\n<p>To test the stand alone RAG interactively on your own documents Run <code>orchestration_agent/tools/rag_search/interactive.py</code></p>\n<h3 id=\"uncovering-nuance-with-deep-research\">Uncovering Nuance with Deep Research</h3>\n<p>The deep research tool acts as an intelligent research assistant, capable of synthesizing information from multiple sources across the web. This is invaluable for:</p>\n<ul>\n<li><strong>Error Investigation</strong>: Looking up obscure error codes or messages.</li>\n<li><strong>Technology Stack Issues</strong>: Researching bugs or performance issues in open-source components.</li>\n<li><strong>Third-Party Service Outages</strong>: Checking if a third-party service is experiencing a known outage.</li>\n<li><strong>Security Vulnerabilities</strong>: Investigating potential security threats.</li>\n</ul>\n<p>This deep research tool employs a multi-agent approach, with specialized agents for decision-making, query generation, and question-answering. This allows it to autonomously gather, process, and synthesize information, providing a comprehensive overview with citations and suggestions for follow-up questions. checkout <code>orchestration_agent/tools/deep_research/interactive.py</code> to run it as a stand alone deep researcher.</p>\n<h2 id=\"reflection-the-key-to-improved-performance\">Reflection: The Key to Improved Performance</h2>\n<p>Interleaving reasoning and action (the ReAct framework) with moments of reflection has been shown to significantly improve performance. The more complex a task, the more potential failure points exist. After each step, our agent reflects on the generated response and the retrieved context. If the answer isn&rsquo;t satisfactory, the agent can iterate to refine its output.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"k\">def</span> <span class=\"nf\">generate_final_answer</span><span class=\"p\">(</span><span class=\"n\">agent</span><span class=\"p\">,</span> <span class=\"n\">input_schema</span><span class=\"p\">,</span> <span class=\"n\">tool_response</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Generate a final answer based on the tool&#39;s output - this is the reflection step.&quot;&quot;&quot;</span>\n\n    <span class=\"c1\"># Temporarily switch to final answer schema</span>\n    <span class=\"n\">original_schema</span> <span class=\"o\">=</span> <span class=\"n\">agent</span><span class=\"o\">.</span><span class=\"n\">output_schema</span>\n    <span class=\"n\">agent</span><span class=\"o\">.</span><span class=\"n\">output_schema</span> <span class=\"o\">=</span> <span class=\"n\">FinalAnswerSchema</span>\n\n    <span class=\"c1\"># Add tool response to memory for context</span>\n    <span class=\"n\">agent</span><span class=\"o\">.</span><span class=\"n\">memory</span><span class=\"o\">.</span><span class=\"n\">add_message</span><span class=\"p\">(</span><span class=\"s2\">&quot;system&quot;</span><span class=\"p\">,</span> <span class=\"n\">tool_response</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Run agent again to synthesize final answer</span>\n    <span class=\"n\">final_answer_obj</span> <span class=\"o\">=</span> <span class=\"n\">agent</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">input_schema</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Restore original schema</span>\n    <span class=\"n\">agent</span><span class=\"o\">.</span><span class=\"n\">output_schema</span> <span class=\"o\">=</span> <span class=\"n\">original_schema</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">final_answer_obj</span>\n</code></pre></div>\n\n<h2 id=\"bringing-it-all-together-llm-reasoning-and-prompt-assembly\">Bringing It All Together: LLM Reasoning and Prompt Assembly</h2>\n<p>With all the pieces in place, the agent assembles a prompt for the LLM. This prompt includes:</p>\n<ul>\n<li>The alert itself (the problem statement).</li>\n<li>The results from each step of the executed plan (logs, deployment info, metrics).</li>\n<li>Any additional context retrieved from the knowledge base.</li>\n</ul>\n<p>The prompt instructs the LLM to analyze all this information and derive a root cause and a solution. The goal is to get a summarized, actionable response that explains what happened, where it happened, and how to fix it.</p>\n<h2 id=\"conclusion-augmenting-not-replacing-human-expertise\">Conclusion: Augmenting, Not Replacing, Human Expertise</h2>\n<p>By combining AI-driven analysis with domain-specific knowledge and tools, an Agentic-SRE can dramatically reduce the complexity and time burden of incident management.</p>\n<p>The key to success is maintaining a balance between automation and human judgment. While this AI assistant can significantly reduce cognitive load and accelerate troubleshooting, it remains a tool designed to augment, not replace, human expertise. By focusing on search space reduction, context enhancement, and pattern recognition, the assistant empowers SRE teams to make better, faster decisions.</p>\n<p>As modern digital infrastructure continues to grow in complexity, tools like the Agentic-SRE will become increasingly vital for maintaining the reliability and performance that users expect. With Atomic-Agents framework they can be quickly integrated and modified. Through thoughtful implementation and continuous refinement, we can achieve new levels of operational excellence.</p>\n<p>Let me know what you think?</p>\n<ul>\n<li>Can this agent help narrow the search space in SRE?</li>\n<li>Are you an SRE? how has new tooling changed your workflow?</li>\n<li>Where else could this type of orchestrator be applied?</li>\n<li>What other features would you like to see?</li>\n</ul>\n<p>I&rsquo;d love to hear your thoughts, comment or DM.</p>\n<p><strong>Donald McGillivray</strong></p>\n<p><strong>mcgillivray.d@gmail.com</strong></p>\n<div class=\"footnote\">\n<hr />\n<ol>\n<li id=\"fn:1\">\n<p>I love markdown! I can edit it in any application including mobile, it&rsquo;s simple but has all the formatting I need without bloat, and most importantly I can write and edit using keyboard shortcuts like code.&#160;<a class=\"footnote-backref\" href=\"#fnref:1\" title=\"Jump back to footnote 1 in the text\">&#8617;</a></p>\n</li>\n</ol>\n</div>",
  "reading_time": 10,
  "excerpt": "Building an Agentic-SRE: Future of Incident Response In Site Reliability Engineering (SRE), every second counts. When an alert fires, its a race to identify...",
  "metadata": {}
}