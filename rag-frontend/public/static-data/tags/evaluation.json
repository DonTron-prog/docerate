{
  "tag": "Evaluation",
  "posts": [
    {
      "slug": "rag-context-engineering",
      "title": "RAG Context Engineering",
      "date": "2025-09-12",
      "tags": [
        "RAG",
        "AI",
        "Evaluation",
        "Agents",
        "Context Engineering",
        "Guardrails"
      ],
      "category": "Infrastructure",
      "description": "This article explores Retrieval-Augmented Generation (RAG) as a key technique for context engineering in LLMs. It details the RAG workflow, covering data indexing (preprocessing, chunking, embedding, and vector database storage) and online retrieval and generation. It then explores evaluation methods, advanced techniques, and its shortcomings",
      "image": null,
      "excerpt": "Intelligence is what one can do with a little bit of information. Stupidity is what one can't do with a lot of information. Today's LLMs and AI fall into the...",
      "reading_time": 11
    },
    {
      "slug": "evaluating-llm-performance-on-specialized-tasks",
      "title": "Evaluating LLM Performance on Specialized Tasks",
      "date": "2025-04-28",
      "tags": [
        "LLM",
        "AI",
        "Machine Learning",
        "Evaluation",
        "Metrics",
        "Performance",
        "NLP",
        "Data Science",
        "Optimization"
      ],
      "category": "Technology",
      "description": "A comprehensive guide to evaluating and optimizing Large Language Models for specialized tasks, covering metric selection, semantic analysis, worst-case performance assessment, and best practices for systematic LLM evaluation.",
      "image": "LLM_evaluation_process.png",
      "excerpt": "EVALUATING LLM PERFORMANCE ON SPECIALIZED TASKS METRIC EVALUATION BEST PRACTICES INTRODUCTION: THE LLM OPTIMIZATION PARADIGM \"What gets measured gets...",
      "reading_time": 13
    }
  ],
  "total": 2
}