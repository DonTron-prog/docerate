[
  {
    "chunk_id": "319c1d22cfdd41df",
    "content": "# Welcome\n\nI'd like to introduce you to this new site, powered by a custom-built static site generator. I developed this platform using Python (the language I know best) and chose to host it on AWS S3, aiming for a balance of simplicity and functionality.",
    "post_slug": "welcome-to-my-blog",
    "post_title": "Welcome to My New Website: Backend Decitions for a Python-Powered Static Site",
    "section_heading": null,
    "tags": [
      "python",
      "web-development",
      "static-site",
      "aws"
    ],
    "url_fragment": "",
    "position": 0,
    "token_count": 63,
    "date": "2025-10-11T22:29:31.749575"
  },
  {
    "chunk_id": "a3f680d2c69e40ef",
    "content": "There are many excellent options available however I had specific requirements that led me to create my own solution. I wanted a platform that would:\n\n1. Give me complete control over both layout and functionality\n2. Remain minimal and fast without requiring JavaScript frameworks\n3. Integrate seamlessly with my existing AWS infrastructure\n4. Support all the technical blogging features I need\n\n![System Architecture Diagram](system_arch_diagram.png)",
    "post_slug": "welcome-to-my-blog",
    "post_title": "Welcome to My New Website: Backend Decitions for a Python-Powered Static Site",
    "section_heading": "Why I Built My Own Static Site Generator",
    "tags": [
      "python",
      "web-development",
      "static-site",
      "aws"
    ],
    "url_fragment": "#why-i-built-my-own-static-site-generator",
    "position": 1,
    "token_count": 112,
    "date": "2025-10-11T22:29:31.749575"
  },
  {
    "chunk_id": "c3438c107d8c9a07",
    "content": "I can write in markdown with comprehensive support for:\n\n- Code blocks with syntax highlighting\n- Tables for structured data\n- Images with automatic optimization\n- Task lists for project tracking\n- Footnotes for additional context[^1]",
    "post_slug": "welcome-to-my-blog",
    "post_title": "Welcome to My New Website: Backend Decitions for a Python-Powered Static Site",
    "section_heading": "Key Features > Markdown Support",
    "tags": [
      "python",
      "web-development",
      "static-site",
      "aws"
    ],
    "url_fragment": "#key-features-markdown-support",
    "position": 2,
    "token_count": 58,
    "date": "2025-10-11T22:29:31.749575"
  },
  {
    "chunk_id": "9ae351cafb573d7f",
    "content": "Here's an example of the Python code that converts markdown to HTML:\n\n```python\ndef parse_markdown(content):\n    \"\"\"Convert markdown content to HTML with extensions.\"\"\"\n    md = markdown.Markdown(extensions=[\n        'codehilite',\n        'tables',\n        'fenced_code',\n        'footnotes',\n        'toc'\n    ])\n    return md.convert(content)\n```",
    "post_slug": "welcome-to-my-blog",
    "post_title": "Welcome to My New Website: Backend Decitions for a Python-Powered Static Site",
    "section_heading": "Key Features > Code Highlighting",
    "tags": [
      "python",
      "web-development",
      "static-site",
      "aws"
    ],
    "url_fragment": "#key-features-code-highlighting",
    "position": 3,
    "token_count": 87,
    "date": "2025-10-11T22:29:31.749575"
  },
  {
    "chunk_id": "99cdc85d8d23c694",
    "content": "The system supports GitHub-flavored markdown tables, making it easy to present structured data:\n\n| Feature | Description | Status |\n|---------|-------------|--------|\n| Markdown parsing | Convert .md files to HTML | \u2705 Complete |\n| Syntax highlighting | Beautiful code blocks | \u2705 Complete |\n| Image optimization | Automatic thumbnail generation | \u2705 Complete |\n| Search | Client-side search with JSON index | \u2705 Complete |\n| RSS Feed | For feed readers | \ud83d\udea7 Coming soon |",
    "post_slug": "welcome-to-my-blog",
    "post_title": "Welcome to My New Website: Backend Decitions for a Python-Powered Static Site",
    "section_heading": "Key Features > Table Support",
    "tags": [
      "python",
      "web-development",
      "static-site",
      "aws"
    ],
    "url_fragment": "#key-features-table-support",
    "position": 4,
    "token_count": 116,
    "date": "2025-10-11T22:29:31.749575"
  },
  {
    "chunk_id": "9620abd8d1744e2b",
    "content": "Track project progress with task lists:\n\n- [x] Set up Python project structure\n- [x] Implement markdown parser\n- [x] Create responsive templates\n- [x] Add syntax highlighting\n- [ ] Implement RSS feed\n- [ ] Add sitemap generation",
    "post_slug": "welcome-to-my-blog",
    "post_title": "Welcome to My New Website: Backend Decitions for a Python-Powered Static Site",
    "section_heading": "Key Features > Task Lists",
    "tags": [
      "python",
      "web-development",
      "static-site",
      "aws"
    ],
    "url_fragment": "#key-features-task-lists",
    "position": 5,
    "token_count": 57,
    "date": "2025-10-11T22:29:31.749575"
  },
  {
    "chunk_id": "3dfbb7f7a4b2c73b",
    "content": "The generator is built with just a few Python dependencies:\n\n```python\n# Core dependencies\nmarkdown==3.5.1          # Markdown parsing\npymdown-extensions==10.5 # Extended markdown features\njinja2==3.1.2           # Template engine\npygments==2.17.2        # Syntax highlighting\npillow==10.2.0          # Image processing\n```",
    "post_slug": "welcome-to-my-blog",
    "post_title": "Welcome to My New Website: Backend Decitions for a Python-Powered Static Site",
    "section_heading": "Technical Implementation",
    "tags": [
      "python",
      "web-development",
      "static-site",
      "aws"
    ],
    "url_fragment": "#technical-implementation",
    "position": 6,
    "token_count": 80,
    "date": "2025-10-11T22:29:31.749575"
  },
  {
    "chunk_id": "d617566255ea9893",
    "content": "My build process follows a straightforward workflow:\n\n1. Parse markdown files with YAML frontmatter\n2. Generate HTML using Jinja2 templates\n3. Optimize images and create thumbnails\n4. Build search index as JSON\n5. Deploy to S3 via GitHub Actions\n![The build process flow](build_process.png)",
    "post_slug": "welcome-to-my-blog",
    "post_title": "Welcome to My New Website: Backend Decitions for a Python-Powered Static Site",
    "section_heading": "Technical Implementation > The Build Process",
    "tags": [
      "python",
      "web-development",
      "static-site",
      "aws"
    ],
    "url_fragment": "#technical-implementation-the-build-process",
    "position": 7,
    "token_count": 72,
    "date": "2025-10-11T22:29:31.749575"
  },
  {
    "chunk_id": "62f771a673cf20e4",
    "content": "The site performs quite well across several key metrics:\n\n| Metric | Result |\n|--------|--------|\n| Build time | ~1 second for 100 posts |\n| Page size | <50KB including CSS |\n| Time to interactive | <500ms with CDN |\n| Lighthouse score | 100/100 |",
    "post_slug": "welcome-to-my-blog",
    "post_title": "Welcome to My New Website: Backend Decitions for a Python-Powered Static Site",
    "section_heading": "Technical Implementation > Performance Metrics",
    "tags": [
      "python",
      "web-development",
      "static-site",
      "aws"
    ],
    "url_fragment": "#technical-implementation-performance-metrics",
    "position": 8,
    "token_count": 61,
    "date": "2025-10-11T22:29:31.749575"
  },
  {
    "chunk_id": "c5ee42e0d2e2a10f",
    "content": "I've implemented a simple but effective AWS setup:\n\n```yaml\nInfrastructure:\n  Storage: S3 bucket with static hosting\n  CDN: CloudFront for global distribution\n  DNS: Route 53 for domain management\n  Deployment: GitHub Actions CI/CD\n```",
    "post_slug": "welcome-to-my-blog",
    "post_title": "Welcome to My New Website: Backend Decitions for a Python-Powered Static Site",
    "section_heading": "AWS Infrastructure",
    "tags": [
      "python",
      "web-development",
      "static-site",
      "aws"
    ],
    "url_fragment": "#aws-infrastructure",
    "position": 9,
    "token_count": 58,
    "date": "2025-10-11T22:29:31.749575"
  },
  {
    "chunk_id": "18f91634520a4cc7",
    "content": "I have several features planned for future development:\n\n1. RSS feed for subscribers\n2. Dark mode toggle\n3. Comments via GitHub discussions\n4. Analytics with privacy-friendly tracking\n5. Newsletter integration",
    "post_slug": "welcome-to-my-blog",
    "post_title": "Welcome to My New Website: Backend Decitions for a Python-Powered Static Site",
    "section_heading": "Future Plans",
    "tags": [
      "python",
      "web-development",
      "static-site",
      "aws"
    ],
    "url_fragment": "#future-plans",
    "position": 10,
    "token_count": 52,
    "date": "2025-10-11T22:29:31.749575"
  },
  {
    "chunk_id": "423a73eb57039d6e",
    "content": "The entire static site generator is open source. You can use it for your own blog or as inspiration for building your own tools. Check out the [GitHub repository](#) for installation instructions and documentation.\n\n---\n\nThanks for reading. I look forward to sharing more technical content, tutorials, and insights on this platform. Stay tuned for posts about Python, AWS, web development, and more.\n\n[^1]: I love markdown! I can edit it in any application including mobile, it's simple but has all the formatting I need without bloat, and most importantly I can write and edit using keyboard shortcuts like code.",
    "post_slug": "welcome-to-my-blog",
    "post_title": "Welcome to My New Website: Backend Decitions for a Python-Powered Static Site",
    "section_heading": "Source Code Availability",
    "tags": [
      "python",
      "web-development",
      "static-site",
      "aws"
    ],
    "url_fragment": "#source-code-availability",
    "position": 11,
    "token_count": 153,
    "date": "2025-10-11T22:29:31.749575"
  },
  {
    "chunk_id": "5f546441e70745a0",
    "content": "- **[Try the Investment Research Agent on Hugging Face](https://huggingface.co/spaces/DonTron/investment-research)**\n- **[See implementation on Github](https://github.com/DonTron-prog/vector-chain.git)**\n\n# Pydantic-AI Investment Research System\n\nThis document details a multi-agent investment research system designed to autonomously gather and analyze financial data. The system performs comprehensive financial analysis and generates actionable investment insights. Its adaptive architecture, featuring dynamic plan adjustment and intelligent memory management, allows it to evolve its research strategy based on real-time findings.",
    "post_slug": "Investment Agent",
    "post_title": "Investment Research Workflow",
    "section_heading": null,
    "tags": [
      "AI",
      "Agents",
      "Investment",
      "Financial Analysis",
      "Pydantic-AI",
      "Multi-Agent",
      "Research",
      "Automation",
      "Finance"
    ],
    "url_fragment": "",
    "position": 0,
    "token_count": 158,
    "date": "2025-10-11T22:29:31.750328"
  },
  {
    "chunk_id": "19b18f6131ba7c1c",
    "content": "This investment research system leverages a multi-agent architecture built on Pydantic-AI to provide comprehensive, adaptive financial analysis. The system combines autonomous AI agents with specialized tools to gather data from multiple sources, perform calculations, and generate actionable investment insights. The system uses adaptive memory management and dynamic plan adjustment, to evolve its research strategy based on findings.",
    "post_slug": "Investment Agent",
    "post_title": "Investment Research Workflow",
    "section_heading": "Overview",
    "tags": [
      "AI",
      "Agents",
      "Investment",
      "Financial Analysis",
      "Pydantic-AI",
      "Multi-Agent",
      "Research",
      "Automation",
      "Finance"
    ],
    "url_fragment": "#overview",
    "position": 1,
    "token_count": 109,
    "date": "2025-10-11T22:29:31.750328"
  },
  {
    "chunk_id": "7206ac547881253a",
    "content": "The system employs specialized agents built using `pydantic-ai` that handle different aspects of the research process.\n![Investment agent topology](investment_topology.png)\n*Figure 1: Agent topology. Blue - planning agents, Green - Research agents, Purple - Specilized agents. Red - agent output, Yellow - Tools*",
    "post_slug": "Investment Agent",
    "post_title": "Investment Research Workflow",
    "section_heading": " The Agents",
    "tags": [
      "AI",
      "Agents",
      "Investment",
      "Financial Analysis",
      "Pydantic-AI",
      "Multi-Agent",
      "Research",
      "Automation",
      "Finance"
    ],
    "url_fragment": "#-the-agents",
    "position": 2,
    "token_count": 78,
    "date": "2025-10-11T22:29:31.750328"
  },
  {
    "chunk_id": "bb2950dba3966aeb",
    "content": "The project follows an atomic, modular architecture with clear separation of concerns to help me focus on composability:\n\n```\nagents/\n\u251c\u2500\u2500 dependencies.py        # Type-safe shared context and resources\n\u251c\u2500\u2500 memory_processors.py   # Advanced memory management for conversations\n\u251c\u2500\u2500 planning_agent.py      # Investment research planning and adaptation\n\u2514\u2500\u2500 research_agent.py      # Research execution with tool orchestration\n\ntools/\n\u251c\u2500\u2500 calculator.py          # Financial metrics and ratio calculations\n\u251c\u2500\u2500 pdf_extractor.py       # Hybrid PDF extraction (PyMuPDF + VLM)\n\u251c\u2500\u2500 vector_search.py       # ChromaDB document search with caching\n\u251c\u2500\u2500 web_scraper.py         # BeautifulSoup content extraction\n\u2514\u2500\u2500 web_search.py          # Tavily API integration (not SearxNG)\n\nmodels/\n\u2514\u2500\u2500 schemas.py             # Pydantic data models for type safety\n\nstreamlit_app.py           # Web interface with multiple research modes\nmain.py                    # CLI entry point and core workflows\nconfig.py                  # OpenRouter/OpenAI configuration\nlogfire_config.py          # Observability and monitoring setup\n```",
    "post_slug": "Investment Agent",
    "post_title": "Investment Research Workflow",
    "section_heading": "Architecture",
    "tags": [
      "AI",
      "Agents",
      "Investment",
      "Financial Analysis",
      "Pydantic-AI",
      "Multi-Agent",
      "Research",
      "Automation",
      "Finance"
    ],
    "url_fragment": "#architecture",
    "position": 3,
    "token_count": 275,
    "date": "2025-10-11T22:29:31.750328"
  },
  {
    "chunk_id": "bce7795e581eab44",
    "content": "| Agent                       | Purpose                                                                  | Input                                                 | Output                                                       | Memory Strategy                                                                   |\n| --------------------------- | ------------------------------------------------------------------------ | ----------------------------------------------------- | ------------------------------------------------------------ | --------------------------------------------------------------------------------- |\n| **planning_agent**          | Creates initial 2-4 step research plans following investment methodology | User query + context                                  | `ResearchPlan` with logical steps, reasoning, and priorities | Uses `adaptive_memory_processor` for conversation management                      |\n| **adaptive_planning_agent** | Evaluates execution feedback and dynamically adjusts plans               | `PlanUpdateRequest` with feedback and remaining steps | `PlanUpdateResponse` indicating plan updates needed          | Minimal context strategy - keeps only system prompt + last 2 successful decisions |",
    "post_slug": "Investment Agent",
    "post_title": "Investment Research Workflow",
    "section_heading": "Core Agents > Planning Agents",
    "tags": [
      "AI",
      "Agents",
      "Investment",
      "Financial Analysis",
      "Pydantic-AI",
      "Multi-Agent",
      "Research",
      "Automation",
      "Finance"
    ],
    "url_fragment": "#core-agents-planning-agents",
    "position": 4,
    "token_count": 309,
    "date": "2025-10-11T22:29:31.750328"
  },
  {
    "chunk_id": "7e0d3b7e0ff9f31c",
    "content": "| Agent              | Purpose                                       | Input                                      | Output                                                                      | Memory Strategy                                               |\n| ------------------ | --------------------------------------------- | ------------------------------------------ | --------------------------------------------------------------------------- | ------------------------------------------------------------- |\n| **research_agent** | Executes research plans using available tools | Query + plan + `ResearchDependencies`      | `InvestmentFindings` with summary, insights, metrics, risks, recommendation | Uses `filter_research_responses` to preserve valuable content |\n| **feedback_agent** | Evaluates research quality after each step    | Step description + findings + expectations | `ExecutionFeedback` with quality scores, gaps, confidence                   | Stateless evaluation                                          |",
    "post_slug": "Investment Agent",
    "post_title": "Investment Research Workflow",
    "section_heading": "Core Agents > Research Agents",
    "tags": [
      "AI",
      "Agents",
      "Investment",
      "Financial Analysis",
      "Pydantic-AI",
      "Multi-Agent",
      "Research",
      "Automation",
      "Finance"
    ],
    "url_fragment": "#core-agents-research-agents",
    "position": 5,
    "token_count": 257,
    "date": "2025-10-11T22:29:31.750328"
  },
  {
    "chunk_id": "38ebe877ce08fd86",
    "content": "| Agent             | Purpose                                                      | Usage                                          | Key Features                                            |\n| ----------------- | ------------------------------------------------------------ | ---------------------------------------------- | ------------------------------------------------------- |\n| **summary_agent** | Condenses conversation history while preserving key findings | Activated when conversations exceed 6 messages | Summarizes older messages while keeping recent 3 intact |\n| **vlm_agent**     | Vision-Language Model for PDF text extraction                | Fallback when PyMuPDF quality < threshold      | Processes PDF pages as images for complex layouts       |",
    "post_slug": "Investment Agent",
    "post_title": "Investment Research Workflow",
    "section_heading": "Core Agents > Specialized Agents",
    "tags": [
      "AI",
      "Agents",
      "Investment",
      "Financial Analysis",
      "Pydantic-AI",
      "Multi-Agent",
      "Research",
      "Automation",
      "Finance"
    ],
    "url_fragment": "#core-agents-specialized-agents",
    "position": 6,
    "token_count": 191,
    "date": "2025-10-11T22:29:31.750328"
  },
  {
    "chunk_id": "93c27f956f821b19",
    "content": "The `research_agent` autonomously decides when and how to use these tools:\n\n| Tool                            | Function                          | Features                                                        | Implementation                    |\n| ------------------------------- | --------------------------------- | --------------------------------------------------------------- | --------------------------------- |\n| **search_internal_docs**        | Searches ChromaDB vector database | Query enhancement, result caching (5min TTL), relevance scoring | ChromaDB with embeddings          |\n| **search_web**                  | Current market news and analysis  | Privacy-focused search, multiple categories                     | Tavily API (not SearxNG)          |\n| **scrape_webpage**              | Extract content from web pages    | Article/table/full content modes                                | aiohttp + BeautifulSoup4          |\n| **calculate_financial_metrics** | Compute financial ratios          | P/E, debt ratios, ROE, margins, etc.                            | LLM-based parsing and calculation |",
    "post_slug": "Investment Agent",
    "post_title": "Investment Research Workflow",
    "section_heading": "Available Tools",
    "tags": [
      "AI",
      "Agents",
      "Investment",
      "Financial Analysis",
      "Pydantic-AI",
      "Multi-Agent",
      "Research",
      "Automation",
      "Finance"
    ],
    "url_fragment": "#available-tools",
    "position": 7,
    "token_count": 279,
    "date": "2025-10-11T22:29:31.750328"
  },
  {
    "chunk_id": "f7010dde61ef5f2e",
    "content": "The [`adaptive_memory_processor`](agents/memory_processors.py:203) implements sophisticated conversation management:\n\n- **Short conversations (\u22646 messages)**: Keep all with validation\n- **Medium conversations (7-12 messages)**: Filter responses, keep 8 recent\n- **Long conversations (>12 messages)**: Aggressive filtering, keep 6 essential\n\nKey features:\n- **Tool call sequence integrity**: Maintains proper tool call \u2192 response pairs\n- **Research keyword preservation**: Keeps messages with \"analysis\", \"findings\", \"recommendation\", etc.\n- **Context preservation**: Always maintains system prompts\n- **Session-level caching**: 5-minute TTL for vector search results",
    "post_slug": "Investment Agent",
    "post_title": "Investment Research Workflow",
    "section_heading": "Memory Management System",
    "tags": [
      "AI",
      "Agents",
      "Investment",
      "Financial Analysis",
      "Pydantic-AI",
      "Multi-Agent",
      "Research",
      "Automation",
      "Finance"
    ],
    "url_fragment": "#memory-management-system",
    "position": 8,
    "token_count": 166,
    "date": "2025-10-11T22:29:31.750328"
  },
  {
    "chunk_id": "e4d6195775472635",
    "content": "The system operates to transforms a user's investment query into comprehensive analysis. The process begins when the user submits their question (either through either the CLI or the Streamlit web app). This query, along with any relevant context about investment goals or constraints, is processed by the planning agent, which analyzes the request and formulates a logical research strategy consisting of 2-4 steps. That is followed by orchestration of research consisting of a loop of data gathering and analysis, culminating in a report, final recommendation, and its confidence. Upon creation of the initial plan, the research agent takes over execution, working through each planned step sequentially. For each step, the agent autonomously determines which tools are most appropriate for gathering the required information. It might start by searching the internal vector database for relevant SEC filings and earnings reports, then expand to web searches for current market sentiment and news, and finally perform financial calculations on the gathered data. (It will soon be able to use API as well) Throughout this process, the agent maintains awareness of what information has already been collected and what gaps remain to be filled. The adaptive workflow mode adds an additional layer of intelligence through continuous evaluation and adjustment of the plan. After each research step completes, the feedback agent assesses the quality and completeness of the findings, generating structured feedback that includes confidence scores, identified data gaps, and unexpected discoveries. This feedback is then passed to the adaptive planning agent, which decides whether the original plan needs modification. It updates the state through memory. If significant gaps are identified or valuable unexpected information is discovered, the planning agent can insert new steps, reorder priorities, or remove redundant tasks.",
    "post_slug": "Investment Agent",
    "post_title": "Investment Research Workflow",
    "section_heading": "How It All Connects: The Workflow",
    "tags": [
      "AI",
      "Agents",
      "Investment",
      "Financial Analysis",
      "Pydantic-AI",
      "Multi-Agent",
      "Research",
      "Automation",
      "Finance"
    ],
    "url_fragment": "#how-it-all-connects-the-workflow",
    "position": 9,
    "token_count": 481,
    "date": "2025-10-11T22:29:31.750328"
  },
  {
    "chunk_id": "4d0ee4c1a5bb955a",
    "content": "If significant gaps are identified or valuable unexpected information is discovered, the planning agent can insert new steps, reorder priorities, or remove redundant tasks. This creates an iterative loop where the system learns and adapts as it progresses, much like a human analyst who adjusts their approach based on initial findings. ![Adaptive Workflow](adaptive_workflow.png)\n*Blue - Start/End, Red - Core Process, Orange - Decisions, Green - Adaptation, Purple - Execution.*\n\nThroughout the entire workflow, the memory management system ensures efficient operation by intelligently filtering and summarizing conversation history to prevent token explosion while preserving essential context. The system maintains multiple types of memory: permanent storage in the vector database for documents, session-level caching for search results, and adaptive conversation memory that adjusts its retention strategy based on conversation length. All findings are continuously aggregated and structured according to Pydantic schemas, ensuring type safety and consistency. The workflow concludes when either all planned steps are completed or the confidence threshold is met, at which point the system generates a final InvestmentAnalysis containing a comprehensive summary, key insights, risk factors, opportunities, and a clear investment recommendation with supporting rationale.",
    "post_slug": "Investment Agent",
    "post_title": "Investment Research Workflow",
    "section_heading": "How It All Connects: The Workflow",
    "tags": [
      "AI",
      "Agents",
      "Investment",
      "Financial Analysis",
      "Pydantic-AI",
      "Multi-Agent",
      "Research",
      "Automation",
      "Finance"
    ],
    "url_fragment": "#how-it-all-connects-the-workflow",
    "position": 10,
    "token_count": 344,
    "date": "2025-10-11T22:29:31.750328"
  },
  {
    "chunk_id": "892bb00252373812",
    "content": "The system supports multiple research modes through the Streamlit interface:\n\n1. **Simple Chat**: Basic Q&A without tools\n2. **RAG Only**: Vector database search only\n3. **Deep Research**: Web search + analysis\n4. **Full Planning**: Complete workflow with planning agent\n5. **Adaptive Memory**: Full workflow with dynamic plan adaptation",
    "post_slug": "Investment Agent",
    "post_title": "Investment Research Workflow",
    "section_heading": "Workflow Modes",
    "tags": [
      "AI",
      "Agents",
      "Investment",
      "Financial Analysis",
      "Pydantic-AI",
      "Multi-Agent",
      "Research",
      "Automation",
      "Finance"
    ],
    "url_fragment": "#workflow-modes",
    "position": 11,
    "token_count": 84,
    "date": "2025-10-11T22:29:31.750328"
  },
  {
    "chunk_id": "bd2db88eb3344527",
    "content": "```bash\n# Install dependencies\npoetry install\n\n# Set environment variables\nexport OPENROUTER_API_KEY=\"your-openrouter-api-key\"\nexport TAVILY_API_KEY=\"your-tavily-api-key\"  # For web search\n\n# Optional: Logfire for observability\nexport LOGFIRE_TOKEN=\"your-logfire-token\"\n```",
    "post_slug": "Investment Agent",
    "post_title": "Investment Research Workflow",
    "section_heading": "Running the System > Prerequisites",
    "tags": [
      "AI",
      "Agents",
      "Investment",
      "Financial Analysis",
      "Pydantic-AI",
      "Multi-Agent",
      "Research",
      "Automation",
      "Finance"
    ],
    "url_fragment": "#running-the-system-prerequisites",
    "position": 12,
    "token_count": 68,
    "date": "2025-10-11T22:29:31.750328"
  },
  {
    "chunk_id": "c4fb917b0d95928e",
    "content": "1. **Command Line Interface**:\n```bash\npython main.py\n```\n\n2. **Streamlit Web Interface** (recommended):\n```bash\nstreamlit run streamlit_app.py\n```\n\n3. **Programmatic Usage**:\n```python\nfrom main import adaptive_research_investment\n\nanalysis = await adaptive_research_investment(\n    query=\"Should I invest in Apple?\",\n    context=\"Conservative investor, 5-year horizon\",\n    max_adaptations=3\n)\n```",
    "post_slug": "Investment Agent",
    "post_title": "Investment Research Workflow",
    "section_heading": "Running the System > Usage Options",
    "tags": [
      "AI",
      "Agents",
      "Investment",
      "Financial Analysis",
      "Pydantic-AI",
      "Multi-Agent",
      "Research",
      "Automation",
      "Finance"
    ],
    "url_fragment": "#running-the-system-usage-options",
    "position": 13,
    "token_count": 99,
    "date": "2025-10-11T22:29:31.750328"
  },
  {
    "chunk_id": "f730a043c269affc",
    "content": "- **LLM Provider**: Configured for OpenRouter (supports OpenAI-compatible APIs)\n- **Vector Database**: ChromaDB with local persistence at `./investment_chroma_db/`\n- **Knowledge Base**: Pre-loaded documents in `./knowledge_base/`\n- **Default Model**: GPT-4 variants via OpenRouter\n\nThe system's modular design allows easy extension with new agents, tools, or data sources while maintaining clear boundaries between components.",
    "post_slug": "Investment Agent",
    "post_title": "Investment Research Workflow",
    "section_heading": "Running the System > Configuration",
    "tags": [
      "AI",
      "Agents",
      "Investment",
      "Financial Analysis",
      "Pydantic-AI",
      "Multi-Agent",
      "Research",
      "Automation",
      "Finance"
    ],
    "url_fragment": "#running-the-system-configuration",
    "position": 14,
    "token_count": 106,
    "date": "2025-10-11T22:29:31.750328"
  },
  {
    "chunk_id": "82f8fc1a9da8ee8f",
    "content": "This Pydantic-AI based system offers a powerful and flexible framework for building sophisticated, AI-driven investment research applications. Its emphasis on modularity, composability, natural tool use, and adaptive planning.\n\nBefore embarking on any endeavour research must be conducted. By automating the time-consuming work of data gathering and analysis, the agent significantly reduces the search space, illuminating a path for humans. It serves as a powerful tool to enhance context, recognize patterns, and ultimately empower us to make better, faster decisions. The complexity of financial markets are growing, agentic systems like this one are vital for maintaining a competitive edge.",
    "post_slug": "Investment Agent",
    "post_title": "Investment Research Workflow",
    "section_heading": "Conclusion: Augmenting Human Expertise",
    "tags": [
      "AI",
      "Agents",
      "Investment",
      "Financial Analysis",
      "Pydantic-AI",
      "Multi-Agent",
      "Research",
      "Automation",
      "Finance"
    ],
    "url_fragment": "#conclusion-augmenting-human-expertise",
    "position": 15,
    "token_count": 173,
    "date": "2025-10-11T22:29:31.750328"
  },
  {
    "chunk_id": "ea7dd2f32e8989a0",
    "content": "The site Reliability Engineering (SRE) position was created out of necessity by Google to maintian its gigital infrastructure and is now a critical discipline across the technology industry. Today, SRE teams face mounting pressure to maintain system reliability while managing unprecedented scale and complexity. I believe The emergence of AI-driven assistants represents a paradigm shift in how we approach reliability engineering in all fields, promising to augment human expertise with intelligent automation, and its starting with SREs. Agentic SRE systems autonomously reason about infrastructure problems, plan investigation strategies, and recommend solutions. Unlike traditional monitoring tools that simply alert on problems, these systems actively participate in the incident response process, reducing the cognitive load on human engineers and accelerating time to resolution. The development of such systems requires a phased approach, similar to onboarding a new team member:\n\n1. **Learning Phase**: The agent integrates with existing data sources, builds understanding of infrastructure and workflows, and provides immediate value through knowledge retrieval and context aggregation. 2. **Reasoning Phase**: The system develops sophisticated capabilities for root cause analysis and remediation suggestions, incorporating planning and advanced tool use while maintaining read-only access for safety. 3. **Automation Phase**: With deep system understanding, the agent begins automating routine procedures under human oversight\u2014the ultimate goal of autonomous reliability engineering. *To see how I tackled developing such systems see: [Agentic Reliability Engineering](Agentic%20Reliability%20Engineering.md), and the code at [github](https://github.com/DonTron-prog/agent_sre.git)*\n\nThroughout this evolution, safety, reliability, and human oversight remain paramount.",
    "post_slug": "Agentic SRE",
    "post_title": "Agentic SRE",
    "section_heading": "Introduction",
    "tags": [
      "SRE",
      "AI",
      "AIOps",
      "Incident Management",
      "Platform Engineering",
      "Observability",
      "Automation",
      "DevOps"
    ],
    "url_fragment": "#introduction",
    "position": 0,
    "token_count": 470,
    "date": "2025-10-11T22:29:31.751042"
  },
  {
    "chunk_id": "1b96c1f635b06a47",
    "content": "The challenge lies in building systems that provide genuine value to engineers while navigating the complexities of heterogeneous infrastructure, ensuring output accuracy, and maintaining robust safety protocols.",
    "post_slug": "Agentic SRE",
    "post_title": "Agentic SRE",
    "section_heading": "Introduction",
    "tags": [
      "SRE",
      "AI",
      "AIOps",
      "Incident Management",
      "Platform Engineering",
      "Observability",
      "Automation",
      "DevOps"
    ],
    "url_fragment": "#introduction",
    "position": 1,
    "token_count": 53,
    "date": "2025-10-11T22:29:31.751042"
  },
  {
    "chunk_id": "dccab4502f157c6a",
    "content": "Modern digital platforms are huge and wildly complex. Distributed architectures, microservices, cloud-native technologies, and continuous deployment cycles create intricate interdependencies that generate vast amounts of operational data. SRE teams have to minimize downtime, optimize critical metrics like Mean Time to Detect (MTTD) and Mean Time to Resolve (MTTR), to honor increasingly stringent Service Level Agreements (SLAs).\n\nTraditional manual approaches and siloed tooling struggle to keep pace with this complexity. The industry has responded with AIOps platforms that leverage AI/ML to automate routine tasks, enhance observability through intelligent analysis of logs, metrics, and traces, and promise to detect anomalies, predict failures, and accelerate root cause analysis.",
    "post_slug": "Agentic SRE",
    "post_title": "Agentic SRE",
    "section_heading": "The Evolution of Site Reliability Engineering",
    "tags": [
      "SRE",
      "AI",
      "AIOps",
      "Incident Management",
      "Platform Engineering",
      "Observability",
      "Automation",
      "DevOps"
    ],
    "url_fragment": "#the-evolution-of-site-reliability-engineering",
    "position": 2,
    "token_count": 197,
    "date": "2025-10-11T22:29:31.751042"
  },
  {
    "chunk_id": "1243869e53ecd732",
    "content": "The emergence of agentic SRE platforms represents a significant evolution beyond traditional AIOps. These systems don't just monitor and alert\u2014they actively reason about problems, plan investigations, and recommend solutions. The following table showcases the current landscape of agentic SRE platforms and tools:\n\n| **Agentic SRE** | **Description**                                                                                                                                                                                          | **Good For**                                                                                                    | **Open-Source** | **Website**                                        |\n| --------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------- | --------------- | -------------------------------------------------- |\n| **Rootly**      | AI-native incident response platform integrating with Slack and Teams to automate on-call workflows and postmortems. | Incident response, Slack integration, postmortems. | \u274c               | [rootly.com](https://rootly.com/)                  |\n| **Nudgebee**    | AI-agentic assistants platform for modern SRE and Ops teams, offering specialized agents for cloud-native operations. | Kubernetes troubleshooting, cloud-native ops automation. | \u274c               | [nudgebee.com](https://nudgebee.com/)              |\n| **Cleric**      | Autonomous AI SRE designed to manage, optimize, and heal software infrastructure, reducing alert fatigue. | Production issue diagnosis, infrastructure optimization. | \u274c               | [cleric.io](https://cleric.io/)                    |\n| **K8sGPT**      | AI-powered tool that diagnoses and fixes Kubernetes issues with intelligent insights and automated troubleshooting.",
    "post_slug": "Agentic SRE",
    "post_title": "Agentic SRE",
    "section_heading": "The Evolution of Site Reliability Engineering > The Agentic SRE Landscape",
    "tags": [
      "SRE",
      "AI",
      "AIOps",
      "Incident Management",
      "Platform Engineering",
      "Observability",
      "Automation",
      "DevOps"
    ],
    "url_fragment": "#the-evolution-of-site-reliability-engineering-the-agentic-sre-landscape",
    "position": 3,
    "token_count": 514,
    "date": "2025-10-11T22:29:31.751042"
  },
  {
    "chunk_id": "f97e814f48279e4b",
    "content": "| Kubernetes diagnostics, cluster management. | \u2705               | [k8sgpt.ai](https://k8sgpt.ai/)                    |\n| **Parity**      | AI SRE acting as the first line of defense for on-call engineers, particularly in Kubernetes environments. | Incident response, Kubernetes operations. | \u274c               | [tryparity.com](https://www.tryparity.com/)        |\n| **SRE.ai**      | AI-powered automation platform for Salesforce development teams, providing agents that deploy between environments, configure and execute workflows, and query for information with just a chat message. | Salesforce DevOps, CI/CD, merge conflict resolution, environment management. | \u274c               | [sre.ai](https://sre.ai/)                          |\n| **Agent SRE**   | Open-source framework enabling autonomous interaction with computers through an agent-computer interface. | Automating GUI tasks, AI agent development. | \u2705               | [GitHub](https://github.com/simular-ai/Agent-S)    |\n| **Beeps**       | An on-call platform tailored for Next.js developers, offering rapid incident resolution with integrations to GitHub, Vercel, Slack, and more. | Autonomous task execution, content creation.Incident response, Next.js applications, observability integration. | \u274c               | [beeps.dev](https://beeps.dev)                     |\n| **ControlFlow** | Python framework for building agentic AI workflows, allowing structured task delegation to LLMs. | AI workflow orchestration, task automation.",
    "post_slug": "Agentic SRE",
    "post_title": "Agentic SRE",
    "section_heading": "The Evolution of Site Reliability Engineering > The Agentic SRE Landscape",
    "tags": [
      "SRE",
      "AI",
      "AIOps",
      "Incident Management",
      "Platform Engineering",
      "Observability",
      "Automation",
      "DevOps"
    ],
    "url_fragment": "#the-evolution-of-site-reliability-engineering-the-agentic-sre-landscape",
    "position": 4,
    "token_count": 373,
    "date": "2025-10-11T22:29:31.751042"
  },
  {
    "chunk_id": "9162d2c3a828d570",
    "content": "| AI workflow orchestration, task automation. | \u2705               | [GitHub](https://github.com/PrefectHQ/ControlFlow) |\n|                 |                                                                                                                                                                                                          |                                                                                                                 |                 |                                                    |\n| **RunWhen**     | Platform to build specialized SRE, Platform, and DevOps assistants leveraging a library of pre-built automation. | Alert response, ticket drafting, DevOps automation. | \u274c               | [runwhen.com](https://www.runwhen.com/)            |\n| **Wild Moose**  | AI-powered SRE copilot that automates root cause analysis and streamlines incident response. | Real-time incident analysis, observability integration. | \u274c               | [wildmoose.ai](https://www.wildmoose.ai/)          |\n| **Kura**        | Intelligent DevOps copilot that integrates directly with AWS to answer questions, generate code, and assist with everyday DevOps tasks. | AWS infrastructure management, incident response, resource provisioning. | \u274c               | [usekura.com](https://www.usekura.com/)            |",
    "post_slug": "Agentic SRE",
    "post_title": "Agentic SRE",
    "section_heading": "The Evolution of Site Reliability Engineering > The Agentic SRE Landscape",
    "tags": [
      "SRE",
      "AI",
      "AIOps",
      "Incident Management",
      "Platform Engineering",
      "Observability",
      "Automation",
      "DevOps"
    ],
    "url_fragment": "#the-evolution-of-site-reliability-engineering-the-agentic-sre-landscape",
    "position": 5,
    "token_count": 333,
    "date": "2025-10-11T22:29:31.751042"
  },
  {
    "chunk_id": "25e3fa3adf10e162",
    "content": "![SRE workflow](sre_workflow.png)\nintegrating AI SRE within a heterogeneous landscape of tools and data sources is challenging. Sources include: \n- monitoring and observability platforms (e.g., Datadog, Prometheus, Grafana),\n- code repositories (GitHub),\n- documentation platforms (Confluence, Notion),\n- communication channels (Slack),\n- incident management systems (Jira, PagerDuty, ServiceNow),\n- databases, and custom internal tools.\nEach enterprise will be different and will bring their own systems, procedures, and datasets. \n\nDistributed infrastructure like Kubernetes and Kafka adds additional layers of complexity: \n- clusters may span multiple clouds and on\u2011prem environments, \n- workloads are highly ephemeral, \n- and autoscaling or self\u2011healing actions can erase the evidence an investigator needs. \nSREs must also cope with cascading failures triggered by resource\u2011starved nodes, stuck rollouts, or misconfigured network policies that can silently break. The AI agent must be able to speak to the Kubernetes API or Kafka topics fluently:\n- watching events and topics, querying etcd\u2011backed states, \n- and correlating pod restarts, image hashes, and recent `kubectl apply` operations with upstream alerts. \n\nComplicating SRE design, each enterprise will bring its own cluster topologies, admission\u2011controller policies, and GitOps workflows, so these bespoke attributes need to be imported and integrated in the base agent. The prompts, logic, and pipelines should therefore generalize not only across tool stacks but also across wildly different Kubernetes footprints, enabling rapid RCA and remediation in any customer environment.",
    "post_slug": "Agentic SRE",
    "post_title": "Agentic SRE",
    "section_heading": "Magnitude of the task",
    "tags": [
      "SRE",
      "AI",
      "AIOps",
      "Incident Management",
      "Platform Engineering",
      "Observability",
      "Automation",
      "DevOps"
    ],
    "url_fragment": "#magnitude-of-the-task",
    "position": 6,
    "token_count": 411,
    "date": "2025-10-11T22:29:31.751042"
  },
  {
    "chunk_id": "8ea4a88894ccb9cb",
    "content": "Successful agentic SRE systems share several fundamental capabilities that distinguish them from traditional monitoring and alerting tools:",
    "post_slug": "Agentic SRE",
    "post_title": "Agentic SRE",
    "section_heading": "Core Capabilities of Agentic SRE Systems",
    "tags": [
      "SRE",
      "AI",
      "AIOps",
      "Incident Management",
      "Platform Engineering",
      "Observability",
      "Automation",
      "DevOps"
    ],
    "url_fragment": "#core-capabilities-of-agentic-sre-systems",
    "position": 7,
    "token_count": 34,
    "date": "2025-10-11T22:29:31.751042"
  },
  {
    "chunk_id": "4f389a4419a704f5",
    "content": "Rather than simply forwarding alerts, agentic systems can analyze incoming incidents, assess severity and impact, and prioritize response efforts. They can correlate multiple alerts to identify potential cascading failures and route incidents to the most appropriate team members based on expertise and availability.",
    "post_slug": "Agentic SRE",
    "post_title": "Agentic SRE",
    "section_heading": "Core Capabilities of Agentic SRE Systems > Intelligent Incident Triage",
    "tags": [
      "SRE",
      "AI",
      "AIOps",
      "Incident Management",
      "Platform Engineering",
      "Observability",
      "Automation",
      "DevOps"
    ],
    "url_fragment": "#core-capabilities-of-agentic-sre-systems-intelligent-incident-triage",
    "position": 8,
    "token_count": 79,
    "date": "2025-10-11T22:29:31.751042"
  },
  {
    "chunk_id": "1c7d57f769bef49b",
    "content": "These systems excel at gathering relevant context from diverse sources\u2014runbooks, previous incident reports, system documentation, recent deployments, and real-time metrics. This capability dramatically reduces the time engineers spend searching for relevant information during high-pressure incidents.",
    "post_slug": "Agentic SRE",
    "post_title": "Agentic SRE",
    "section_heading": "Core Capabilities of Agentic SRE Systems > Contextual Knowledge Retrieval",
    "tags": [
      "SRE",
      "AI",
      "AIOps",
      "Incident Management",
      "Platform Engineering",
      "Observability",
      "Automation",
      "DevOps"
    ],
    "url_fragment": "#core-capabilities-of-agentic-sre-systems-contextual-knowledge-retrieval",
    "position": 9,
    "token_count": 75,
    "date": "2025-10-11T22:29:31.751042"
  },
  {
    "chunk_id": "e6ffd2566003274f",
    "content": "By analyzing patterns across logs, metrics, traces, and system events, agentic SRE systems can identify potential root causes and present them in order of likelihood. This doesn't replace human judgment but significantly narrows the investigation scope.",
    "post_slug": "Agentic SRE",
    "post_title": "Agentic SRE",
    "section_heading": "Core Capabilities of Agentic SRE Systems > Automated Root Cause Analysis",
    "tags": [
      "SRE",
      "AI",
      "AIOps",
      "Incident Management",
      "Platform Engineering",
      "Observability",
      "Automation",
      "DevOps"
    ],
    "url_fragment": "#core-capabilities-of-agentic-sre-systems-automated-root-cause-analysis",
    "position": 10,
    "token_count": 63,
    "date": "2025-10-11T22:29:31.751042"
  },
  {
    "chunk_id": "e0120170bbc43f4e",
    "content": "The most sophisticated systems learn from each incident, building institutional knowledge that improves future responses. They can identify recurring patterns, suggest process improvements, and even recommend infrastructure changes to prevent similar issues.",
    "post_slug": "Agentic SRE",
    "post_title": "Agentic SRE",
    "section_heading": "Core Capabilities of Agentic SRE Systems > Adaptive Learning",
    "tags": [
      "SRE",
      "AI",
      "AIOps",
      "Incident Management",
      "Platform Engineering",
      "Observability",
      "Automation",
      "DevOps"
    ],
    "url_fragment": "#core-capabilities-of-agentic-sre-systems-adaptive-learning",
    "position": 11,
    "token_count": 64,
    "date": "2025-10-11T22:29:31.751042"
  },
  {
    "chunk_id": "860fba9f2d2b3916",
    "content": "The agentic SRE ecosystem leverages several categories of tools and technologies:",
    "post_slug": "Agentic SRE",
    "post_title": "Agentic SRE",
    "section_heading": "Common Tools and Technologies",
    "tags": [
      "SRE",
      "AI",
      "AIOps",
      "Incident Management",
      "Platform Engineering",
      "Observability",
      "Automation",
      "DevOps"
    ],
    "url_fragment": "#common-tools-and-technologies",
    "position": 12,
    "token_count": 20,
    "date": "2025-10-11T22:29:31.751042"
  },
  {
    "chunk_id": "6c9cb37f4002f119",
    "content": "- **Monitoring Systems**: Datadog, New Relic, Prometheus, Grafana\n- **Logging Platforms**: Elasticsearch, Splunk, Fluentd\n- **Tracing Systems**: Jaeger, Zipkin, AWS X-Ray\n- **APM Tools**: AppDynamics, Dynatrace",
    "post_slug": "Agentic SRE",
    "post_title": "Agentic SRE",
    "section_heading": "Common Tools and Technologies > Observability Platforms",
    "tags": [
      "SRE",
      "AI",
      "AIOps",
      "Incident Management",
      "Platform Engineering",
      "Observability",
      "Automation",
      "DevOps"
    ],
    "url_fragment": "#common-tools-and-technologies-observability-platforms",
    "position": 13,
    "token_count": 52,
    "date": "2025-10-11T22:29:31.751042"
  },
  {
    "chunk_id": "04a10c5770a2c4ee",
    "content": "- **Alerting**: PagerDuty, Opsgenie, VictorOps\n- **Communication**: Slack, Microsoft Teams, dedicated incident channels\n- **Ticketing**: Jira, ServiceNow, Linear",
    "post_slug": "Agentic SRE",
    "post_title": "Agentic SRE",
    "section_heading": "Common Tools and Technologies > Incident Management",
    "tags": [
      "SRE",
      "AI",
      "AIOps",
      "Incident Management",
      "Platform Engineering",
      "Observability",
      "Automation",
      "DevOps"
    ],
    "url_fragment": "#common-tools-and-technologies-incident-management",
    "position": 14,
    "token_count": 40,
    "date": "2025-10-11T22:29:31.751042"
  },
  {
    "chunk_id": "878dcac78b9f4d92",
    "content": "- **Documentation**: Confluence, Notion, GitBook\n- **Runbooks**: Automated playbooks, decision trees\n- **Post-mortem Systems**: Incident retrospective tools",
    "post_slug": "Agentic SRE",
    "post_title": "Agentic SRE",
    "section_heading": "Common Tools and Technologies > Knowledge Management",
    "tags": [
      "SRE",
      "AI",
      "AIOps",
      "Incident Management",
      "Platform Engineering",
      "Observability",
      "Automation",
      "DevOps"
    ],
    "url_fragment": "#common-tools-and-technologies-knowledge-management",
    "position": 15,
    "token_count": 39,
    "date": "2025-10-11T22:29:31.751042"
  },
  {
    "chunk_id": "6e732feb57c82cb9",
    "content": "- **Configuration Management**: Ansible, Terraform, Pulumi\n- **Container Orchestration**: Kubernetes, Docker Swarm\n- **CI/CD Pipelines**: Jenkins, GitLab CI, GitHub Actions\n\nFor a detailed technical implementation of an agentic SRE system, see [Agentic Reliability Engineering](Agentic%20Reliability%20Engineering.md).",
    "post_slug": "Agentic SRE",
    "post_title": "Agentic SRE",
    "section_heading": "Common Tools and Technologies > Infrastructure as Code",
    "tags": [
      "SRE",
      "AI",
      "AIOps",
      "Incident Management",
      "Platform Engineering",
      "Observability",
      "Automation",
      "DevOps"
    ],
    "url_fragment": "#common-tools-and-technologies-infrastructure-as-code",
    "position": 16,
    "token_count": 79,
    "date": "2025-10-11T22:29:31.751042"
  },
  {
    "chunk_id": "aa0f121268a15849",
    "content": "While current systems focus on diagnosis and recommendation, the next generation will increasingly handle automated remediation for well-understood, low-risk scenarios. This includes automatic scaling, service restarts, configuration rollbacks, and traffic rerouting.",
    "post_slug": "Agentic SRE",
    "post_title": "Agentic SRE",
    "section_heading": "Future Outlook > Autonomous Remediation",
    "tags": [
      "SRE",
      "AI",
      "AIOps",
      "Incident Management",
      "Platform Engineering",
      "Observability",
      "Automation",
      "DevOps"
    ],
    "url_fragment": "#future-outlook-autonomous-remediation",
    "position": 17,
    "token_count": 66,
    "date": "2025-10-11T22:29:31.751042"
  },
  {
    "chunk_id": "9008855c65d63fce",
    "content": "Advanced systems will move beyond reactive incident response to predictive reliability engineering. By analyzing trends in system behavior, deployment patterns, and external factors, these systems will identify potential issues before they impact users.",
    "post_slug": "Agentic SRE",
    "post_title": "Agentic SRE",
    "section_heading": "Future Outlook > Predictive Reliability",
    "tags": [
      "SRE",
      "AI",
      "AIOps",
      "Incident Management",
      "Platform Engineering",
      "Observability",
      "Automation",
      "DevOps"
    ],
    "url_fragment": "#future-outlook-predictive-reliability",
    "position": 18,
    "token_count": 63,
    "date": "2025-10-11T22:29:31.751042"
  },
  {
    "chunk_id": "672dc44b5c69c9ea",
    "content": "Future agentic SRE platforms will operate across organizational boundaries, sharing anonymized insights about common failure patterns, effective remediation strategies, and emerging threats across the broader technology ecosystem.",
    "post_slug": "Agentic SRE",
    "post_title": "Agentic SRE",
    "section_heading": "Future Outlook > Cross-System Intelligence",
    "tags": [
      "SRE",
      "AI",
      "AIOps",
      "Incident Management",
      "Platform Engineering",
      "Observability",
      "Automation",
      "DevOps"
    ],
    "url_fragment": "#future-outlook-cross-system-intelligence",
    "position": 19,
    "token_count": 57,
    "date": "2025-10-11T22:29:31.751042"
  },
  {
    "chunk_id": "0c81c116c269edda",
    "content": "For the time being, most implementations will focus on augmenting human expertise rather than replacing it. This includes intelligent workload distribution, context-aware assistance, and adaptive interfaces that match individual engineer preferences and expertise levels.",
    "post_slug": "Agentic SRE",
    "post_title": "Agentic SRE",
    "section_heading": "Future Outlook > Human-AI Collaboration",
    "tags": [
      "SRE",
      "AI",
      "AIOps",
      "Incident Management",
      "Platform Engineering",
      "Observability",
      "Automation",
      "DevOps"
    ],
    "url_fragment": "#future-outlook-human-ai-collaboration",
    "position": 20,
    "token_count": 67,
    "date": "2025-10-11T22:29:31.751042"
  },
  {
    "chunk_id": "c92e0f356971650b",
    "content": "As systems become more autonomous, they'll need to incorporate regulatory requirements, compliance frameworks, and audit trails directly into their decision-making processes.",
    "post_slug": "Agentic SRE",
    "post_title": "Agentic SRE",
    "section_heading": "Future Outlook > Regulatory and Compliance Integration",
    "tags": [
      "SRE",
      "AI",
      "AIOps",
      "Incident Management",
      "Platform Engineering",
      "Observability",
      "Automation",
      "DevOps"
    ],
    "url_fragment": "#future-outlook-regulatory-and-compliance-integration",
    "position": 21,
    "token_count": 43,
    "date": "2025-10-11T22:29:31.751042"
  },
  {
    "chunk_id": "67897730aa439dfd",
    "content": "Building systems that engineers trust requires transparent decision-making, consistent performance, and graceful failure modes. The cost of false positives or incorrect remediation can be severe.",
    "post_slug": "Agentic SRE",
    "post_title": "Agentic SRE",
    "section_heading": "Challenges and Considerations > Trust and Reliability",
    "tags": [
      "SRE",
      "AI",
      "AIOps",
      "Incident Management",
      "Platform Engineering",
      "Observability",
      "Automation",
      "DevOps"
    ],
    "url_fragment": "#challenges-and-considerations-trust-and-reliability",
    "position": 22,
    "token_count": 48,
    "date": "2025-10-11T22:29:31.751042"
  },
  {
    "chunk_id": "56aff6ad03df81ef",
    "content": "Modern infrastructure involves dozens of tools and platforms. Agentic systems must navigate this complexity while maintaining performance and reliability.",
    "post_slug": "Agentic SRE",
    "post_title": "Agentic SRE",
    "section_heading": "Challenges and Considerations > Integration Complexity",
    "tags": [
      "SRE",
      "AI",
      "AIOps",
      "Incident Management",
      "Platform Engineering",
      "Observability",
      "Automation",
      "DevOps"
    ],
    "url_fragment": "#challenges-and-considerations-integration-complexity",
    "position": 23,
    "token_count": 38,
    "date": "2025-10-11T22:29:31.751042"
  },
  {
    "chunk_id": "3e4c45adbf26b02d",
    "content": "As these systems become more capable, SRE roles will evolve. Engineers will need to develop new skills in AI system management, prompt engineering, and human-AI collaboration.",
    "post_slug": "Agentic SRE",
    "post_title": "Agentic SRE",
    "section_heading": "Challenges and Considerations > Skills Evolution",
    "tags": [
      "SRE",
      "AI",
      "AIOps",
      "Incident Management",
      "Platform Engineering",
      "Observability",
      "Automation",
      "DevOps"
    ],
    "url_fragment": "#challenges-and-considerations-skills-evolution",
    "position": 24,
    "token_count": 43,
    "date": "2025-10-11T22:29:31.751042"
  },
  {
    "chunk_id": "fadd54c58b883505",
    "content": "Autonomous systems making decisions about critical infrastructure raise important questions about accountability, bias, and the appropriate level of human oversight.",
    "post_slug": "Agentic SRE",
    "post_title": "Agentic SRE",
    "section_heading": "Challenges and Considerations > Ethical Considerations",
    "tags": [
      "SRE",
      "AI",
      "AIOps",
      "Incident Management",
      "Platform Engineering",
      "Observability",
      "Automation",
      "DevOps"
    ],
    "url_fragment": "#challenges-and-considerations-ethical-considerations",
    "position": 25,
    "token_count": 41,
    "date": "2025-10-11T22:29:31.751042"
  },
  {
    "chunk_id": "304ac2eace18cebc",
    "content": "system reliability engineering is in flux with new procedures and protocols. Combining AI capabilities with domain expertise, these systems promise to make reliability engineering more effective, efficient, and scalable. SRE's are where this is being applied to first. Success will requires careful attention to trust, safety, and the evolving role of human expertise in an increasingly automated world.\n\nThe field is rapidly evolving, with new platforms and capabilities emerging regularly. Organizations considering agentic SRE implementations should focus on clear use cases, robust evaluation frameworks, and gradual capability expansion while maintaining strong human oversight and safety protocols.",
    "post_slug": "Agentic SRE",
    "post_title": "Agentic SRE",
    "section_heading": "Conclusion",
    "tags": [
      "SRE",
      "AI",
      "AIOps",
      "Incident Management",
      "Platform Engineering",
      "Observability",
      "Automation",
      "DevOps"
    ],
    "url_fragment": "#conclusion",
    "position": 26,
    "token_count": 176,
    "date": "2025-10-11T22:29:31.751042"
  },
  {
    "chunk_id": "8ade5725e9056ba8",
    "content": "1. [Navigating the Future: SRE and the Rise of LLM-based AI Assistants](https://www.questglobal.com/insights/thought-leadership/navigating-the-future-sre-and-the-rise-of-llm-based-ai-assistants/)\n2. [The Role of AI in SRE: Revolutionizing System Reliability and Efficiency](https://www.squadcast.com/blog/the-role-of-ai-in-sre-revolutionizing-system-reliability-and-efficiency#conclusion)\n3. [What is an AI SRE?](https://cleric.ai/blog/what-is-an-ai-sre)\n4. [Keep - Open Source Alert Management](https://github.com/keephq/keep)\n5. [Annual Outage Analysis 2023](https://datacenter.uptimeinstitute.com/rs/711-RIA-145/images/AnnualOutageAnalysis2023.03092023.pdf)",
    "post_slug": "Agentic SRE",
    "post_title": "Agentic SRE",
    "section_heading": "References",
    "tags": [
      "SRE",
      "AI",
      "AIOps",
      "Incident Management",
      "Platform Engineering",
      "Observability",
      "Automation",
      "DevOps"
    ],
    "url_fragment": "#references",
    "position": 27,
    "token_count": 165,
    "date": "2025-10-11T22:29:31.751042"
  },
  {
    "chunk_id": "09559b7cd8837b4d",
    "content": "# Building an Agentic-SRE: Future of Incident Response\n\nIn Site Reliability Engineering (SRE), every second counts. When an alert fires, its a race to identify the root cause and implement a fix. This process is often a manual, time-consuming journey through logs, metrics, and documentation. \n\nIn this article I outline an SRE agent that can plan, reflect, identify root causes, and recommend remediation when an alert arrives. While the ultimate goal is a fully reactive system, the initial objective is to significantly reduce the search space that a human engineer needs to navigate. \n\nI demonstrate a proof-of-concept orchestrator agent I've been building that will be expanded to form the main core of an agentic SRE, https://github.com/DonTron-prog/agent_sre.git. The orchestrator chooses a tool (RAG, search, deep-research, and calculator) that is based off the awesome Atomic-Agents framework.",
    "post_slug": "Agentic Reliability Engineering",
    "post_title": "Agentic Reliability Engineering",
    "section_heading": null,
    "tags": [
      "SRE",
      "AI",
      "Agents",
      "Orchestration",
      "Incident Response",
      "Reliability",
      "Atomic Agents"
    ],
    "url_fragment": "",
    "position": 0,
    "token_count": 225,
    "date": "2025-10-11T22:29:31.751751"
  },
  {
    "chunk_id": "243ea9be31729a2a",
    "content": "At its core, the Agentic-SRE is an intelligent system designed to mimic the troubleshooting process of a seasoned engineer. Upon receiving an alert, it embarks on a structured investigation, leveraging a suite of powerful tools and techniques.(Pienaar et. al.)[[https://cleric.ai/blog/what-is-an-ai-sre](https://cleric.ai/blog/what-is-an-ai-sre)]\n\n![sre agent overview](sre_agent_overview.png)\n*Operational workflow of the Agentic SRE, showing how it processes alerts through a structured decision-making pipeline that includes trajectory planning, reflection-based reasoning, and orchestration of task execution and tool integration, before generating final responses.*\n\nThe process can be visualized in two key stages: the high-level orchestration of tools and the iterative reasoning loop.\n\nFirst, the **Orchestrator Agent** acts as the central coordinator. When an alert comes in, it analyzes the context and decides which tool is best suited for the initial investigation. This could be a quick search of internal knowledge bases or a more complex deep research query.\n\nOnce a plan is formed, the agent enters a **ReAct (Reasoning and Acting) loop**[Yao el al. 2022](https://arxiv.org/abs/2210.03629). It executes a task, reflects on the output, and refines its approach. This iterative process of execution, reflection, and error correction allows the agent to intelligently navigate the problem space until a satisfactory solution is found.",
    "post_slug": "Agentic Reliability Engineering",
    "post_title": "Agentic Reliability Engineering",
    "section_heading": "How it Works: An Overview",
    "tags": [
      "SRE",
      "AI",
      "Agents",
      "Orchestration",
      "Incident Response",
      "Reliability",
      "Atomic Agents"
    ],
    "url_fragment": "#how-it-works-an-overview",
    "position": 1,
    "token_count": 361,
    "date": "2025-10-11T22:29:31.751751"
  },
  {
    "chunk_id": "c976b973fa39deed",
    "content": "An alert set the process in motion. These can come from various monitoring services like Datadog, Prometheus, or via webhooks. To be useful, these alerts are normalized into a structured format, typically a JSON object, containing key information:\n\n```python\nexample_alerts = [\n    {\n        \"alert\": \"Critical failure: 'ExtPluginReplicationError: Code 7749 - Sync Timeout with AlphaNode' in 'experimental-geo-sync-plugin v0.1.2'\",\n        \"context\": \"System: Primary PostgreSQL Database. Plugin: experimental third-party plugin integrated yesterday. Internal Documentation: NO internal docs exist for this plugin.\"\n    },\n    {\n        \"alert\": \"Pod CrashLoopBackOff for 'checkout-service'. Error: 'java.lang.OutOfMemoryError: Java heap space'\",\n        \"context\": \"System: Kubernetes microservice (Java Spring Boot). Traffic: 3x normal load due to flash sale.\"\n    },\n    {\n        \"alert\": \"API endpoint /api/v2/orders returning 503 Service Unavailable for 5% of requests. Latency P99 is 2500ms.\",\n        \"context\": \"Current error rate threshold: < 1%. Latency SLO: P99 < 800ms.\"\n    }\n]\n```\n\nThe more system information that can be added to the alert is better (such as time, origin or type). The `alert` and `context` form the basis of our queries for gathering more context.",
    "post_slug": "Agentic Reliability Engineering",
    "post_title": "Agentic Reliability Engineering",
    "section_heading": "The Starting Point: Alerts",
    "tags": [
      "SRE",
      "AI",
      "Agents",
      "Orchestration",
      "Incident Response",
      "Reliability",
      "Atomic Agents"
    ],
    "url_fragment": "#the-starting-point-alerts",
    "position": 2,
    "token_count": 320,
    "date": "2025-10-11T22:29:31.751751"
  },
  {
    "chunk_id": "9f78cf3a6db5416c",
    "content": "Recurring issues can often be resolved more quickly if the resolution was documented. One of the first things the Agentic-SRE should do is check its memory for similar past incidents. If a new alert matches a previously resolved one, the agent can immediately suggest the documented fix and associated Runbook. This addresses a common scenario where on-call engineers may not be aware of prior resolutions for repeat incidents.\n\nIn this implementation, we, naturally, have the internal memory of the LLMs, and have focused on a short-term, conversational memory system that:\n\n- Manages the chat history for the AI agents,\n- Maintains a coherent conversational flow,\n- Has a configurable message limit to manage memory usage.\n\nThis memory is key to the agent's ability to understand the problem and iterate on its plan by retaining the context of previous steps. It will be built upon to include long-term memory connected to RAG to store user or engineer info and preferences, significant events or findings, and record postmortems.[Bae et al]([https://arxiv.org/abs/2210.08750](https://arxiv.org/abs/2210.08750))",
    "post_slug": "Agentic Reliability Engineering",
    "post_title": "Agentic Reliability Engineering",
    "section_heading": "The Power of Memory: Learning from Past Incidents",
    "tags": [
      "SRE",
      "AI",
      "Agents",
      "Orchestration",
      "Incident Response",
      "Reliability",
      "Atomic Agents"
    ],
    "url_fragment": "#the-power-of-memory-learning-from-past-incidents",
    "position": 3,
    "token_count": 278,
    "date": "2025-10-11T22:29:31.751751"
  },
  {
    "chunk_id": "bbf75dd7de51e83b",
    "content": "For new and complex alerts, a plan is essential. The agent must break down the high-level task of \"find the root cause\" into a series of smaller, manageable steps. This planning is hierarchical. A planner agent devises the overall strategy, while the orchestrator executes the individual tasks.\n\nPlanning is inherently a path-finding problem. The agent must choose the most promising path from various options and tools. As it gathers more information with each step, it can prune unproductive paths and learn necessary parameters, zeroing in on the solution.\n\nTo aid in this process, the planner must have the necessary context of the system. For example a **knowledge graph** provides a high-level, up-to-date view of the system's architecture. This graph, which can be deterministically built and using tools like `kubectl` and injected into the planners context. The graph represents the interconnected components of the platform, from regions and projects down to individual containers and processes and helps in isolating the root cause.[Hao et al]([https://arxiv.org/abs/2305.14992](https://arxiv.org/abs/2305.14992))",
    "post_slug": "Agentic Reliability Engineering",
    "post_title": "Agentic Reliability Engineering",
    "section_heading": "Planning: Charting a Course to Resolution",
    "tags": [
      "SRE",
      "AI",
      "Agents",
      "Orchestration",
      "Incident Response",
      "Reliability",
      "Atomic Agents"
    ],
    "url_fragment": "#planning-charting-a-course-to-resolution",
    "position": 4,
    "token_count": 281,
    "date": "2025-10-11T22:29:31.751751"
  },
  {
    "chunk_id": "b1eddfd156a98a4e",
    "content": "![Distributed cloud infrastructure](sre_distributed_infra.png)\n*hierarchical structure of modern cloud infrastructure, showing how applications flow from regional cloud boundaries down through networking layers to containerized workloads running application code. The architecture demonstrates the nested relationship between cloud services, from high-level account organization to the granular execution environment where business logic operates.*",
    "post_slug": "Agentic Reliability Engineering",
    "post_title": "Agentic Reliability Engineering",
    "section_heading": "Planning: Charting a Course to Resolution > Example of Distributed cloud infrastrucrture",
    "tags": [
      "SRE",
      "AI",
      "Agents",
      "Orchestration",
      "Incident Response",
      "Reliability",
      "Atomic Agents"
    ],
    "url_fragment": "#planning-charting-a-course-to-resolution-example-of-distributed-cloud-infrastrucrture",
    "position": 5,
    "token_count": 112,
    "date": "2025-10-11T22:29:31.751751"
  },
  {
    "chunk_id": "2b82f08815a93aeb",
    "content": "The Agentic-SRE described bellow is built using [Atomic Agents](https://github.com/BrainBlend-AI/atomic-agents \"https://github.com/BrainBlend-AI/atomic-agents\"), which is an approach to AI agent development that prioritizes developer control and maintainability. It was easy to build upon the prebuilt tools and examples to suit this task.\n\nUnlike frameworks that bury you in abstractions, Atomic Agents follows a simple **Input-Process-Output (IPO)** model. Every agent and tool has:\n\n- **Input Schema**\u00a0(via Pydantic) - exactly what data goes in\n- **Processing Function**\u00a0- the actual logic\n- **Output Schema**\u00a0- precisely what comes out\n\nThis means no guesswork about data shapes, no debugging nightmares through layers of abstraction, and no black-box orchestrators making decisions for you.\n\n```python\n# Clean, predictable agent definition\nagent = BaseAgent(\n    BaseAgentConfig(\n        client=instructor.from_openai(openai.OpenAI()),\n        model=\"gpt-4o-mini\",\n        input_schema=OrchestratorInputSchema,\n        output_schema=OrchestratorOutputSchema,\n        system_prompt_generator=system_prompt_generator\n    )\n)\n```\n\nWhat makes Atomic Agents particularly powerful for SRE use cases:\n- Modularity: Each tool (RAG, web search, calculator) is an independent \"atom\" that can be swapped, tested, or debugged in isolation.\n- Schema Chaining: Tools connect seamlessly when their input/output schemas align - no manual data transformation needed.\n- Debuggability: Set breakpoints anywhere. See exactly what's in your system prompt, input data, or output JSON. \n- Performance: scale it like any traditional backend using python with standard deployment patterns.\n\nThe framework's philosophy of \"doing one thing well\" aligns perfectly with SRE principles. After getting lost in LangGraph and and CrewAI I found it an ideal foundation for the Agentic-SRE.",
    "post_slug": "Agentic Reliability Engineering",
    "post_title": "Agentic Reliability Engineering",
    "section_heading": "Planning: Charting a Course to Resolution > **Built on Atomic Agents: A Developer-First Framework**",
    "tags": [
      "SRE",
      "AI",
      "Agents",
      "Orchestration",
      "Incident Response",
      "Reliability",
      "Atomic Agents"
    ],
    "url_fragment": "#planning-charting-a-course-to-resolution-built-on-atomic-agents-a-developer-first-framework",
    "position": 6,
    "token_count": 465,
    "date": "2025-10-11T22:29:31.751751"
  },
  {
    "chunk_id": "f9bc1810dd93f2ff",
    "content": "The orchestrator is the heart of our Agentic-SRE. It's an intelligent decision-making coordinator that analyzes incoming requests and routes them to the most appropriate tool. To try it out follow the setup in the README then run:\n\n```Bash\npython orchestration_agent/orchestrator.py\n```\n\n![orchestrator](sre_orchestrator_agent.png)\n\n*Orchestrator Agent architecture that intelligently routes alerts through specialized tools - using Calulator, Web-Search, RAG Search for knowledge base queries against documents and incident history, and Deep Research for complex web-based investigations - before generating comprehensive resolutions through an iterative decision-making process.*\n\nIt follows a simple yet powerful pattern: analyze, decide, and route. 1. **Analyze the context**: It examines the input (alerts, error messages, user queries) along with any relevant contextual information. 2. **Make intelligent decisions**: It uses LLM reasoning to determine which specialized tool will provide the most valuable information. 3. **Routes and executes**: It directs the request to the chosen tool with the correctly formatted parameters. 4. **Repeats as needed**: This loop continues as the agent gathers more information.",
    "post_slug": "Agentic Reliability Engineering",
    "post_title": "Agentic Reliability Engineering",
    "section_heading": "Orchestration: The Intelligent Coordinator",
    "tags": [
      "SRE",
      "AI",
      "Agents",
      "Orchestration",
      "Incident Response",
      "Reliability",
      "Atomic Agents"
    ],
    "url_fragment": "#orchestration-the-intelligent-coordinator",
    "position": 7,
    "token_count": 305,
    "date": "2025-10-11T22:29:31.751751"
  },
  {
    "chunk_id": "aa1ad01eec06570c",
    "content": "3. **Routes and executes**: It directs the request to the chosen tool with the correctly formatted parameters. 4. **Repeats as needed**: This loop continues as the agent gathers more information. ```python\ndef execute_tool(searxng_tool, calculator_tool, rag_tool, deep_research_tool, orchestrator_output):\n    \"\"\"Route to the appropriate tool based on orchestrator decision.\"\"\"\n    \n    if orchestrator_output.tool in (\"search\", \"web-search\"):\n        if not isinstance(orchestrator_output.tool_parameters, SearxNGSearchToolInputSchema):\n            raise ValueError(f\"Invalid parameters for search tool\")\n        return searxng_tool.run(orchestrator_output.tool_parameters)\n    \n    elif orchestrator_output.tool == \"rag\":\n        if not isinstance(orchestrator_output.tool_parameters, RAGSearchToolInputSchema):\n            raise ValueError(f\"Invalid parameters for RAG tool\")\n        return rag_tool.run(orchestrator_output.tool_parameters)\n    \n    elif orchestrator_output.tool == \"deep-research\":\n        return deep_research_tool.run(orchestrator_output.tool_parameters)\n    \n    elif orchestrator_output.tool == \"calculator\":\n        return calculator_tool.run(orchestrator_output.tool_parameters)\n    \n    else:\n        raise ValueError(f\"Unknown tool: {orchestrator_output.tool}\")\n```\n\n\nIn my implementation, the Agentic-SRE uses the following tools:\n\n- **RAG (Retrieval-Augmented Generation)**: For querying internal knowledge bases, runbooks, and documentation. - **Web Search**: For finding external information like error codes, CVEs, or troubleshooting guides. For this, I used SearxNG, a privacy-respecting metasearch engine. - **Deep Research**: For comprehensive, multi-source analysis of complex problems. - **Calculator**: For metric calculations, threshold analysis, and numerical computations. To ensure reliability, Atomic-Agents uses `Pydantic` for validating inputs and outputs and `Instructor` to work with structured outputs from LLMs, which simplifies managing validation, retries, and streaming responses.",
    "post_slug": "Agentic Reliability Engineering",
    "post_title": "Agentic Reliability Engineering",
    "section_heading": "Orchestration: The Intelligent Coordinator",
    "tags": [
      "SRE",
      "AI",
      "Agents",
      "Orchestration",
      "Incident Response",
      "Reliability",
      "Atomic Agents"
    ],
    "url_fragment": "#orchestration-the-intelligent-coordinator",
    "position": 8,
    "token_count": 508,
    "date": "2025-10-11T22:29:31.751751"
  },
  {
    "chunk_id": "5d90ee2a11ea7256",
    "content": "```python\n# Setup structured LLM client\nclient = instructor.from_openai(openai.OpenAI(api_key=config[\"openai_api_key\"]))\n\n# Agent automatically validates and retries based on Pydantic schemas\nagent = BaseAgent(\n    BaseAgentConfig(\n        client=client,\n\t\t...\n    )\n)\n```",
    "post_slug": "Agentic Reliability Engineering",
    "post_title": "Agentic Reliability Engineering",
    "section_heading": "Instructor Setup",
    "tags": [
      "SRE",
      "AI",
      "Agents",
      "Orchestration",
      "Incident Response",
      "Reliability",
      "Atomic Agents"
    ],
    "url_fragment": "#instructor-setup",
    "position": 9,
    "token_count": 68,
    "date": "2025-10-11T22:29:31.751751"
  },
  {
    "chunk_id": "43b255b81f102607",
    "content": "```python\nclass OrchestratorInputSchema(BaseIOSchema):\n    \"\"\"Input schema for the SRE Orchestrator Agent.\"\"\"\n    \n    system_alert: str = Field(..., description=\"The system alert received (e.g., 'High CPU utilization on server X').\")\n    system_context: str = Field(..., description=\"Contextual information about the system (e.g., 'Production web server, recent deployment v1.2').\")\n\nclass OrchestratorOutputSchema(BaseIOSchema):\n    \"\"\"Output schema containing the tool to use and its parameters.\"\"\"\n    \n    tool: str = Field(..., description=\"The tool to use: 'search', 'calculator', 'rag', or 'deep-research'\")\n    tool_parameters: Union[SearxNGSearchToolInputSchema, CalculatorToolInputSchema, RAGSearchToolInputSchema, DeepResearchToolInputSchema] = Field(\n        ..., description=\"The parameters for the selected tool\"\n    )\n```",
    "post_slug": "Agentic Reliability Engineering",
    "post_title": "Agentic Reliability Engineering",
    "section_heading": "Pydantic validations",
    "tags": [
      "SRE",
      "AI",
      "Agents",
      "Orchestration",
      "Incident Response",
      "Reliability",
      "Atomic Agents"
    ],
    "url_fragment": "#pydantic-validations",
    "position": 10,
    "token_count": 209,
    "date": "2025-10-11T22:29:31.751751"
  },
  {
    "chunk_id": "221fca9e7807d1a6",
    "content": "When an alert isn't immediately recognized from memory, the agent needs to gather more context. This is where Retrieval-Augmented Generation (RAG) comes in. RAG provides the LLM with relevant reference material from an external knowledge base. For this pipeline, I'm using `ChromaDB` as the vector store.\n\nFor example, a generic LLM might know what a \u2018CrashLoopBackOff\u2019 is. But with RAG, we can provide it with our internal documentation, informing it that for our specific `auth-service`, a common cause is a misconfigured database connection\u2014something only found in our private docs.\n\nThe RAG process involves:\n\n1. **Ingesting Data**: Relevant data sources like infrastructure knowledge graphs, code repositories, documentation, and communication channels are ingested. Given that an estimated two-thirds of outages are due to configuration and human errors, recent information from Git and Slack is particularly valuable.\n2. **Chunking and Embedding**: The source data is broken into chunks and converted into numerical representations (embeddings) using an embedding model.\n3. **Storing**: These embeddings are stored in a vector store (ChromaDB) for efficient semantic search.\n4. **Retrieving**: When an alert comes in, its summary and details are used to query the vector store, which returns the most relevant documents.\n\nTo test the stand alone RAG interactively on your own documents Run `orchestration_agent/tools/rag_search/interactive.py`",
    "post_slug": "Agentic Reliability Engineering",
    "post_title": "Agentic Reliability Engineering",
    "section_heading": "Tools > Internal Knowledge: Retrieval-Augmented Generation (RAG)",
    "tags": [
      "SRE",
      "AI",
      "Agents",
      "Orchestration",
      "Incident Response",
      "Reliability",
      "Atomic Agents"
    ],
    "url_fragment": "#tools-internal-knowledge-retrieval-augmented-generation-rag",
    "position": 11,
    "token_count": 362,
    "date": "2025-10-11T22:29:31.751751"
  },
  {
    "chunk_id": "1c66e06ada5765a9",
    "content": "The deep research tool acts as an intelligent research assistant, capable of synthesizing information from multiple sources across the web. This is invaluable for:\n\n- **Error Investigation**: Looking up obscure error codes or messages.\n- **Technology Stack Issues**: Researching bugs or performance issues in open-source components.\n- **Third-Party Service Outages**: Checking if a third-party service is experiencing a known outage.\n- **Security Vulnerabilities**: Investigating potential security threats.\n\nThis deep research tool employs a multi-agent approach, with specialized agents for decision-making, query generation, and question-answering. This allows it to autonomously gather, process, and synthesize information, providing a comprehensive overview with citations and suggestions for follow-up questions. checkout `orchestration_agent/tools/deep_research/interactive.py` to run it as a stand alone deep researcher.",
    "post_slug": "Agentic Reliability Engineering",
    "post_title": "Agentic Reliability Engineering",
    "section_heading": "Tools > Uncovering Nuance with Deep Research",
    "tags": [
      "SRE",
      "AI",
      "Agents",
      "Orchestration",
      "Incident Response",
      "Reliability",
      "Atomic Agents"
    ],
    "url_fragment": "#tools-uncovering-nuance-with-deep-research",
    "position": 12,
    "token_count": 232,
    "date": "2025-10-11T22:29:31.751751"
  },
  {
    "chunk_id": "622723926aceb5cb",
    "content": "Interleaving reasoning and action (the ReAct framework) with moments of reflection has been shown to significantly improve performance. The more complex a task, the more potential failure points exist. After each step, our agent reflects on the generated response and the retrieved context. If the answer isn't satisfactory, the agent can iterate to refine its output.\n\n\n```python\ndef generate_final_answer(agent, input_schema, tool_response):\n    \"\"\"Generate a final answer based on the tool's output - this is the reflection step.\"\"\"\n    \n    # Temporarily switch to final answer schema\n    original_schema = agent.output_schema\n    agent.output_schema = FinalAnswerSchema\n    \n    # Add tool response to memory for context\n    agent.memory.add_message(\"system\", tool_response)\n    \n    # Run agent again to synthesize final answer\n    final_answer_obj = agent.run(input_schema)\n    \n    # Restore original schema\n    agent.output_schema = original_schema\n    \n    return final_answer_obj\n```",
    "post_slug": "Agentic Reliability Engineering",
    "post_title": "Agentic Reliability Engineering",
    "section_heading": "Reflection: The Key to Improved Performance",
    "tags": [
      "SRE",
      "AI",
      "Agents",
      "Orchestration",
      "Incident Response",
      "Reliability",
      "Atomic Agents"
    ],
    "url_fragment": "#reflection-the-key-to-improved-performance",
    "position": 13,
    "token_count": 248,
    "date": "2025-10-11T22:29:31.751751"
  },
  {
    "chunk_id": "919f848b3cbb8877",
    "content": "With all the pieces in place, the agent assembles a prompt for the LLM. This prompt includes:\n\n- The alert itself (the problem statement).\n- The results from each step of the executed plan (logs, deployment info, metrics).\n- Any additional context retrieved from the knowledge base.\n\nThe prompt instructs the LLM to analyze all this information and derive a root cause and a solution. The goal is to get a summarized, actionable response that explains what happened, where it happened, and how to fix it.",
    "post_slug": "Agentic Reliability Engineering",
    "post_title": "Agentic Reliability Engineering",
    "section_heading": "Bringing It All Together: LLM Reasoning and Prompt Assembly",
    "tags": [
      "SRE",
      "AI",
      "Agents",
      "Orchestration",
      "Incident Response",
      "Reliability",
      "Atomic Agents"
    ],
    "url_fragment": "#bringing-it-all-together-llm-reasoning-and-prompt-assembly",
    "position": 14,
    "token_count": 126,
    "date": "2025-10-11T22:29:31.751751"
  },
  {
    "chunk_id": "8db7110a0ad9d948",
    "content": "By combining AI-driven analysis with domain-specific knowledge and tools, an Agentic-SRE can dramatically reduce the complexity and time burden of incident management.\n\nThe key to success is maintaining a balance between automation and human judgment. While this AI assistant can significantly reduce cognitive load and accelerate troubleshooting, it remains a tool designed to augment, not replace, human expertise. By focusing on search space reduction, context enhancement, and pattern recognition, the assistant empowers SRE teams to make better, faster decisions.\n\nAs modern digital infrastructure continues to grow in complexity, tools like the Agentic-SRE will become increasingly vital for maintaining the reliability and performance that users expect. With Atomic-Agents framework they can be quickly integrated and modified. Through thoughtful implementation and continuous refinement, we can achieve new levels of operational excellence.\n\nLet me know what you think?\n\n- Can this agent help narrow the search space in SRE?\n- Are you an SRE? how has new tooling changed your workflow?\n- Where else could this type of orchestrator be applied?\n- What other features would you like to see?\n\nI'd love to hear your thoughts, comment or DM.\n\n**Donald McGillivray**\n\n**mcgillivray.d@gmail.com**",
    "post_slug": "Agentic Reliability Engineering",
    "post_title": "Agentic Reliability Engineering",
    "section_heading": "Conclusion: Augmenting, Not Replacing, Human Expertise",
    "tags": [
      "SRE",
      "AI",
      "Agents",
      "Orchestration",
      "Incident Response",
      "Reliability",
      "Atomic Agents"
    ],
    "url_fragment": "#conclusion-augmenting-not-replacing-human-expertise",
    "position": 15,
    "token_count": 324,
    "date": "2025-10-11T22:29:31.751751"
  },
  {
    "chunk_id": "a10db42885e86341",
    "content": "# EVALUATING LLM PERFORMANCE ON SPECIALIZED TASKS\n**METRIC EVALUATION BEST PRACTICES**",
    "post_slug": "Evaluating LLM Performance on Specialized Tasks",
    "post_title": "Evaluating LLM Performance on Specialized Tasks",
    "section_heading": null,
    "tags": [
      "LLM",
      "AI",
      "Machine Learning",
      "Evaluation",
      "Metrics",
      "Performance",
      "NLP",
      "Data Science",
      "Optimization"
    ],
    "url_fragment": "",
    "position": 0,
    "token_count": 21,
    "date": "2025-10-11T22:29:31.752545"
  },
  {
    "chunk_id": "37b0d3297145e5b5",
    "content": "_\"What gets measured gets mastered.\"_ - adage\n\nThis data science adage has never been more relevant than in the era of Large Language Models (LLMs). This report outlines how to approach LLM evaluation, covering everything from initial exploratory assessments to sophisticated performance metrics that drive systematic optimization across prompting, synthetic data generation, and fine-tuning stages. Only in understanding the measurement, can we have confidence in our application of these powerful yet complex language systems.\n\nUnlike traditional machine learning where success depends on carefully engineering features for specific tasks, LLMs represent a paradigm shift in how we approach evaluation and optimization. These deep neural networks automatically extract and learn relevant features from massive datasets, developing general capabilities that transfer across domains without task-specific training. This emergent property\u2014where models demonstrate reasonable performance on diverse tasks \"out of the box\"\u2014creates both opportunities and challenges for measuring and enhancing single task performance. The challenge lies in developing appropriate metrics that capture not just accuracy, but nuanced aspects of language understanding, reasoning, and task-specific utility [1, 2]. Without robust evaluation frameworks, we risk being deceived and optimizing for the wrong objectives or missing critical failure modes.\n\n---",
    "post_slug": "Evaluating LLM Performance on Specialized Tasks",
    "post_title": "Evaluating LLM Performance on Specialized Tasks",
    "section_heading": "INTRODUCTION: THE LLM OPTIMIZATION PARADIGM",
    "tags": [
      "LLM",
      "AI",
      "Machine Learning",
      "Evaluation",
      "Metrics",
      "Performance",
      "NLP",
      "Data Science",
      "Optimization"
    ],
    "url_fragment": "#introduction-the-llm-optimization-paradigm",
    "position": 1,
    "token_count": 358,
    "date": "2025-10-11T22:29:31.752545"
  },
  {
    "chunk_id": "35b2ff6af80b07d6",
    "content": "_\"Simpler is smarter\"_ - adage\n\nBefore embarking on any optimization or pipeline, and as a general best practice when working with LLMs, perform a \"vibe check.\" Interview the models. See how well it answers questions related to your specific task. This initial hands-on interaction serves several crucial purposes:",
    "post_slug": "Evaluating LLM Performance on Specialized Tasks",
    "post_title": "Evaluating LLM Performance on Specialized Tasks",
    "section_heading": "PHASE 1: INITIAL EXPLORATION AND \"VIBE CHECK\"",
    "tags": [
      "LLM",
      "AI",
      "Machine Learning",
      "Evaluation",
      "Metrics",
      "Performance",
      "NLP",
      "Data Science",
      "Optimization"
    ],
    "url_fragment": "#phase-1-initial-exploration-and-vibe-check",
    "position": 2,
    "token_count": 78,
    "date": "2025-10-11T22:29:31.752545"
  },
  {
    "chunk_id": "288fbeeb1f7caece",
    "content": "**Task Refinement**: It forces one to clearly articulate the task, know the inputs and closely examine the outputs. Almost always, preconceived ideas about the task change at this stage.\n\n**Manual Prompt Tuning**: Plan on iterating through at least 10 examples, manually refining the prompt yourself. This develops intuition about what works and what doesn't, and will serve as a jumping off point to start optimizing.\n\n**Understanding Nuances**: Grasp the subtleties of your task, anticipate the kind of data you'll need, and identify the types of metrics that will be valuable for scoring performance.\n\nEssentially, this initial phase helps you internalize the new mantra for the age of LLMs: replacing \"know your data\" with \"know your task.\" (but still know your data)\n\n---",
    "post_slug": "Evaluating LLM Performance on Specialized Tasks",
    "post_title": "Evaluating LLM Performance on Specialized Tasks",
    "section_heading": "PHASE 1: INITIAL EXPLORATION AND \"VIBE CHECK\" > Key Benefits",
    "tags": [
      "LLM",
      "AI",
      "Machine Learning",
      "Evaluation",
      "Metrics",
      "Performance",
      "NLP",
      "Data Science",
      "Optimization"
    ],
    "url_fragment": "#phase-1-initial-exploration-and-vibe-check-key-benefits",
    "position": 3,
    "token_count": 194,
    "date": "2025-10-11T22:29:31.752545"
  },
  {
    "chunk_id": "efdf3cf7a9b3ff83",
    "content": "_\"You can't improve what you don't measure.\"_ - adage\n\nAs we move from initial exploration to systematic optimization of the task, robust metrics become essential. There are a few standard metrics in evaluating these NLP models (as noted in table 1), these can be combined and built upon. Frameworks exist to assist in this like DSPy [3], Comet, and LangSmith, they are powerful and help get started quickly, but still require an understanding on how to evaluate the task [1]. The metrics chosen in these frameworks will identify weaknesses and guide improvements.\n\nThe metrics can be a second order optimization problem, one on top of optimizing for your task. They will evolve, adapt, and it is crucial they can be tuned [1, 4]. This happens 1) because new information is gathered, or the metric is not capturing all the dimensions of the task, and 2) the chosen metric is not sensitive enough and not providing enough signal. But like in Phase 1 \"Simpler is smarter\" the important thing is to start somewhere and iterate.\n\nA multi-faceted evaluation approach, combining computationally efficient traditional metrics with more complex evaluators, can balance efficiency and fidelity (and cost) [5]. By layering simple quantitative measures with more nuanced qualitative assessments, we can create an evaluation system that captures both surface-level performance and deeper aspects of model capability. Balancing multiple measurements ensures efforts target genuine improvements rather than metric-gaming behaviours [1].\n\nInitially, the same set of metrics might be used across different optimization stages (baseline evaluation, prompt optimization, synthetic data generation, fine-tuning, multi-hop systems). However, finer-grained scoring might be necessary depending on the task and stage [6].",
    "post_slug": "Evaluating LLM Performance on Specialized Tasks",
    "post_title": "Evaluating LLM Performance on Specialized Tasks",
    "section_heading": "PHASE 2: ESTABLISHING EVALUATION METRICS",
    "tags": [
      "LLM",
      "AI",
      "Machine Learning",
      "Evaluation",
      "Metrics",
      "Performance",
      "NLP",
      "Data Science",
      "Optimization"
    ],
    "url_fragment": "#phase-2-establishing-evaluation-metrics",
    "position": 4,
    "token_count": 449,
    "date": "2025-10-11T22:29:31.752545"
  },
  {
    "chunk_id": "c31c13835b2aeef2",
    "content": "| Metric Category                     | Description                                                                                                            | Examples                                        | Drawbacks                                                                                             |\n| ----------------------------------- | ---------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------- | ----------------------------------------------------------------------------------------------------- |\n| **Coarse-Grained Traditional**      | Performs an initial, surface-level analysis. Computationally simple and allows for rapid scoring. [1, 7]               | Accuracy, Precision, Token F1, binary pass/fail | While fast, they often fail to gauge coherence, factual correctness, or nuanced quality.              |\n| **Fine-Grained Traditional**        | Provides more detailed textual analysis, often comparing generated text to reference(s). [8]                           | ROUGE, Perplexity, BLEU, BERTSCORE              | Most require high-quality reference texts, which can be costly and may not capture all valid outputs. |\n| **Expert Ranking (LLM-as-a-Judge)** | Provides a more holistic assessment that can correlate better with human judgment and downstream task performance. [5] | LLM-as-a-Judge evaluation protocols             | Slow and computationally expensive; susceptible to biases; requires careful prompt engineering.       |",
    "post_slug": "Evaluating LLM Performance on Specialized Tasks",
    "post_title": "Evaluating LLM Performance on Specialized Tasks",
    "section_heading": "PHASE 2: ESTABLISHING EVALUATION METRICS > Types of Evaluation Metrics",
    "tags": [
      "LLM",
      "AI",
      "Machine Learning",
      "Evaluation",
      "Metrics",
      "Performance",
      "NLP",
      "Data Science",
      "Optimization"
    ],
    "url_fragment": "#phase-2-establishing-evaluation-metrics-types-of-evaluation-metrics",
    "position": 5,
    "token_count": 393,
    "date": "2025-10-11T22:29:31.752545"
  },
  {
    "chunk_id": "8df372853eb4d175",
    "content": "A single metric rarely tells the whole story. Combining metrics into a composite score, weighted according to task priorities, often provides the best signal for optimization. But, of course \"simpler is smarter\". The following are some task specific considerations [1]:\n\n**Summarization**: Blend faithfulness (ROUGE, BERTSCORE), conciseness (length checks), and coherence (LLM-judge assessment).[7, 8, 9]\n\n**Classification/Categorization**: Rely on standard metrics like Accuracy, Precision, Recall, and F1-score, appropriately weighted for the real-world cost of errors.\n\n**Information Extraction**: Focus on entity-level metrics with strict or relaxed span matching rather than token overlap.\n\n**Question Answering**: Combine Exact Match, F1 score, BERTSCORE, and LLM-judge assessment of factual consistency [7, 8].\n\n**Code Generation**: Evaluate functional correctness (unit tests), adherence to specifications (LLM-judge), efficiency, and code quality (linters).\n\n**Reasoning**: Accuracy on specific reasoning benchmarks (e.g., mathematical problems, logical deduction tasks), evaluation of intermediate steps (Chain-of-Thought consistency) [1, 5], LLM-as-a-judge for logical soundness and factual consistency checks.\n\nComparing metrics against each other is crucial [8]. If coarse and fine-grained metrics don't track together across examples, it can highlight outliers or specific failure modes. Periodically check the correlation between different metric types for your specific task.",
    "post_slug": "Evaluating LLM Performance on Specialized Tasks",
    "post_title": "Evaluating LLM Performance on Specialized Tasks",
    "section_heading": "PHASE 2: ESTABLISHING EVALUATION METRICS > Task-Specific Metric Selection",
    "tags": [
      "LLM",
      "AI",
      "Machine Learning",
      "Evaluation",
      "Metrics",
      "Performance",
      "NLP",
      "Data Science",
      "Optimization"
    ],
    "url_fragment": "#phase-2-establishing-evaluation-metrics-task-specific-metric-selection",
    "position": 6,
    "token_count": 372,
    "date": "2025-10-11T22:29:31.752545"
  },
  {
    "chunk_id": "261390c8d8d8889b",
    "content": "If a reference or \"gold standard\" example of input and output exist then the quantitative measures of similarity can be known and simplify the evaluations. However, these comparisons often struggle with semantics and variations in expression, for example multiple correct answers or ways to summarize a text. Embedding models and LLM-as-judge can help if this is the case (as described below) [8].\n\nWhen reference texts aren't available, several alternatives exist: (1) Use LLM-as-Judge approaches where a stronger model evaluates outputs based on criteria-specific rubrics (GEval is a versatile framework for deploying LLM-as-Judge) [1, 5, 9]; (2) Develop task-specific heuristics that check for required elements or constraints; (3) Implement self-consistency checks comparing multiple outputs from the same model with different sampling parameters; or (4) Create synthetic references using strong models, though these should be validated through human review. The most robust approach combines multiple evaluation methods to overcome the limitations of any single technique, creating a more comprehensive assessment framework even without traditional references. The key is recognizing that even without perfect references, structured evaluation criteria still yield valuable optimization signals.\n\n---",
    "post_slug": "Evaluating LLM Performance on Specialized Tasks",
    "post_title": "Evaluating LLM Performance on Specialized Tasks",
    "section_heading": "PHASE 2: ESTABLISHING EVALUATION METRICS > Reference-Based Metrics",
    "tags": [
      "LLM",
      "AI",
      "Machine Learning",
      "Evaluation",
      "Metrics",
      "Performance",
      "NLP",
      "Data Science",
      "Optimization"
    ],
    "url_fragment": "#phase-2-establishing-evaluation-metrics-reference-based-metrics",
    "position": 7,
    "token_count": 326,
    "date": "2025-10-11T22:29:31.752545"
  },
  {
    "chunk_id": "5a73dd37d2723b82",
    "content": "_\"Words don't mean; people mean.\"_ - S.I. Hayakawa\n\nTo measure the true meaning we have to move beyond surface-level text comparison. Semantic similarity measures the equivalence of groups of words through vector representations of language. This is a hard problem. Humans misinterpret even face to face spoken word with the extra context of body language and intonations. Measuring perfect semantic similarity is hard in NLP tasks but perfection is not required for it to be valuable [3].\n\nModern approaches to measure semantics leverage contextual embeddings that map text to high-dimensional vectors where cosine similarity measures conceptual closeness. Earlier embedding models like word2vec and GloVe have advanced to contextual embeddings like BERT, RoBERTa, or sentence transformers (There are now thousands of specialty trained task specific embedding models) [15]. When evaluating lengthy texts with multiple ideas, chunking becomes crucial. Chunking divides texts into manageable segments (sentences, paragraphs, or semantic units) [16]. Fine-grained sentence-level chunking captures detailed alignments but may miss broader themes, while paragraph-level chunks preserve more context but blur fine details. The final similarity score typically combines chunk-level scores through weighted averaging, with weights potentially adjusted to emphasize crucial sections. Thoughtful choice of embedding model and chunking can extract the necessary concepts desired for evaluation.",
    "post_slug": "Evaluating LLM Performance on Specialized Tasks",
    "post_title": "Evaluating LLM Performance on Specialized Tasks",
    "section_heading": "PHASE 3: BEYOND BASIC METRICS: NUANCED PERFORMANCE ANALYSIS > Measuring Semantics",
    "tags": [
      "LLM",
      "AI",
      "Machine Learning",
      "Evaluation",
      "Metrics",
      "Performance",
      "NLP",
      "Data Science",
      "Optimization"
    ],
    "url_fragment": "#phase-3-beyond-basic-metrics-nuanced-performance-analysis-measuring-semantics",
    "position": 8,
    "token_count": 371,
    "date": "2025-10-11T22:29:31.752545"
  },
  {
    "chunk_id": "c08855e07956673c",
    "content": "_\"When a measure becomes a target it ceases to be a good metric.\"_ - Charles Goodhart\n\nSeveral common pitfalls can undermine evaluation efforts. Perhaps the most frequent is over-reliance on a single metric (e.g., using only ROUGE for summarization) or using metrics inappropriate for the task's nuances (e.g., BLEU for code functionality) [5, 9]. Another danger is ignoring known metric limitations and biases. For example, n-gram metrics overlook semantic meaning, reference-based metrics depend heavily on reference quality, and LLM judges can exhibit positional bias, prefer verbosity, or be 'gamed' by models optimizing for the judge's preferences rather than true quality (Goodhart's Law). Moreover, focusing solely on average performance metrics might mask critical failures in worst-case scenarios, which could be crucial depending on the application's risk profile. Finally, it should be stressed, failing to correlate metrics with human judgment can lead you to optimize for the wrong things, and is why Phase 1 is so important.",
    "post_slug": "Evaluating LLM Performance on Specialized Tasks",
    "post_title": "Evaluating LLM Performance on Specialized Tasks",
    "section_heading": "PHASE 3: BEYOND BASIC METRICS: NUANCED PERFORMANCE ANALYSIS > Metric Correlation and Pitfalls",
    "tags": [
      "LLM",
      "AI",
      "Machine Learning",
      "Evaluation",
      "Metrics",
      "Performance",
      "NLP",
      "Data Science",
      "Optimization"
    ],
    "url_fragment": "#phase-3-beyond-basic-metrics-nuanced-performance-analysis-metric-correlation-and-pitfalls",
    "position": 9,
    "token_count": 259,
    "date": "2025-10-11T22:29:31.752545"
  },
  {
    "chunk_id": "8bd11dc917e03f28",
    "content": "Evaluating intermediate reasoning steps (Chain-of-Thought) can provide rich evaluation signals, by evaluating the reasoning process and not just the outcome [7, 13]. Maintaining, and evaluating CoT traces allows for error localization and diagnosis, identify flawed logic, and recognition partial correctness. They are valuable in distillation and training smaller models. In SFT teaching the process is more effective than just showing final answers, it reduces the amount of data typically needed for fine-tuning or standard distillation [2]. That being said they are complex to evaluate and an active area of research in RL. LLM-as-a-Judge have been adapted to assess reasoning, checking logical flow and factual consistency. They provide value in guiding the model to better response and richer training data [1, 6].",
    "post_slug": "Evaluating LLM Performance on Specialized Tasks",
    "post_title": "Evaluating LLM Performance on Specialized Tasks",
    "section_heading": "PHASE 3: BEYOND BASIC METRICS: NUANCED PERFORMANCE ANALYSIS > Reasoning Evaluation",
    "tags": [
      "LLM",
      "AI",
      "Machine Learning",
      "Evaluation",
      "Metrics",
      "Performance",
      "NLP",
      "Data Science",
      "Optimization"
    ],
    "url_fragment": "#phase-3-beyond-basic-metrics-nuanced-performance-analysis-reasoning-evaluation",
    "position": 10,
    "token_count": 205,
    "date": "2025-10-11T22:29:31.752545"
  },
  {
    "chunk_id": "640e82900e0c90a6",
    "content": "_\"Primum non nocere.\" (First, do no harm.)_ - Hippocratic Oath\n\nIt is necessary to understand how the model performs in its worst cases as they often have a much greater impact on user trust and safety than average-case performance and can undermine the entire system. Conditional Value-at-Risk (CVaR) focuses on the average error in the worst percentile of outputs, quantifying the expected severity of major failures [10]. Unlike standard metrics that report mean performance across all samples, CVaR (also called Expected Shortfall) specifically targets tail risk by calculating the average loss within the worst-performing subset of examples, typically the bottom 5% or 10%.\n\n**CVaR is calculated by:**\n\n1. Sorting all test examples by their error scores\n2. Selecting the worst \u03b1-fraction of examples\n3. Computing the mean error across this subset\n\nExamining the best and worst samples has always provided valuable insight to me. By tracking CVaR alongside mean metrics, one can ensure that optimization efforts don't improve average performance at the expense of worst-case scenarios.",
    "post_slug": "Evaluating LLM Performance on Specialized Tasks",
    "post_title": "Evaluating LLM Performance on Specialized Tasks",
    "section_heading": "PHASE 3: BEYOND BASIC METRICS: NUANCED PERFORMANCE ANALYSIS > Worst-Case Performance Assessment",
    "tags": [
      "LLM",
      "AI",
      "Machine Learning",
      "Evaluation",
      "Metrics",
      "Performance",
      "NLP",
      "Data Science",
      "Optimization"
    ],
    "url_fragment": "#phase-3-beyond-basic-metrics-nuanced-performance-analysis-worst-case-performance-assessment",
    "position": 11,
    "token_count": 272,
    "date": "2025-10-11T22:29:31.752545"
  },
  {
    "chunk_id": "fb3e3500f5869f84",
    "content": "Complementing statistical approaches like CVaR, adversarial testing deliberately probes for failure modes through systematic challenges designed to break the model. While not necessarily a set of scores to optimize, once failure modes are discovered they can be addressed. This involves:\n\n**Systematic Red-Teaming**: Having skilled practitioners (or specialized models) deliberately craft inputs designed to provoke problematic outputs, exploring boundaries of performance.\n\n**Targeted Challenge Sets**: Creating specialized test datasets focusing on known vulnerabilities or edge cases relevant to the specific task.\n\n**Perturbation Analysis**: Introducing controlled variations to inputs (changing wording, adding irrelevant information, or introducing ambiguity) to test robustness.\n\n**Counterfactual Testing**: Modifying key aspects of well-performing examples to identify which changes cause performance degradation.\n\nAdversarial testing provides qualitative insights that purely statistical methods might miss, revealing specific failure patterns and vulnerabilities. When combined with CVaR, these methods provide a comprehensive understanding of worst-case performance, enabling targeted improvements to enhance model reliability in critical scenarios.",
    "post_slug": "Evaluating LLM Performance on Specialized Tasks",
    "post_title": "Evaluating LLM Performance on Specialized Tasks",
    "section_heading": "PHASE 3: BEYOND BASIC METRICS: NUANCED PERFORMANCE ANALYSIS > Adversarial Testing and Red-Teaming",
    "tags": [
      "LLM",
      "AI",
      "Machine Learning",
      "Evaluation",
      "Metrics",
      "Performance",
      "NLP",
      "Data Science",
      "Optimization"
    ],
    "url_fragment": "#phase-3-beyond-basic-metrics-nuanced-performance-analysis-adversarial-testing-and-red-teaming",
    "position": 12,
    "token_count": 315,
    "date": "2025-10-11T22:29:31.752545"
  },
  {
    "chunk_id": "bd0959c6244c4e84",
    "content": "When transitioning from evaluating model outputs to generating data for SFT or distillation, the quality threshold fundamentally shifts [13]. While evaluation accepts \"adequately correct\" responses, synthetic training data demands exemplary outputs that represent ideal performance. Training large language models on a smaller quantity of high-quality synthetic data can lead to better performance compared to training on a larger quantity of unvalidated data [5, 14]. These examples become the ceiling for what the distilled model can learn. Each synthetic example must not only be correct but optimal in its approach, reasoning, and format consistency [2].\n\nUnlike test sets that might deliberately oversample edge cases, synthetic training data must match the expected production distribution to prevent distribution shift [5, 11, 12]. They also must maintain appropriate diversity without overrepresenting rare scenarios. And avoid introducing unintended biases or shortcuts that would be amplified through distillation.\n\n**A new layer of metrics becomes necessary to evaluate and filter the dataset as a whole:**\n\n- Dataset-level diversity measurements\n- Coverage analysis of the problem space\n- Consistency scores across similar examples\n- Learning signal strength (how clearly the examples demonstrate the patterns to be learned)\n- Counterfactual completeness (including both positive and negative examples)\n\nThe fundamental shift is that evaluation asks \"Is this output good?\" while synthetic data generation asks \"Is this the ideal example to learn from?\" The latter requires considering not just correctness, but learnability, generalizability, and how the example fits within the broader learning curriculum.\n\n---",
    "post_slug": "Evaluating LLM Performance on Specialized Tasks",
    "post_title": "Evaluating LLM Performance on Specialized Tasks",
    "section_heading": "PHASE 3: BEYOND BASIC METRICS: NUANCED PERFORMANCE ANALYSIS > Synthetic Data Generation for Distillation: Key Considerations",
    "tags": [
      "LLM",
      "AI",
      "Machine Learning",
      "Evaluation",
      "Metrics",
      "Performance",
      "NLP",
      "Data Science",
      "Optimization"
    ],
    "url_fragment": "#phase-3-beyond-basic-metrics-nuanced-performance-analysis-synthetic-data-generation-for-distillation-key-considerations",
    "position": 13,
    "token_count": 431,
    "date": "2025-10-11T22:29:31.752545"
  },
  {
    "chunk_id": "0833a2dead1ef5e6",
    "content": "_\"The ultimate metric is user feedback.\"_ - adage\n\nWhile the goal is often automation, human judgement remains the gold standard for validating LLM quality [8]. This is especially true for assessing nuanced aspects like creativity, genuine helpfulness, subtle biases, or alignment with complex human values.\n\nLLMs and the data they process are constantly in flux. Input data drifts, models are updated, metrics evolve, and benchmarks saturate. Therefore, evaluation requires ongoing monitoring. Be prepared to recalibrate your metrics and evaluation criteria as the ecosystem changes and your understanding of the task deepens.\n\n---",
    "post_slug": "Evaluating LLM Performance on Specialized Tasks",
    "post_title": "Evaluating LLM Performance on Specialized Tasks",
    "section_heading": "PHASE 4: HUMAN OVERSIGHT AND CONTINUOUS MONITORING",
    "tags": [
      "LLM",
      "AI",
      "Machine Learning",
      "Evaluation",
      "Metrics",
      "Performance",
      "NLP",
      "Data Science",
      "Optimization"
    ],
    "url_fragment": "#phase-4-human-oversight-and-continuous-monitoring",
    "position": 14,
    "token_count": 158,
    "date": "2025-10-11T22:29:31.752545"
  },
  {
    "chunk_id": "8c28deb449419bf0",
    "content": "Evaluating LLMs effectively, especially when optimizing them for specific tasks, requires a thoughtful, multi-faceted, and iterative approach. Starting with manual \"vibe checks\" and grounding the process in human judgment provides essential intuition. This should be complemented by a combination of traditional automated metrics and sophisticated LLM-as-a-Judge techniques, carefully selected and weighted based on task goals. By embracing this comprehensive methodology, we can better understand, optimize, and confidently deploy LLMs for specialized applications.\n\n---",
    "post_slug": "Evaluating LLM Performance on Specialized Tasks",
    "post_title": "Evaluating LLM Performance on Specialized Tasks",
    "section_heading": "CONCLUSION: A COMPREHENSIVE APPROACH",
    "tags": [
      "LLM",
      "AI",
      "Machine Learning",
      "Evaluation",
      "Metrics",
      "Performance",
      "NLP",
      "Data Science",
      "Optimization"
    ],
    "url_fragment": "#conclusion-a-comprehensive-approach",
    "position": 15,
    "token_count": 142,
    "date": "2025-10-11T22:29:31.752545"
  },
  {
    "chunk_id": "026aec13c04c9f49",
    "content": "1. Chang, Y., Wang, X., Wang, J., Wu, Y., Yang, L., Zhu, K., Chen, H., Yi, X., Wang, C., Wang, Y., Ye, W., Zhang, Y., Chang, Y., Yu, P. S., Yang, Q., & Xie, X. (2024). A Survey on Evaluation of Large Language Models. ACM Transactions on Intelligent Systems and Technology, 15(3), Article 39. [https://doi.org/10.1145/3641289]\n    \n2. Hsieh, C.-Y., Li, C.-L., Yeh, C.-K., Nakhost, H., Fujii, Y., Ratner, A., Krishna, R., Lee, C.-Y., & Pfister, T. (2023). Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes. arXiv preprint arXiv:2305.02301. [https://arxiv.org/abs/2305.02301]\n    \n3. Khattab, O., Singhvi, A., Maheshwari, P., Zhang, Z., Santhanam, K., Vardhamanan, S., Haq, S., Sharma, A., Joshi, T. T., Moazam, H., Miller, H., Zaharia, M., & Potts, C. (2023). DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines. arXiv preprint arXiv:2310.03714. [https://arxiv.org/abs/2310.03714]\n    \n4. Wang, Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S., Ren, W., Arulraj, A., He, X., Jiang, Z., Li, T., Ku, M., Wang, K., Zhuang, A., Fan, R., Yue, X., & Chen, W. (2024). MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark. arXiv preprint arXiv:2406.01574 [cs.CL]. [https://arxiv.org/abs/2406.01574]\n    \n5. Wang, J., Yang, J., Li, H., Zhuang, H., Chen, C., & Zeng, Z. (2024). RewardDS: Privacy-Preserving Fine-Tuning for Large Language Models via Reward Driven Data Synthesis. arXiv preprint arXiv:2403.16953. [https://arxiv.org/abs/2403.16953]\n    \n6. Ma, Y., Qing, L., Liu, J., Kang, Y., Zhang, Y., Lu, W., Liu, X., & Cheng, Q. (2024). From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications. arXiv preprint arXiv:2310.07613. [https://arxiv.org/abs/2310.07613]\n    \n7. Le-Duc, K., Nguyen, K.-N., Tat, B. P., Le, D., Ngo, J., Vo-Dang, L., Nguyen, A. T., & Hy, T.-S. (2024). Sentiment Reasoning for Healthcare. arXiv preprint arXiv:2404.03367. [https://arxiv.org/abs/2404.03367]\n    \n8. Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., & Artzi, Y. (2020).",
    "post_slug": "Evaluating LLM Performance on Specialized Tasks",
    "post_title": "Evaluating LLM Performance on Specialized Tasks",
    "section_heading": "REFERENCES",
    "tags": [
      "LLM",
      "AI",
      "Machine Learning",
      "Evaluation",
      "Metrics",
      "Performance",
      "NLP",
      "Data Science",
      "Optimization"
    ],
    "url_fragment": "#references",
    "position": 16,
    "token_count": 532,
    "date": "2025-10-11T22:29:31.752545"
  },
  {
    "chunk_id": "8a7aaaa89185e703",
    "content": "T., & Hy, T.-S. (2024). Sentiment Reasoning for Healthcare. arXiv preprint arXiv:2404.03367. [https://arxiv.org/abs/2404.03367]\n    \n8. Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., & Artzi, Y. (2020). BERTSCORE: Evaluating Text Generation with BERT. In International Conference on Learning Representations (ICLR). arXiv preprint arXiv:1904.09675. [https://arxiv.org/abs/1904.09675]\n    \n9. Zhang, Y., Jin, H., Meng, D., Wang, J., & Tan, J. (2024). A Comprehensive Survey on Automatic Text Summarization with Exploration of LLM-Based Methods. arXiv preprint arXiv:2403.14195. [https://arxiv.org/abs/2403.14195]\n    \n10. Chaudhary, S., Dinesha, U., Kalathil, D., & Shakkottai, S. (2024). Risk-Averse Fine-tuning of Large Language Models. Accepted to International Conference on Learning Representations (ICLR) 2025. Available on OpenReview. 11. Hashimoto, T. B., Srivastava, M., Namkoong, H., & Liang, P. (2018). Fairness Without Demographics in Repeated Loss Minimization. In Proceedings of the 35th International Conference on Machine Learning (ICML) (pp. 1929-1938). PMLR. [http://proceedings.mlr.press/v80/hashimoto18a.html]\n    \n12. Duchi, J. C., & Namkoong, H. (2018). Learning models with uniform performance via distributionally robust optimization. arXiv preprint arXiv:1810.08750. [https://arxiv.org/abs/1810.08750]\n    \n13. Shirgaonkar, A., Pandey, N., Abay, N. C., Aktas, T., & Aski, V. (2024). KNOWLEDGE DISTILLATION USING FRONTIER OPEN-SOURCE LLMS: GENERALIZABILITY AND THE ROLE OF SYNTHETIC DATA. arXiv preprint arXiv:2403.14164. [https://arxiv.org/abs/2403.14164]\n    \n14. Iskander, S., Cohen, N., Karnin, Z., Shapira, O., & Tolmach, S. (2024). Quality Matters: Evaluating Synthetic Data for Tool-Using LLMs. arXiv preprint arXiv:2409.16341. [https://arxiv.org/abs/2409.16341]\n    \n15. Warner, B., Chaffin, A., Clavi\u00e9, B., Weller, O., Hallstr\u00f6m, O., Taghadouini, S., Gallagher, A., Biswas, R., Ladhak, F., Aarsens, T., Cooper, N., Adams, G., Howard, J., & Poli, I. (2024). _Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference_. arXiv preprint arXiv:2412.13663.",
    "post_slug": "Evaluating LLM Performance on Specialized Tasks",
    "post_title": "Evaluating LLM Performance on Specialized Tasks",
    "section_heading": "REFERENCES",
    "tags": [
      "LLM",
      "AI",
      "Machine Learning",
      "Evaluation",
      "Metrics",
      "Performance",
      "NLP",
      "Data Science",
      "Optimization"
    ],
    "url_fragment": "#references",
    "position": 17,
    "token_count": 540,
    "date": "2025-10-11T22:29:31.752545"
  },
  {
    "chunk_id": "a7b99bc109bf770a",
    "content": "(2024). _Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference_. arXiv preprint arXiv:2412.13663. [https://arxiv.org/abs/2412.13663]\n    \n16. Schwaber-Cohen, R. (2023, June 30). Chunking strategies for LLM applications. Pinecone. [https://pinecone.io/learn/chunking-strategies/]",
    "post_slug": "Evaluating LLM Performance on Specialized Tasks",
    "post_title": "Evaluating LLM Performance on Specialized Tasks",
    "section_heading": "REFERENCES",
    "tags": [
      "LLM",
      "AI",
      "Machine Learning",
      "Evaluation",
      "Metrics",
      "Performance",
      "NLP",
      "Data Science",
      "Optimization"
    ],
    "url_fragment": "#references",
    "position": 18,
    "token_count": 89,
    "date": "2025-10-11T22:29:31.752545"
  },
  {
    "chunk_id": "4dbcbae55dd11b0e",
    "content": "Intelligence is what one can do with a little bit of information. Stupidity is what one can't do with a lot of information. Today's LLMs and AI fall into the latter category; however, we can push them in the direction of the former by providing them with the context they require to make decisions. The most effective way of doing that is with Retrieval-Augmented Generation (RAG).[1]\n\n![monkey stacking context](monkey_context.png)\nThe latest buzzword is **context engineering**. Gone is the way of prompt engineering, where the user had to think of ways to best elicit the desired output from the model. We now know enough to provide the right phrasing in the system prompt and then programmatically retrieve the right context needed for the task to fill in the rest. For example: documentation, date and time, few-shot examples, available tools, etc. RAG is the obvious way to search for relevant information. Ill note retreaval can also be done using agents, however RAG offers the benifite of trading offline compute for online query performance.\n\nRAG dynamically updates the model's context with relevant information by connecting to data sources and selecting the entries matching the search. RAG is good for real-time data access, combating hallucinations, combating stale knowledge, applying rules or guardrails, and overcoming the model's lack of memory.",
    "post_slug": "RAG_Context_Engineering",
    "post_title": "RAG Context Engineering",
    "section_heading": null,
    "tags": [
      "RAG",
      "AI",
      "Evaluation",
      "Agents",
      "Context Engineering",
      "Guardrails"
    ],
    "url_fragment": "",
    "position": 0,
    "token_count": 341,
    "date": "2025-10-11T22:29:31.753259"
  },
  {
    "chunk_id": "b029536d0c292d29",
    "content": "There's a meme that RAG is dead. Proponents invoke the argument that RAG might not be needed in the future when model context length grows significantly or through \"agentic RAG,\" where an agent searches for the relevant context. However, I believe it will stick around because it can be used with small, fast LLMs or LLMs running on restricted hardware.[2] While context lengths are growing rapidly, in some applications, the data is increasing even faster. Possibly, the volume of data will always be more than the context window. And, of course, RAG is key for context engineering. This is an optimization problem where you want to give the model the maximum relevant context without overfitting, causing inadvertent steering from irrelevant data, or causing \"context rot.\"\n\nRAG is only useful if your data has value. If the LLM alone can provide the answer, then you don't need RAG, and your dataset is more or less worthless. You've been scooped. I am being jocose, because context engineering with your data can prime the model to get it in the right latent space and frame it to provide a valuable answer. But it is here that we see the trade-off between prompt engineering and context engineering at its clearest. If the model can be prompted easily to return a correct solution, that's all that's needed. If a bit of extra context is warranted, then RAG can be useful, given other considerations such as latency and compute. RAG can be as simple as a few lines of code and can scale to the enterprise level. I've deployed a few RAG systems and these are my learnings. I'll walk you through something that can immediately provide value on your chatbot, website search, blog, business resources, or other applications. ![RAG workflow](RAG_pipeline.png)\nRAG is a trade-off: it improves query-time performance at the cost of write-time performance. It can be divided into **data indexing** (offline processing) and **retrieval and generation** (online processing).",
    "post_slug": "RAG_Context_Engineering",
    "post_title": "RAG Context Engineering",
    "section_heading": "The RAG Workflow",
    "tags": [
      "RAG",
      "AI",
      "Evaluation",
      "Agents",
      "Context Engineering",
      "Guardrails"
    ],
    "url_fragment": "#the-rag-workflow",
    "position": 1,
    "token_count": 492,
    "date": "2025-10-11T22:29:31.753259"
  },
  {
    "chunk_id": "3faaf734230e9d78",
    "content": "It can be divided into **data indexing** (offline processing) and **retrieval and generation** (online processing). Since we can pre-process the dataset offline, we can balance accuracy and speed and can return responses in the fashion suitable for the application. This is a continuation of the decoupling trend of separating read/write and storage/compute operations of modern distributed systems and ensures the system is easily adaptable. RAG can be further split into five main components:\n\nOffline:\n\n1. Preprocessing the documents (cleaning and chunking the data)\n    \n2. Embedding the chunks into vectors using an embedding model\n    \n3. Vector database storage \n\nOnline:\n    \n4. Retrieval\n    \n5. Generation\n    \n---",
    "post_slug": "RAG_Context_Engineering",
    "post_title": "RAG Context Engineering",
    "section_heading": "The RAG Workflow",
    "tags": [
      "RAG",
      "AI",
      "Evaluation",
      "Agents",
      "Context Engineering",
      "Guardrails"
    ],
    "url_fragment": "#the-rag-workflow",
    "position": 2,
    "token_count": 181,
    "date": "2025-10-11T22:29:31.753259"
  },
  {
    "chunk_id": "441d774f5e0b0e42",
    "content": "In machine learning, pre-processing our data has always been, I would argue from the data science side, the biggest pain and time and accuracy bottleneck. The law, _garbage in, garbage out_, always applies. The quality of RAG's responses depends on how the data is stored and pre-processed. This requires looking at the data, converting it all to unified plain text, and removing images, artifacts, and other unwanted data. PDFs are the worst file format; on conversion, they often have useless headers and footers. It is recommended to use an OCR tool, like Tesseract,[7] to extract info from images, diagrams, and charts and to extract tables. Specialized PDF parsing tools like NeMo Retriever can significantly improve extraction accuracy and efficiency compared to general-purpose approaches.[11] These tools use multi-stage processing with dedicated models for different content types, resulting in better retrieval performance. On the governance side, the data needs to be scrubbed of unwanted information, especially personally identifiable information (PII), to be compliant with the standards you are working within (e.g., GDPR, HIPAA...). It is here that security starts, and access control filtering and audit trails can be implemented.",
    "post_slug": "RAG_Context_Engineering",
    "post_title": "RAG Context Engineering",
    "section_heading": "Part 1: Indexing (Offline) > Ingestion of Documents",
    "tags": [
      "RAG",
      "AI",
      "Evaluation",
      "Agents",
      "Context Engineering",
      "Guardrails"
    ],
    "url_fragment": "#part-1-indexing-offline-ingestion-of-documents",
    "position": 3,
    "token_count": 311,
    "date": "2025-10-11T22:29:31.753259"
  },
  {
    "chunk_id": "9901d39655f87a20",
    "content": "Having just looked at the data to clean it, it is easier to envision how one can split it up into chunks. How you chunk depends on how you intend to retrieve it later. The size of the chunk is a balance between small, diverse, and computationally heavy embeddings versus large, self-contained but superfluous data with lightweight embeddings. If the data is code, you can use a tool like tree-sitter to chunk by methods,[8] functions, or classes. If it's documents, you can chunk by pages, paragraphs, sentences, or use a recursive strategy. Q&A pairs should be split together. The most thoughtless chunking strategy is a fixed length of a number of words. This isn't ideal, as it often breaks sentences or ideas in half. The goal is for each chunk to be a complete semantic unit, not a random fragment that gets split mid-concept.[10] If you must use fixed length, overlap chunks so context between them is not lost, and spend some time optimizing the responses. The chunk size must not exceed the context length of the generative or the embedding model. There are documented best practices depending on the data\u2014like for Chinese, math, images, etc.\u2014and a quick search can save a lot of re-engineering.",
    "post_slug": "RAG_Context_Engineering",
    "post_title": "RAG Context Engineering",
    "section_heading": "Part 1: Indexing (Offline) > Chunking",
    "tags": [
      "RAG",
      "AI",
      "Evaluation",
      "Agents",
      "Context Engineering",
      "Guardrails"
    ],
    "url_fragment": "#part-1-indexing-offline-chunking",
    "position": 4,
    "token_count": 300,
    "date": "2025-10-11T22:29:31.753259"
  },
  {
    "chunk_id": "0632cd4a2276a0b5",
    "content": "With chunks in hand, they can be passed through an embedding model. It converts the important properties of the original data into a vector. A model like `nvidia/Llama-3.2-nv-embedqa-1b-v2` is fast, efficient,[9] has a long context window, supports multiple languages, and scores well on retrieval tasks. But there are lots of off-the-shelf models to choose from, specific to the types of input data, and it's not hard to fine-tune one for your own use case to squeeze that extra bit of accuracy out. When selecting an embedding model, it's also useful to consult the **MTEB benchmark**,[12] which evaluates embeddings across a wide range of retrieval and semantic tasks, helping guide the choice for your application.\n\nThe vectors are stored in a vector database along with the plain text chunks and any metadata you want to include. The vector database does two things: it stores the data and it performs vector search. The former is easy; the latter is hard and where the magic is. Vector search is a nearest neighbor search problem: given a query, find the k-nearest neighbors to it. It does this by computing similarity scores (e.g cosine similarity) between the vectors and then ranks all vectors based on their scores, returning the k vectors with the highest scores. For large datasets, the Approximate Nearest Neighbor (ANN) algorithm is used for speed. You can use a variety of vector databases, from open-source options like `pgvector`, `Qdrant`, and `Chroma` to managed enterprise solutions. Metadata (such as author, page number, date, section, etc.) is valuable to include with the chunks. The more detailed the index, the better the accuracy; however, it will be slower to build (I am increasingly running into cases were metadata was greater than the actual data it was discribing, and I suspect this will continue).\n\n---",
    "post_slug": "RAG_Context_Engineering",
    "post_title": "RAG Context Engineering",
    "section_heading": "Part 1: Indexing (Offline) > Embedding and Storage",
    "tags": [
      "RAG",
      "AI",
      "Evaluation",
      "Agents",
      "Context Engineering",
      "Guardrails"
    ],
    "url_fragment": "#part-1-indexing-offline-embedding-and-storage",
    "position": 5,
    "token_count": 459,
    "date": "2025-10-11T22:29:31.753259"
  },
  {
    "chunk_id": "9f494b15fbba4d73",
    "content": "With the data cleaned, chunked, embedded, and neatly stored away, we can get to the exciting part: retrieval. While vector search is powerful for finding semantically similar information, we can get a lot smarter. A more robust approach is a **hybrid search**, which combines the best of both worlds: dense retrieval (our vector search) for understanding meaning, and sparse retrieval (keyword-based search like BM25 or Elasticsearch) for pinpointing exact lexical matches. While dense retrieval is powerful, for many use cases, fast and cheap sparse retrieval is all that's needed. This dual-pronged attack ensures we get results that are not only conceptually related but also contain the specific terms we are looking for.\n\nTo merge the results of dense and sparse retrieval, we can use **Reciprocal Rank Fusion (RRF)** to combine the ranked lists from both searches into a single, more relevant list.[6] For large datasets, the initial retrieval uses ANN\u2014a fast and relatively cheap first pass\u2014to fetch a broad set of candidate documents. After this, **reranking** should be done. A more sophisticated and computationally expensive reranking model can perform a deeper analysis on this smaller subset, reordering them to place the absolute best matches at the top. This two-stage process is a classic trade-off, balancing speed and cost with precision and ensuring the model gets the most potent context to work with.",
    "post_slug": "RAG_Context_Engineering",
    "post_title": "RAG Context Engineering",
    "section_heading": "Part 2: Retrieval and Generation (Online) > Retrieval",
    "tags": [
      "RAG",
      "AI",
      "Evaluation",
      "Agents",
      "Context Engineering",
      "Guardrails"
    ],
    "url_fragment": "#part-2-retrieval-and-generation-online-retrieval",
    "position": 6,
    "token_count": 355,
    "date": "2025-10-11T22:29:31.753259"
  },
  {
    "chunk_id": "3b96a5689f2676dd",
    "content": "Despite the focus on context engineering, the prompt hasn't gone away. While a simple prompt will work, thought needs to be put into it. The prompt should be optimized, and it's a good thing we looked at our data because it rests on the underlying data type (e.g., code, question/answers, law documents). It is based on a number of factors: the generator model being used, the number of documents retrieved, the specific format of the input/output, etc. It is important that the prompt encourages the model to say \"I don't know,\" otherwise you will get hallucinations. For chatbot RAG applications, a separate model can be used for query rewriting so that context from previous turns of the conversation can be included if needed.\n\n```python\nRAG_TEMPLATE = \"\"\"\n#CONTEXT:\n{context}\n\nQUERY:\n{query}\n\nUse the provided context to answer the provided query. Only use the provided \ncontext to answer the query. If you do not know the answer, or it's not contained \nin the provided context respond with \u201cI don't know\u201d\n\"\"\"\n```\n\nMost people consider the LLM to be the most critical part of an AI system, but it's not in RAG. Most of the heavy lifting should have been done at this point. Often, all that is needed is a model just smart enough to assemble the answer from the query and the retrieved context. The choice of generator model is based on the use case, domain, context window, and cost.",
    "post_slug": "RAG_Context_Engineering",
    "post_title": "RAG Context Engineering",
    "section_heading": "Part 2: Retrieval and Generation (Online) > Prompting and Generation",
    "tags": [
      "RAG",
      "AI",
      "Evaluation",
      "Agents",
      "Context Engineering",
      "Guardrails"
    ],
    "url_fragment": "#part-2-retrieval-and-generation-online-prompting-and-generation",
    "position": 7,
    "token_count": 347,
    "date": "2025-10-11T22:29:31.753259"
  },
  {
    "chunk_id": "9b80ffc636f67f88",
    "content": "Once the system is working, it\u2019s critical to test it thoroughly. Challenge it with edge cases, such as queries about data not present in the corpus or with deliberately misleading metadata. Build a \u201cgold standard\u201d dataset of query-document pairs and measure metrics like **Context Precision** (are the retrieved documents relevant?) and **Context Recall** (did the system retrieve all relevant documents?). For small datasets, this can be done manually. For larger ones, you can automate evaluation with an **LLM-as-a-judge**.[3] In this approach, a separate model reviews the retrieval and rates its relevance. benchmarks like MTEB for scoring embeddings or Tools like **DSPy** can also be used here to optimize prompts, structure evaluations, and experiment with retrieval strategies in a systematic way.[4,5]",
    "post_slug": "RAG_Context_Engineering",
    "post_title": "RAG Context Engineering",
    "section_heading": "Evaluation and Advanced Techniques > Testing and Evaluation",
    "tags": [
      "RAG",
      "AI",
      "Evaluation",
      "Agents",
      "Context Engineering",
      "Guardrails"
    ],
    "url_fragment": "#evaluation-and-advanced-techniques-testing-and-evaluation",
    "position": 8,
    "token_count": 202,
    "date": "2025-10-11T22:29:31.753259"
  },
  {
    "chunk_id": "449ceb16ea4b79d3",
    "content": "Once your PoC is validated, you can scale the system to be as complex as your use case requires. This could involve sharding and replicating your database for performance and reliability. You can also focus on several areas of optimization:\n\n- **Retrieval Optimization:** Fine-tuning the search process for better accuracy and speed.\n    \n- **Model Optimization:** Using more efficient models, which can lead to significant cost savings.\n    \n- **Prompt Optimization:** Refining the prompts sent to the LLM for more consistent and accurate generation.\n    \n\nBeyond a basic setup, the world of RAG offers advanced techniques for different data types. These include **multimodal RAG** for images, **tabular RAG** for structured data, and my personal favorite, **graph RAG**, which is excellent for retrieving information based on its relationships to other data points.",
    "post_slug": "RAG_Context_Engineering",
    "post_title": "RAG Context Engineering",
    "section_heading": "Evaluation and Advanced Techniques > Scaling and Advanced RAG",
    "tags": [
      "RAG",
      "AI",
      "Evaluation",
      "Agents",
      "Context Engineering",
      "Guardrails"
    ],
    "url_fragment": "#evaluation-and-advanced-techniques-scaling-and-advanced-rag",
    "position": 9,
    "token_count": 216,
    "date": "2025-10-11T22:29:31.753259"
  },
  {
    "chunk_id": "6538a7c47cb3f47f",
    "content": "Maybe it should have been mentioned at the beginning, but RAG should not be used in all scenarios. It should be used for Q&A, not to make a model smarter. If the model already knows the answer, you don't need RAG. It's also not great for creative writing, as semantic meaning doesn't work the same way for poetry. Don't use it if extremely low latency is needed. It can be faster than providing all the data directly to an LLM, but it does add some latency. It's not good for volatile (e.g., stock tickers) or conflicting data. If the dataset is small enough that it can easily fit in the context window (before context rot) or the use case is limited, it could be overkill. RAG systems do need to be maintained, monitored, and adjusted.",
    "post_slug": "RAG_Context_Engineering",
    "post_title": "RAG Context Engineering",
    "section_heading": "Final Thoughts > When Not to Use RAG",
    "tags": [
      "RAG",
      "AI",
      "Evaluation",
      "Agents",
      "Context Engineering",
      "Guardrails"
    ],
    "url_fragment": "#final-thoughts-when-not-to-use-rag",
    "position": 10,
    "token_count": 184,
    "date": "2025-10-11T22:29:31.753259"
  },
  {
    "chunk_id": "271e61b7a1989db5",
    "content": "So what have we done? We traded write-time performance (indexing, which can be done offline) for query-time performance. In so doing, we reduced hallucinations, provided real-time data access, reduced out-of-date and stale knowledge, and gave the model memory. Search is a key workflow in modern AI systems, and RAG is the workhorse that provides semantic search and better results for your queries.\n\n---",
    "post_slug": "RAG_Context_Engineering",
    "post_title": "RAG Context Engineering",
    "section_heading": "Final Thoughts > Conclusion",
    "tags": [
      "RAG",
      "AI",
      "Evaluation",
      "Agents",
      "Context Engineering",
      "Guardrails"
    ],
    "url_fragment": "#final-thoughts-conclusion",
    "position": 11,
    "token_count": 101,
    "date": "2025-10-11T22:29:31.753259"
  },
  {
    "chunk_id": "c1d05aa7e6dc2644",
    "content": "1. Huyen, C. (2024). _AI Engineering: Building Applications with Foundation Models_. Chip Huyen.\n    \n2. Lewis, P., et al. (2020). \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.\" _arXiv preprint arXiv:2005.11401_.\n    \n3. Zheng, L., et al. (2023). \"Judging LLM-as-a-judge with MT-Bench and Chatbot Arena.\" _arXiv preprint arXiv:2306.05685_.\n    \n4. Muennighoff, N., et al. (2022). \"MTEB: Massive Text Embedding Benchmark.\" _arXiv preprint arXiv:2210.07316_.\n    \n5. Khattab, O., et al. (2023). \"DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines.\" _arXiv preprint arXiv:2310.03714_.\n    \n6. Cormack, G. V., Clarke, C. L., & Buettcher, S. (2009). \"Reciprocal Rank Fusion for Extreme Scale Search.\" In _Proceedings of the 18th international conference on World wide web_.\n    \n7. Tesseract OCR. (n.d.). GitHub Repository. Retrieved from `https://github.com/tesseract-ocr/tesseract`\n    \n8. Tree-sitter. (n.d.). Official Website. Retrieved from `https://tree-sitter.github.io/tree-sitter/`\n    \n9. NVIDIA NIM. (2024). `Llama-3.2-nv-embedqa-1b-v2` Model Card. Hugging Face. Retrieved from `https://huggingface.co/nvidia/Llama-3.2-nv-embedqa-1b-v2`\n    \n10. Li, Y. K., Wornell, G. W., & Win, M. Z. (2024). \"Information-Theoretic Limits on Compression of Semantic Information.\" _arXiv preprint arXiv:2407.03969_.\n\n11. NVIDIA Developer Blog. (2024). \"Approaches to PDF Data Extraction for Information Retrieval.\" Retrieved from `https://developer.nvidia.com/blog/approaches-to-pdf-data-extraction-for-information-retrieval/`\n\n12. Muennighoff, N., et al. (2022). \"MTEB: Massive Text Embedding Benchmark.\" Hugging Face Leaderboard. Retrieved from `https://huggingface.co/spaces/mteb/leaderboard`",
    "post_slug": "RAG_Context_Engineering",
    "post_title": "RAG Context Engineering",
    "section_heading": "References",
    "tags": [
      "RAG",
      "AI",
      "Evaluation",
      "Agents",
      "Context Engineering",
      "Guardrails"
    ],
    "url_fragment": "#references",
    "position": 12,
    "token_count": 433,
    "date": "2025-10-11T22:29:31.753259"
  }
]